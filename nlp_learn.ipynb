{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using NLTK library, we can do lot of text preprocesing\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "#function to split text into word\n",
    "\n",
    "tokens = word_tokenize(\"The quick brown fox jumps over the lazy dog\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['All', 'work', 'and', 'no', 'play', 'makes', 'jack', 'a', 'dull', 'boy', ',', 'all', 'work', 'and', 'no', 'play']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/user/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "nltk.download('punkt')\n",
    "data = \"All work and no play makes jack a dull boy, all work and no play\"\n",
    "print(word_tokenize(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "data = \"All work and no play makes jack dull boy. All work and no play makes jack a dull boy.\"\n",
    "stopWords = set(stopwords.words('english')) \n",
    "print(stopWords)\n",
    "phrases = sent_tokenize(data)\n",
    "words = word_tokenize(data)\n",
    " \n",
    "print(phrases)\n",
    "print(words)\n",
    "wordsFiltered = []\n",
    "for w in words:\n",
    "    if w not in stopWords:\n",
    "        wordsFiltered.append(w)\n",
    "print(wordsFiltered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "data = \"All work and no play makes jack dull boy. All work and no play makes jack a dull boy.\"\n",
    "stopWords = set(stopwords.words('english')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stopword removal\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "data = \"All work and no play makes jack dull boy. All work and no play makes jack a dull boy.\"\n",
    "stopWords = set(stopwords.words('english')) \n",
    "print(stopWords)\n",
    "words = word_tokenize(data)\n",
    "wordsFiltered = []\n",
    "for w in words:\n",
    "    if w not in stopWords:\n",
    "        wordsFiltered.append(w)\n",
    "print(wordsFiltered)\n",
    "print(len(stopWords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stemming\n",
    "#3 stemmer alogirthms are used here\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "\n",
    "# went is not working\n",
    "words = [\"go\",\"went\",\"gone\",\"games\"]\n",
    "ps = PorterStemmer()\n",
    "lanca_stemmer = LancasterStemmer()\n",
    "sb_stemmer = SnowballStemmer(\"english\",)\n",
    "print(\"=========Porter Stemmer======\")\n",
    "for w in words:\n",
    "    print(w ,\" : \" ,ps.stem(w))\n",
    "    \n",
    "print(\"=========Lanca Stemmer======\")\n",
    "for w in words:\n",
    "\n",
    "    print(w ,\" : \" ,lanca_stemmer.stem(w))\n",
    "    \n",
    "print(\"=========Snowball Stemmer======\")\n",
    "for w in words:\n",
    "    \n",
    "    print(w ,\" : \" ,sb_stemmer.stem(w))\n",
    "    \n",
    "    \n",
    " #Porter stemmer and Snowball stemmer working in similar manner   \n",
    "#lanca is not so good\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#speech tagging\n",
    "\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "\n",
    "document = 'Whether you\\'re new to programming or an experienced developer, it\\'s easy to learn and use Python.'\n",
    "sentences = nltk.sent_tokenize(document) \n",
    "for sent in sentences:\n",
    "    print(nltk.pos_tag(nltk.word_tokenize(sent)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filtering from Speech tagging\n",
    "import nltk\n",
    "from nltk.corpus import state_union\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "\n",
    "document = 'Today the Netherlands celebrates King\\'s Day. To honor this tradition, the Dutch embassy in San Francisco invited me to'\n",
    "sentences = nltk.sent_tokenize(document)  \n",
    "data = []\n",
    "for sent in sentences:\n",
    "    data = data + nltk.pos_tag(nltk.word_tokenize(sent))\n",
    "for w in data:\n",
    "    if 'VB' in w[1]:\n",
    "        print(w)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#natural language prediction\n",
    "import nltk\n",
    "\n",
    " \n",
    "from nltk.corpus import names\n",
    "\n",
    "def gender_features(word): \n",
    "    return {'last_letter': word[-1]}\n",
    "\n",
    "names = ([(name, 'male') for name in names.words('male.txt')] + \n",
    "         [(name, 'female') for name in names.words('female.txt')])\n",
    "\n",
    "featuresets = [(gender_features(n), g) for (n,g) in names]\n",
    "train_set = featuresets\n",
    "classifier = nltk.MaxentClassifier.train(train_set) \n",
    "print(classifier.classify(gender_features('Frank')))\n",
    "name = input(\"Name: \")\n",
    "print(classifier.classify(gender_features(name)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.classify.util\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.corpus import names\n",
    " \n",
    "def word_feats(words):\n",
    "    return dict([(word, True) for word in words])\n",
    " \n",
    "positive_vocab = [ 'awesome', 'outstanding', 'fantastic', 'terrific', 'good', 'nice', 'great', ':)' ]\n",
    "negative_vocab = [ 'bad', 'terrible','useless', 'hate', ':(' ]\n",
    "neutral_vocab = [ 'movie','the','sound','was','is','actors','did','know','words','not' ]\n",
    " \n",
    "positive_features = [(word_feats(pos), 'pos') for pos in positive_vocab]\n",
    "negative_features = [(word_feats(neg), 'neg') for neg in negative_vocab]\n",
    "neutral_features = [(word_feats(neu), 'neu') for neu in neutral_vocab]\n",
    " \n",
    "train_set = negative_features + positive_features + neutral_features\n",
    " \n",
    "classifier = NaiveBayesClassifier.train(train_set) \n",
    " \n",
    "# Predict\n",
    "neg = 0\n",
    "pos = 0\n",
    "sentence = \"Awesome movie, I liked it\"\n",
    "sentence = sentence.lower()\n",
    "words = sentence.split(' ')\n",
    "for word in words:\n",
    "    classResult = classifier.classify( word_feats(word))\n",
    "    if classResult == 'neg':\n",
    "        neg = neg + 1\n",
    "    if classResult == 'pos':\n",
    "        pos = pos + 1\n",
    " \n",
    "print('Positive: ' + str(float(pos)/len(words)))\n",
    "print('Negative: ' + str(float(neg)/len(words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tree ['This', 'is', 'Gayathri', \"'s\", 'book', 'is', \"n't\", 'it', '?']\n",
      "WordPunct ['This', 'is', 'Gayathri', \"'\", 's', 'book', 'isn', \"'\", 't', 'it', '?']\n",
      "porter stemmer\n",
      "Lemmatizer\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import nltk\n",
    "\n",
    "#Tokenize using TreeBank Tokenizer\n",
    "\n",
    "text=\"This is Gayathri's book isn't it?\"\n",
    "\n",
    "tokenizer=nltk.tokenize.TreebankWordTokenizer()\n",
    "tokenizer1=nltk.tokenize.WordPunctTokenizer()\n",
    "\n",
    "tokens=tokenizer.tokenize(text)\n",
    "tokens1=tokenizer1.tokenize(text) #wordPunct\n",
    "\n",
    "print(\"Tree\", tokens)\n",
    "print(\"WordPunct\" ,tokens1)\n",
    "\n",
    "#stemmer\n",
    "stemmer=nltk.stem.PorterStemmer()\n",
    "print(\"porter stemmer\")\n",
    "\" \".join(stemmer.stem(token)for token in tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The quick brown fox jump over the lazy dog'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import nltk\n",
    "\n",
    "#Tokenzie using TreeBank Tokenizer\n",
    "\n",
    "text=\"This is Gayathri's book isn't it?\"\n",
    "\n",
    "lemmatizer=nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "\" \".join(lemmatizer.lemmatize(token)for token in tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'a', 'sample', 'sentence', ',', 'showing', 'off', 'is', 'was', 'kay', 'the', 'stop', 'words', 'filtration', '.']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['This',\n",
       " 'sample',\n",
       " 'sentence',\n",
       " ',',\n",
       " 'showing',\n",
       " 'kay',\n",
       " 'stop',\n",
       " 'words',\n",
       " 'filtration',\n",
       " '.']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "\n",
    "\n",
    "sentence= []\n",
    "def stopword_remove(tokens):\n",
    "      \n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    print(tokens)\n",
    "    for token in tokens:\n",
    "        if token not in stop_words:\n",
    "            sentence.append(token)\n",
    "    return sentence\n",
    "                     \n",
    "    \n",
    "    \n",
    "    \n",
    "example_sent = \"This is a sample sentence, showing off is was kay the stop words filtration.\"\n",
    "\n",
    "tokens = word_tokenize(example_sent)\n",
    "#print(type(tokens))\n",
    "stopword_remove(tokens)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "['a', 't', 'hers', 'do', 'above', 'any', 're', 'up', 'other', 'about', 'doing', 'further', 'then', 'if', 'when', 'myself', \"should've\", 'our', 'we', 'off', 'between', \"wouldn't\", 'because', 'don', 'my', 'being', 'himself', 'down', 'of', 'and', 'for', \"hasn't\", \"you'll\", 'against', 'each', 'them', 'their', 'ma', 'only', 'over', \"you're\", \"shan't\", 'y', 'are', 'o', 'very', 'all', 'your', 'wouldn', 'he', 'below', 'more', 'nor', 'mightn', 'by', 'once', 'while', 'wasn', 'as', \"weren't\", 'yourself', 'who', 'which', 'ourselves', 'ours', 'itself', 'yours', 'herself', 'no', \"won't\", 'haven', 'both', 'not', 'or', 'has', 'what', 'have', 'so', \"needn't\", 'm', 'didn', 'mustn', 'this', 'themselves', 'under', 'you', \"mustn't\", 'be', \"shouldn't\", \"didn't\", \"haven't\", 'same', 'doesn', 'am', 'with', \"wasn't\", 'until', 'she', 'had', 'him', 'it', 'that', 'into', 'shan', 'to', 'just', 'an', 'before', \"she's\", 'such', 'yourselves', 'theirs', 's', 'now', 'but', 'those', \"hadn't\", \"it's\", 'from', 'on', \"couldn't\", 'hadn', \"that'll\", \"don't\", 'is', 'the', 'having', 'out', 'd', 'during', 'at', 'won', 'was', 'here', 'were', 'me', \"isn't\", 'i', 'they', \"aren't\", 'his', 'too', 'there', \"you'd\", 'isn', 'been', \"you've\", 've', 'shouldn', 'again', 'most', 'did', 'should', 'its', 'how', \"doesn't\", 'whom', 'after', 'needn', 'weren', 'in', 'these', 'aren', 'does', 'ain', 'will', 'couldn', 'through', 'why', 'own', 'few', 'hasn', 'can', 'than', 'some', 'll', 'where', 'her', \"mightn't\"]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['This',\n",
       " 'sample',\n",
       " 'sentence',\n",
       " ',',\n",
       " 'showing',\n",
       " 'kay',\n",
       " 'stop',\n",
       " 'words',\n",
       " 'filtration',\n",
       " '.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "\n",
    "\n",
    "sentence= []\n",
    "def stopword_remove(tokens):\n",
    "  \n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    stop_words = list(stop_words)\n",
    "    print(stop_words)\n",
    "    for token in tokens:\n",
    "        if token not in stop_words:\n",
    "            sentence.append(token)\n",
    "    return sentence\n",
    "                     \n",
    "    \n",
    "    \n",
    "    \n",
    "example_sent = \"This is a sample sentence, showing off is was kay the stop words filtration.\"\n",
    "tokens = word_tokenize(example_sent)\n",
    "print(type(tokens))\n",
    "stopword_remove(tokens)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NLTK', 'leading', 'platform', 'building', 'Python', 'programs', 'work', 'human', 'language', 'data', '.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "input_str = \"NLTK is a leading platform for building Python programs to work with human language data.\"\n",
    "stop_words = set(stopwords.words('english'))\n",
    "from nltk.tokenize import word_tokenize\n",
    "tokens = word_tokenize(input_str)\n",
    "result = [i for i in tokens if not i in stop_words]\n",
    "print (result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of stop words: 305\n",
      "First ten stop words: ['everywhere', 'a', 'noone', 'hers', 'do', 'onto', 'another', 'above', 'any', 'somewhere']\n",
      "\n",
      "['NLTK', 'leading', 'platform', 'building', 'Python', 'programs', 'work', 'human', 'language', 'data', '.']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "spacy_nlp = spacy.load('en_core_web_sm')\n",
    "spacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
    "print('Number of stop words: %d' % len(spacy_stopwords))\n",
    "print('First ten stop words: %s' % list(spacy_stopwords)[:10])\n",
    "doc = spacy_nlp('NLTK is a leading platform for building Python programs to work with human language data.')\n",
    "tokens = [token.text for token in doc if not token.is_stop]\n",
    "\n",
    "print()\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'spacy.tokens.doc.Doc'>\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'token' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-ea404cf1237f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;31m#tokenizer=nltk.tokenize.TreebankWordTokenizer()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m     \u001b[0mwater_token\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Subject_and_Complaint'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m     \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mwater_token\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_stop\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m     \u001b[0;31m#result = [i for i in water_token if not i in stop_words]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-30-ea404cf1237f>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;31m#tokenizer=nltk.tokenize.TreebankWordTokenizer()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m     \u001b[0mwater_token\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Subject_and_Complaint'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m     \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mwater_token\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_stop\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m     \u001b[0;31m#result = [i for i in water_token if not i in stop_words]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'token' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords \n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize \n",
    "import spacy\n",
    "spacy_nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "\n",
    "#Data cleaning\n",
    "\n",
    "# 1. unpunctuate \n",
    "# 2. to lower\n",
    "# 3. Remove numerals\n",
    "# 4. Remove Newline for subject\n",
    "\n",
    "\n",
    "# Data loading\n",
    "dataset= pd.read_csv('/home/user/Complaint/MakeComplaint/data.csv')\n",
    "\n",
    "\n",
    "# unpunctuate and lower case\n",
    "dataset['Subject'] = dataset['Subject'].str.replace('[^\\w\\s]','').str.lower()\n",
    "\n",
    "\n",
    "# unpunctuate and lower case\n",
    "dataset['Complaint'] = dataset['Complaint'].str.replace('[^\\w\\s]','').str.lower() \n",
    "\n",
    "\n",
    "#rRemoving new lines in the subject field\n",
    "dataset['Subject'] = dataset['Subject'].str.rstrip('\\n')\n",
    "\n",
    "#removing Numeric \n",
    "dataset['Complaint'] = dataset['Complaint']\n",
    "\n",
    "\n",
    "# creating dataframe for each departments\n",
    "water = dataset.loc[dataset['Departments'] == 'Water Authority']\n",
    "pwd = dataset.loc[dataset['Departments'] == 'PWD']\n",
    "ksrtc = dataset.loc[dataset['Departments'] == 'KSRTC']\n",
    "kseb = dataset.loc[dataset['Departments'] == 'KSEB']\n",
    "env = dataset.loc[dataset['Departments'] == 'Environment and climate change']\n",
    "\n",
    "#print(env.shape)    #(29, 4)\n",
    "#print(water.shape)  #(17, 4)\n",
    "#print(pwd.shape)    #(39, 4)\n",
    "#print(ksrtc.shape)  #(13, 4)\n",
    "#print(kseb.shape)   #(22, 4)\n",
    "#dataset.head()\n",
    "#print(pwd)\n",
    "\n",
    "#Filtering out Subjects and complaints from the dataframe\n",
    "df_water = water[['Subject','Complaint']]\n",
    "df_pwd   = pwd[['Subject','Complaint']]\n",
    "df_ksrtc = ksrtc[['Subject','Complaint']]\n",
    "df_kseb  = kseb[['Subject','Complaint']]\n",
    "df_env   = env[['Subject','Complaint']]\n",
    "\n",
    "dfwater  = df_water[['Subject','Complaint']]\n",
    "dfpwd    = df_pwd[['Subject','Complaint']]\n",
    "dfksrtc  = df_ksrtc[['Subject','Complaint']]\n",
    "dfkseb   = df_kseb[['Subject','Complaint']]\n",
    "dfenv    = df_env[['Subject','Complaint']]\n",
    "\n",
    "\n",
    "#Dataframe with complaint and subject as one column = Water\n",
    "dfwater['Subject_and_Complaint'] = df_water['Subject'] + \" \"+ df_water['Complaint']\n",
    "dfwater=dfwater[['Subject_and_Complaint']]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Dataframe with complaint and subject as one column = PWD\n",
    "dfpwd['Subject_and_Complaint'] = df_pwd['Subject'] + \" \"+ df_pwd['Complaint']\n",
    "dfpwd=dfpwd[['Subject_and_Complaint']]\n",
    "#print(dfpwd)\n",
    "\n",
    "#Dataframe with complaint and subject as one column = ksrtc\n",
    "dfksrtc['Subject_and_Complaint'] = df_ksrtc['Subject'] + \" \"+ df_ksrtc['Complaint']\n",
    "dfksrtc=dfksrtc[['Subject_and_Complaint']]\n",
    "#print(dfksrtc)\n",
    "\n",
    "#Dataframe with complaint and subject as one column = kseb\n",
    "dfkseb['Subject_and_Complaint'] = df_kseb['Subject'] + \" \"+ df_kseb['Complaint']\n",
    "dfkseb=dfkseb[['Subject_and_Complaint']]\n",
    "#print(dfkseb)\n",
    "\n",
    "\n",
    "#Dataframe with complaint and subject as one column = env\n",
    "dfenv ['Subject_and_Complaint'] = df_env['Subject'] + \" \"+ df_env['Complaint']\n",
    "dfenv =dfenv [['Subject_and_Complaint']]\n",
    "#print(dfenv )\n",
    "\n",
    "#==================================Tokenization Begins : =============================================\n",
    "\n",
    "\n",
    "print(type(doc ))\n",
    "#print(tokens)\n",
    "\n",
    "sentence= []\n",
    "\"\"\"def stopword_remove(tokens):\n",
    "  \n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    stop_words = list(stop_words)\n",
    "    \n",
    "    for token in tokens:\n",
    "        if token not in stop_words:\n",
    "            sentence.append(token)\n",
    "    return sentence\"\"\"\n",
    "    \n",
    "                     \n",
    "\n",
    "spacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
    "\n",
    "water_token = []\n",
    "#Tokenising water data\n",
    "stop_words = set(stopwords.words('english'))\n",
    "for i, row in dfwater.iterrows():\n",
    "    #print(i,row['Subject'], row['Complaint'])\n",
    "    #tokenizer=nltk.tokenize.TreebankWordTokenizer()\n",
    "    water_token = word_tokenize(row['Subject_and_Complaint'])\n",
    "    tokens = [token.text for water_token in doc if not token.is_stop]\n",
    "    #result = [i for i in water_token if not i in stop_words]\n",
    "    print(result)\n",
    "    #water_token.append(tokenizer.tokenize(row['Subject_and_Complaint']))\n",
    "\n",
    "  \n",
    "   \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#print(type(tokens))\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "pwd_token = []\n",
    "#Tokenising pwd data    \n",
    "for i, row in dfpwd.iterrows():\n",
    "    #print(i,row['Subject'], row['Complaint'])\n",
    "    tokenizer=nltk.tokenize.TreebankWordTokenizer()\n",
    "    pwd_token.append(tokenizer.tokenize(row['Subject_and_Complaint']))\n",
    "#print( pwd_token)\n",
    "\n",
    "ksrtc_token =[]\n",
    "#Tokenising ksrtc data    \n",
    "for i, row in dfksrtc.iterrows():\n",
    "    #print(i,row['Subject'], row['Complaint'])\n",
    "    tokenizer=nltk.tokenize.TreebankWordTokenizer()\n",
    "    ksrtc_token.append(tokenizer.tokenize(row['Subject_and_Complaint']))\n",
    "#print( ksrtc_token)\n",
    "\n",
    "kseb_token = []\n",
    "#Tokenising kseb data    \n",
    "for i, row in dfkseb.iterrows():\n",
    "    #print(i,row['Subject'], row['Complaint'])\n",
    "    tokenizer=nltk.tokenize.TreebankWordTokenizer()\n",
    "    kseb_token.append(tokenizer.tokenize(row['Subject_and_Complaint']))\n",
    "#print(kseb_token)\n",
    "\n",
    "env_token = []\n",
    "#Tokenising env data    \n",
    "for i, row in dfenv.iterrows():\n",
    "    #print(i,row['Subject'], row['Complaint'])\n",
    "    tokenizer=nltk.tokenize.TreebankWordTokenizer()\n",
    "    env_token.append(tokenizer.tokenize(row['Subject_and_Complaint']))\n",
    "#print(env_token)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================Original==================================================================\n",
      "There is a road going towards Trivandrum Technopark Phase 3 and it has a gate towards the Technopark buildings from this road. It is starting from Nippol Toyota and going towards Kallingal . The road name is Kallingal - Attinkuzhy Road. This Road is damaged for last 1.5 years and no one is caring about this. there are minimum 300 vehicles daily goes through it in the morning and evening towards offices. Now it is like 1 meter gutter in the middle of the road making health issues to the people. It is in very danger situation. URGENT action required.\n",
      "===================================Processed================================================================\n",
      "there is a road going towards trivandrum technopark phase  and it has a gate towards the technopark buildings from this road. it is starting from nippol toyota and going towards kallingal . the road name is kallingal - attinkuzhy road. this road is damaged for last . years and no one is caring about this. there are minimum  vehicles daily goes through it in the morning and evening towards offices. now it is like  meter gutter in the middle of the road making health issues to the people. it is in very danger situation. urgent action required.\n",
      "=========================================Summary==========================================================\n",
      "it is starting from nippol toyota and going towards kallingal .\n",
      "the road name is kallingal - attinkuzhy road.\n",
      "this road is damaged for last .\n",
      "now it is like  meter gutter in the middle of the road making health issues to the people.\n",
      "==================================Keywords=================================================================\n",
      "technopark\n",
      "road\n",
      "meter\n",
      "vehicles daily\n"
     ]
    }
   ],
   "source": [
    "from gensim.summarization import summarize\n",
    "from gensim.summarization import keywords\n",
    "from nltk.corpus import stopwords \n",
    "import nltk\n",
    "import re\n",
    "\n",
    "\n",
    "\n",
    "text = \"There is a road going towards Trivandrum Technopark Phase 3 and it has a gate towards the \"+ \\\n",
    "\"Technopark buildings from this road. It is starting from Nippol Toyota and going towards \"+\\\n",
    "\"Kallingal . The road name is Kallingal - Attinkuzhy Road. This Road is damaged for last 1.5 \"+\\\n",
    "\"years and no one is caring about this. there are minimum 300 vehicles daily goes through it in \"+\\\n",
    "\"the morning and evening towards offices. Now it is like 1 meter gutter in the middle of the road \"+\\\n",
    " \"making health issues to the people. It is in very danger situation. URGENT action required.\"\n",
    "print(\"================================Original==================================================================\")\n",
    "print(text)\n",
    "\n",
    "text = text.strip().lower()\n",
    "\n",
    "#text=text.re.sub('[^a-zA-Z ]',\"\")\n",
    "\n",
    "#text = nltk.sent_tokenize(text)\n",
    "output = re.sub(r'\\d+', '', text)\n",
    "print(\"===================================Processed================================================================\")\n",
    "print(output)\n",
    "\n",
    "print(\"=========================================Summary==========================================================\")\n",
    "print(summarize(output, ratio = 0.5))\n",
    "print(\"==================================Keywords=================================================================\")\n",
    "\n",
    "print(keywords(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['There', 'road', 'going', 'towards', 'Trivandrum', 'Technopark', 'Phase', 'gate', 'towards', 'Technopark', 'buildings', 'road', 'It', 'starting', 'Nippol', 'Toyota', 'going', 'towards', 'Kallingal', 'The', 'road', 'name', 'Kallingal', 'Attinkuzhy', 'Road', 'This', 'Road', 'damaged', 'last', 'years', 'one', 'caring', 'minimum', 'vehicles', 'daily', 'goes', 'morning', 'evening', 'towards', 'offices', 'Now', 'like', 'meter', 'gutter', 'middle', 'road', 'making', 'health', 'issues', 'people', 'It', 'danger', 'situation', 'URGENT', 'action', 'required']\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "#method 2 summarisation\n",
    "from gensim.summarization import summarize\n",
    "from gensim.summarization import keywords\n",
    "from nltk.corpus import stopwords \n",
    "import nltk\n",
    "import re\n",
    "\n",
    "\n",
    "article_text = \"There is a road going towards Trivandrum Technopark Phase 3 and it has a gate towards the \"+ \\\n",
    "\"Technopark buildings from this road. It is starting from Nippol Toyota and going towards \"+\\\n",
    "\"Kallingal . The road name is Kallingal - Attinkuzhy Road. This Road is damaged for last 1.5 \"+\\\n",
    "\"years and no one is caring about this. there are minimum 300 vehicles daily goes through it in \"+\\\n",
    "\"the morning and evening towards offices. Now it is like 1 meter gutter in the middle of the road \"+\\\n",
    " \"making health issues to the people. It is in very danger situation. URGENT action required.\"\n",
    "stop_words = set(stopwords.words('english'))\n",
    "article_text = re.sub(r'\\[[0-9]*\\]', ' ', article_text)  \n",
    "article_text = re.sub(r'\\s+', ' ', article_text)  \n",
    "formatted_article_text = re.sub('[^a-zA-Z]', ' ', article_text )  \n",
    "formatted_article_text = re.sub(r'\\s+', ' ', formatted_article_text)  \n",
    "sentence_list = nltk.sent_tokenize(article_text)  \n",
    "word_frequencies = []\n",
    "tokenize = nltk.word_tokenize(formatted_article_text)\n",
    "\n",
    "tok =[]\n",
    "for word in tokenize:\n",
    "    result = [i for i in tokenize if not i in stop_words]\n",
    "print(result) \n",
    "\n",
    "word_frequencies = {}  \n",
    "for word in  tokenize:\n",
    "     if word not in stop_words:\n",
    "            if word not in word_frequencies.keys():\n",
    "                word_frequencies[word] = 1\n",
    "            else:\n",
    "                word_frequencies[word] += 1\n",
    "                \n",
    "maximum_frequncy = max(word_frequencies.values()) \n",
    "print(maximum_frequncy)\n",
    "for word in word_frequencies.keys(): \n",
    "     word_frequencies[word] = (word_frequencies[word]/maximum_frequncy)\n",
    "        \n",
    "sentence_scores = {} \n",
    "for sent in sentence_list:\n",
    "    for word in nltk.word_tokenize(sent.lower()):if word in word_frequencies.keys():\n",
    "            \n",
    "        if word in word_frequencies.keys():\n",
    "            if len(sent.split(' ')) < 30:\n",
    "                \n",
    "            \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hellomzcsdn,,m\n",
    "df\n",
    "v\n",
    "fd\n",
    "vjjd\n",
    "ds\n",
    "fsnhs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['The', 'Saudis', 'are', 'preparing', 'a', 'report', 'that', 'will', 'acknowledge', 'that'], ['Saudi', 'journalist', 'Jamal', \"Khashoggi's\", 'death', 'was', 'the', 'result', 'of', 'an'], ['interrogation', 'that', 'went', 'wrong,', 'one', 'that', 'was', 'intended', 'to', 'lead'], ['to', 'his', 'abduction', 'from', 'Turkey,', 'according', 'to', 'two', 'sources.']]\n",
      "Dictionary(33 unique tokens: ['Turkey,', 'his', 'that', 'interrogation', 'death']...)\n",
      "{'Turkey,': 26, 'his': 30, 'that': 7, 'interrogation': 20, 'death': 13, 'lead': 21, 'an': 12, 'of': 15, 'to': 23, 'abduction': 27, \"Khashoggi's\": 10, 'according': 28, 'one': 22, 'wrong,': 25, 'went': 24, 'sources.': 31, 'preparing': 5, 'journalist': 14, 'The': 1, 'a': 2, 'acknowledge': 3, 'two': 32, 'the': 17, 'from': 29, 'report': 6, 'Saudi': 11, 'Jamal': 9, 'intended': 19, 'will': 8, 'result': 16, 'was': 18, 'are': 4, 'Saudis': 0}\n",
      "Dictionary(48 unique tokens: ['his', 'that', 'paths', 'interrogation', 'acknowledge']...)\n",
      "{'his': 30, 'that': 7, 'paths': 36, 'interrogation': 20, 'acknowledge': 3, 'death': 13, 'to': 23, 'intended': 19, 'and': 41, 'A': 46, 'Saudi': 11, 'well': 45, 'in': 34, 'abduction': 27, 'IV': 39, 'according': 28, 'two': 32, 'report': 6, 'ordering': 43, 'result': 16, 'intersection': 35, 'are': 4, 'lead': 21, 'Turkey,': 26, 'of': 15, 'an': 12, 'trees': 37, \"Khashoggi's\": 10, 'survey': 47, 'minors': 42, 'one': 22, 'sources.': 31, 'journalist': 14, 'The': 1, 'a': 2, 'graph': 33, 'the': 17, 'from': 29, 'wrong,': 25, 'preparing': 5, 'Widths': 40, 'Graph': 38, 'went': 24, 'will': 8, 'Jamal': 9, 'quasi': 44, 'was': 18, 'Saudis': 0}\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim import corpora\n",
    "from pprint import pprint\n",
    "\n",
    "documents = [\"The Saudis are preparing a report that will acknowledge that\", \n",
    "             \"Saudi journalist Jamal Khashoggi's death was the result of an\", \n",
    "             \"interrogation that went wrong, one that was intended to lead\", \n",
    "             \"to his abduction from Turkey, according to two sources.\"]\n",
    "\n",
    "documents_2 = [\"One source says the report will likely conclude that\", \n",
    "                \"the operation was carried out without clearance and\", \n",
    "                \"transparency and that those involved will be held\", \n",
    "                \"responsible. One of the sources acknowledged that the\", \n",
    "                \"report is still being prepared and cautioned that\", \n",
    "                \"things could change.\"]\n",
    "\n",
    "# Tokenize(split) the sentences into words\n",
    "texts = [[text for text in doc.split()] for doc in documents]\n",
    "\n",
    "print(texts)\n",
    "\n",
    "# Create dictionary\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "\n",
    "print(dictionary)\n",
    "\n",
    "# Show the word to id map\n",
    "print(dictionary.token2id)\n",
    "\n",
    "# adding new doc to dictionary\n",
    "documents_2 = [\"The intersection graph of paths in trees\",\n",
    "               \"Graph minors IV Widths of trees and well quasi ordering\",\n",
    "               \"Graph minors A survey\"]\n",
    "\n",
    "texts_2 = [[text for text in doc.split()] for doc in documents_2]\n",
    "\n",
    "dictionary.add_documents(texts_2)\n",
    "\n",
    "\n",
    "print(dictionary)\n",
    "\n",
    "\n",
    "print(dictionary.token2id)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'apple': 3, 'egg': 2, 'banana': 1})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "list1=['apple','egg','apple','banana','egg','apple']\n",
    "counts = Counter(list1)\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-32-440e5399e1f1>, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-32-440e5399e1f1>\"\u001b[0;36m, line \u001b[0;32m5\u001b[0m\n\u001b[0;31m    for j in len(:\u001b[0m\n\u001b[0m                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "list1 = [['a','b','c'],['a','a','b']]\n",
    "wordfreq = []\n",
    "inner_list=[]\n",
    "for i in len(list1):\n",
    "    for j in len(:\n",
    "        inner_list.append(i.count(j))\n",
    "    wordfreq.append(inner_list)\n",
    "print(wordfreq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 'Group2', 'A': 'Group1', 'D': 'Group2', 'B': 'Group1'}\n"
     ]
    }
   ],
   "source": [
    "groups = [['Group1', 'A', 'B'], ['Group2', 'C', 'D']]\n",
    "\n",
    "result = {}\n",
    "for group in groups:\n",
    "    for item in group[1:]:\n",
    "        result[item] = group[0]\n",
    "print (result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e 30\n",
      "b 13\n",
      "d 4\n",
      "c 2\n",
      "a 1\n",
      "{'f': 4, 'd': 4, 'a': 1, 'c': 2, 'b': 13, 'e': 30}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "a1 = {'a':1, 'b':13, 'd':4, 'c':2, 'e':30}\n",
    "a1_sorted_keys = sorted(a1, key=a1.get, reverse=True)\n",
    "for r in a1_sorted_keys:\n",
    "    print(r, a1[r])\n",
    "a1.update({'f':4})\n",
    "print(a1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('b', 23), ('d', 17), ('c', 5), ('a', 2), ('e', 1)]\n"
     ]
    }
   ],
   "source": [
    "d = {'a':2, 'b':23, 'c':5, 'd':17, 'e':1}\n",
    "items = [(v, k) for k, v in d.items()]\n",
    "items.sort()\n",
    "items.reverse()\n",
    "items = [(k, v) for v, k in items]\n",
    "print(items)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id                             Subject  \\\n",
      "0   1         No water supply connection.   \n",
      "1   2                  Road re tarring.\\n   \n",
      "2   3  Power cuts without a prior notice.   \n",
      "3   4     Scarcity of water in day time.    \n",
      "4   5                      Water scarcity   \n",
      "\n",
      "                                           Complaint      Departments  \n",
      "0  No water pipeline connection in amma gardens r...  Water Authority  \n",
      "1  I am a resident of sreekaryam  Ambadi Nagar la...              PWD  \n",
      "2  As everyone knows that electricity is the majo...             KSEB  \n",
      "3  There is scarcity of water in my area Vanchiyo...  Water Authority  \n",
      "4  There is a huge scarcity of water in remote ar...  Water Authority  \n",
      "   id Subject                                          Complaint  \\\n",
      "0   1          no water pipeline connection in amma gardens r...   \n",
      "1   2          i am a resident of sreekaryam  ambadi nagar la...   \n",
      "2   3          as everyone knows that electricity is the majo...   \n",
      "3   4          there is scarcity of water in my area vanchiyo...   \n",
      "4   5          there is a huge scarcity of water in remote ar...   \n",
      "\n",
      "       Departments  \n",
      "0  Water Authority  \n",
      "1              PWD  \n",
      "2             KSEB  \n",
      "3  Water Authority  \n",
      "4  Water Authority  \n",
      "(30, 4)\n",
      "                                 Subject_and_Complaint\n",
      "12    the number of deaths from cardiovascular dise...\n",
      "13    people make river water polluted by dumping h...\n",
      "19    due to the sand mining there is more of effec...\n",
      "20    in our areas during night some strangers are ...\n",
      "33    fishes in the river pampa are dying massively...\n",
      "40    construction is going on the trivandrum in th...\n",
      "42    many industries dump wastes into rivers  lake...\n",
      "43    burning of plastics in public places cause a ...\n",
      "44    dumping of wastes in the public places causes...\n",
      "45    activities like waste disposal from residenti...\n",
      "46    overfishing  which causes a reduction in dive...\n",
      "47    artificial light at night is one of the most ...\n",
      "48    the electronic waste problem is huge the elec...\n",
      "49    ocean acidification is caused when co₂ dissol...\n",
      "50    noise produced by vehicles  political parties...\n",
      "58    now a days many people are cutting down the t...\n",
      "62    polution by ksrtc bus is very heavy today so ...\n",
      "65    quarries are bad for the environment in sever...\n",
      "70                            save alappad stop mining\n",
      "72    i would like to inform you that city resident...\n",
      "74    preserving biodiversity  saving forest can he...\n",
      "100   intolerable temperature change in summer seas...\n",
      "103                         intense heat during summer\n",
      "110   one among the factors which shaped kerala as ...\n",
      "112   due to excessive use of chlorine and other di...\n",
      "113   we should have more projects on planting tree...\n",
      "114   need proper regulations to control the pollution\n",
      "116   increased deforestation  uncontrolled constru...\n",
      "118   the pollution levels in the kochi city is ris...\n",
      "120   waste disposal in public premises make life h...\n",
      "\n",
      "\n",
      "Water Tokens\n",
      "\n",
      "\n",
      "[['water', 'pipeline', 'connection', 'amma', 'gardens', 'residential', 'area', 'erattakalangu', 'malayinkeezhu'], ['scarcity', 'water', 'area', 'vanchiyoor', 'day', 'time', 'creates', 'great', 'trouble', 'people', 'getting', 'ready', 'work', 'schools', 'office', 'etc'], ['huge', 'scarcity', 'water', 'remote', 'areas', 'people', 'walk', 'large', 'distances', 'standing', 'queue', 'fetch', 'water', 'problem', 'exiss', 'hilly', 'areas', 'idukki', 'district', 'kerala', 'si', 'humbly', 'request', 'take', 'necessary', 'actions', 'tackle', 'problem', 'near', 'future'], ['frequent', 'scarcity', 'water', 'morning'], ['100', 'houses', 'area', 'water', 'supply', 'available', 'therepeople', 'get', 'water', 'needs', 'therefore', 'request', 'concerning', 'authorities', 'look', 'matter', 'seriously', 'take', 'necessary', 'steps', 'solving', 'problem', 'water', 'supply', 'may', 'kindly', 'made', 'daily', 'locality', 'people', 'may', 'get', 'rid', 'problem', 'water', 'supply'], ['stayed', 'hotel', 'kovalam', 'beach', 'called', 'palm', 'grove', 'second', 'beach', 'road', 'kovalam', 'near', 'kovalam', 'bus', 'stand', 'came', 'know', 'hotel', 'water', 'connection', 'without', 'water', 'supply', 'authority', 'sealed', 'pipe', 'big', 'dues', 'bill', 'stealing', 'water', 'sealed', 'pipe', 'last', 'year', 'huge', 'capacity', 'water', 'storage', 'building', 'approx', '20000', 'ilters', 'capacity', 'stayed', 'hotel', '3', 'nights', 'staff', 'told', 'things', 'regarding', 'please', 'take', 'immediate', 'action', 'fix', 'small', 'lengthy', 'pipe', 'two', 'side', 'thread', 'connector', 'connected', 'sealed', 'pipewhich', 'also', 'caped', 'glue', 'closed', 'thread', 'cap', 'usually', 'water', 'supply', 'hardly', '3', '4', 'days', 'per', 'week', 'day', 'steal', 'water', 'fill', '20000', 'liters', 'appox', 'filled', 'use', '1', 'week', 'please', 'take', 'necessary', 'action'], ['water', 'leakage', 'locality', 'still', 'unrepaired', 'causes', 'water', 'shortage', 'areas'], ['water', 'flowing', 'pipe', 'foul', 'smell', 'therefore', 'unfits', 'drinking', 'cooking', 'purposes'], ['amount', 'unauthorised', 'connection', 'water', 'mains', 'cause', 'huge', 'bills', 'authorised', 'consumer'], ['public', 'pipe', 'triivandrum', 'city', 'leaking', 'thus', 'leads', 'water', 'shortage'], ['drinking', 'water', 'pipe', 'near', 'puthoor', 'junction', 'broken', 'water', 'flowing', '2', 'days', 'please', 'take', 'necessary', 'action'], ['shortage', 'drinking', 'water'], ['scarcity', 'drinking', 'water', 'summer'], ['leakage', 'water', 'mg', 'road'], ['please', 'setup', 'enough', 'water', 'resource', 'rural', 'areas'], ['residing', 'edathua', 'panchayat', 'kuttanad', 'taluk', 'staying', 'rented', 'house', 'water', 'connection', 'meter', 'water', 'getting', 'waterour', 'house', 'consumer', 'number', '471eda', 'neighbouring', 'houseconsumer', 'no818eda', 'also', 'getting', 'water', 'well', 'also', 'waterwe', 'facing', 'great', 'difficulties', 'daytoday', 'needs', 'kindly', 'request', 'take', 'necessary', 'action'], ['totally', 'residing', 'attuparambil', 'eravipuram', 'kollam', 'locality', 'facing', 'severe', 'problem', 'growing', 'instances', 'insufficient', 'water', 'supply', 'last', '5', 'days', 'locality', 'already', 'given', 'many', 'complaints', 'kindly', 'requesting', 'look', 'matter', 'personally', 'needful', 'shall', 'much', 'thankful'], ['last', 'one', 'month', 'kochi', 'facing', 'acute', 'shortage', 'water', 'supply', 'kwa', 'supply', 'system', 'summer', 'getting', 'bad', 'worse', 'shortage', 'water', 'aggravating', 'condition'], ['writing', 'inform', 'residents', 'punnakunnam', 'pulinkunnu', 'kuttanadu', 'alappuzha', 'facing', 'shortage', 'drinking', 'water', 'last', '10', 'monthsin', 'fact', 'every', 'member', 'society', 'disturbed', 'account', 'unavailability', 'water', 'supply', 'basic', 'need', 'every', 'human'], ['new', 'water', 'pipe', 'connection', 'installed', 'front', 'house', 'leaking', 'water', 'leaking', 'great', 'force', 'getting', 'wasted'], ['please', 'note', 'informing', 'drinking', 'water', 'keep', 'flowing', 'road', 'always', 'wet', 'compound', 'fully', 'accumulated', 'meter', 'cabin'], ['may', 'one', 'two', 'week', 'dont', 'get', 'water', 'even', 'provide', 'water', 'supply', 'time', 'late', 'like', 'midnight', 'low', 'pressure'], ['please', 'issue', 'good', 'water', 'meter'], ['consumer', 'waterworks', 'sub', 'division', 'thrissur', 'per', 'mentioned', 'consumer', 'number', 'getting', 'water', 'past', 'four', 'months', 'house', 'leakage', 'tiled', 'portion', 'road', 'thrissur', 'corporation', 'noticed', 'reported', 'assistant', 'engineer', 'action', 'taken', 'rectify', 'leakage', 'restore', 'water', 'supply', 'house'], ['since', 'january', 'consuming', 'mudmix', 'water', 'every', 'supply', 'water', 'authority', 'storage', 'tank', 'destroy', 'mud', 'resulted', 'taps', 'bathroom', 'systems', 'also'], ['public', 'tap', 'remains', 'idle', 'past', 'couple', 'weeks', 'affected', 'lives', 'people', 'neighborhood', 'awkwardly'], ['recieved', 'bill', '792', 'rs', 'house', 'closed', 'state', '3', 'months', 'information', 'consumption', 'details', 'previous', 'reading', 'information', 'bill'], ['water', 'supply', 'vazhathope', 'pachayath', 'two', 'weeks', 'people', 'need', 'look', 'upon', 'water', 'sources', 'much', 'frustrating'], ['regular', 'water', 'supply', 'september', '2017', 'onwards', 'till', 'date'], ['water', 'loosing', 'distribution', 'line', 'every', 'day', 'pumping', 'progressthis', 'water', 'trapped', 'walk', 'way', 'public', 'useand', 'time', 'mosqitos', 'growingseveral', 'time', 'iform', 'water', 'athority', 'kochi', 'office'], ['domestic', 'water', 'supply', 'houses', 'situated', 'near', 'kunnathuvathucal', 'bridge', 'last', '3', 'days'], ['name', 'wrongly', 'recorded', 'water', 'connection', 'records'], ['able', 'complete', 'payment', 'transaction', 'water', 'authority', 'website'], ['pay', 'utility', 'bills', 'mostly', 'via', 'online', 'payment', 'services', 'water', 'bill', 'facing', 'problem', 'paying', 'bill', 'water', 'authority', 'website'], ['wastage', 'drinking', 'water', 'due', 'damaged', 'pipeline'], ['trying', 'make', 'bill', 'payment', 'entered', 'credentials', 'tried', 'login', 'account', 'everytime', 'try', 'login', 'get', 'error', 'message'], ['request', 'check', 'meter', 'replace', 'repair', 'early', 'possible'], ['location', 'water', 'connection', 'since', 'summer', 'facing', 'severe', 'water', 'scarcity', 'please', 'set', 'connection', 'loction'], ['pay', 'bill', 'e', 'payment', 'web', 'site', 'charge', 'additional', 'rupees', '10', 'transaction']]\n",
      "\n",
      "\n",
      "PWD Tokens\n",
      "\n",
      "\n",
      "[['resident', 'sreekaryam', 'ambadi', 'nagar', 'lane', '3', 'road', 'full', 'potholes', 'kindly', 'necessary', 'steps', 'maintenance', 'road'], ['condition', 'road', 'rural', 'areas', 'kerala', 'miserable', 'corruption', 'contract', 'maintenance', 'road', 'causing', 'problems', 'leads', 'frequent', 'damage', 'newly', 'constructed', 'tared', 'roads', 'potholes', 'road', 'leads', 'accidents', 'losing', 'balance', 'vehicles', 'paved', 'way', 'damage', 'road'], ['living', 'area', 'road', 'harsh', '500', 'people', 'living', 'area', 'travelling', 'unpaved', 'road', 'difficult', 'taskirequests', 'tar', 'road', 'early', 'possible'], ['due', 'recent', 'landslide', 'activity', 'roads', 'got', 'shattered', 'traffic', 'stopped', 'many', 'days', 'please', 'make', 'tarring', 'process', 'faster'], ['mr', 'kumar', 'locality', 'constructing', 'wall', 'taking', 'footpath', 'pwd'], ['road', 'works', 'without', 'prior', 'notice', 'day', 'time', 'cause', 'heavy', 'traffic', 'congestion'], ['melattumoozhy', 'thannippara', 'road', 'bad', 'condition', 'accidents', 'increasing', 'day', 'day', 'please', 'take', 'necessary', 'actions', 'retarring', 'process'], ['road', 'sreekaryam', 'proper', 'width', 'well', 'maintained'], ['id', 'like', 'make', 'public', 'works', 'department', 'aware', 'number', 'potholes', 'impeding', 'traffic', 'also', 'causing', 'undue', 'wear', 'tear', 'vehicles', 'hitting', 'pothole', 'even', 'speed', 'limit', 'pop', 'tire', 'damage', 'car', 'even', 'pose', 'physical', 'threat', 'drivers', 'pedestrians', 'know', 'state', 'law', 'permits', 'drivers', 'seek', 'reimbursement', 'losses', 'due', 'road', 'hazards', 'defects', 'result', 'negligence', 'potential', 'liability', 'claims', 'high', 'several', 'potholes', 'located', 'maple', 'street', '12th', '13th', 'streets', 'one', 'least', 'two', 'feet', 'diameter', 'intersection', '13th', 'pine', 'know', 'crew', 'cant', 'get', 'every', 'pothole', 'right', 'away', 'growing', 'many', 'months', 'attached', 'photos', 'illustrate', 'problem'], ['roads', 'safe', 'running', 'heavyweight', 'vehicle', 'poor', 'maintenance', 'problem', 'aggravates', 'especially', 'rainy', 'season', 'major', 'problem', 'india', 'roads', 'high', 'traffic', 'condition', 'road', 'used', 'various', 'types', 'vehicles', 'highspeed', 'trucks', 'cars', 'tractors', 'twowheelers', 'driven', 'carts', 'cyclists', 'etc', 'things', 'create', 'high', 'traffic', 'jam', 'congestion', 'road', 'accident', 'etcthe', 'roads', 'india', 'lack', 'wayside', 'amenities', 'like', 'first', 'aid', 'centers', 'telephone', 'booths', 'repair', 'shops', 'restaurants', 'clean', 'toilets', 'create', 'serious', 'problems', 'drivers', 'moreover', 'little', 'attention', 'given', 'road', 'safety', 'issues', 'also', 'road', 'safety', 'rules', 'violation', 'laws', 'road', 'transportation', 'system', 'country', 'requires', 'immediate', 'modernization', 'latest', 'technology', 'road', 'transport', 'sector', 'use', 'old', 'technology', 'vehicles', 'still', 'prevalent', 'ultimately', 'increase', 'number', 'road', 'accidents', 'due', 'bad', 'road', 'condition', 'transport', 'companies', 'bear', 'huge', 'amount', 'per', 'year', 'wear', 'tear', 'vehicles', 'important', 'point', 'proper', 'attention', 'given', 'road', 'direction', 'railway', 'department', 'always', 'indifferent', 'attitude', 'biggest', 'barrier', 'transport', 'india', 'railway', 'crossings', 'despite', 'government', 'permissions', 'government', 'employees', 'make', 'reasonable', 'contributions', 'even', 'bribe', 'timeconsuming', 'transport', 'traders', 'also', 'large', 'businessmen', 'businessmen', 'also', 'raised'], ['roads', 'many', 'pitfalls', 'sometimes', 'cause', 'accidents', 'unwary', 'drivers'], ['rising', 'peakhour', 'traffic', 'congestion', 'inescapable', 'condition', 'many', 'areas', 'trivandrum', 'city', 'sufficient', 'traffic', 'control', 'signal', 'systems', 'handle', 'situations'], ['roads', 'properly', 'maintained', 'repair', 'works', 'roads', 'last', 'days', 'proper', 'transportation', 'facility', 'basic', 'right', 'citizens'], ['actually', 'dont', 'good', 'roads', 'worthy', 'tax', 'pay', 'hopefully', 'government', 'find', 'way', 'fix', 'thanks'], ['pathholes', 'road', 'extremly', 'half', 'feet', 'size', 'accident', 'prone', 'today', 'effect', 'daily', 'travelling', 'student', 'employees', 'suffering', 'also', 'may', 'cause', 'health', 'problems'], ['drainage', 'work', 'muthambioorallur', 'road', 'thadoli', 'thazha', 'junction', 'koyilandy', 'taluk', 'calicut', 'district', 'length', '100', 'mtr', 'progressing', 'outlet', 'crossing', 'road', 'directed', 'towards', 'compound', 'wall', 'letting', 'water', 'flow', 'nayadan', 'puzha', 'lake', 'completion', 'water', 'flow', 'directly', 'bottom', 'compound', 'wall', 'hitting', 'compound', 'wall', 'changes', 'direction', 'flow', 'damage', 'wall', 'nature', 'place', 'loose', 'mud', 'learnt', 'provision', 'continue', 'drainage', 'limit', 'road', 'requested', 'kindly', 'needful', 'avoid', 'damage', 'wall', 'length', '15', 'mtr', 'early', 'action', 'requested', 'canal', 'water', 'open', 'last', 'january'], ['sir', 'please', 'initiate', 'surprise', 'visit', 'kerala', 'know', 'conditions', 'roads', '6', 'monthsafter', 'rainy', 'season', 'since', 'roads', 'repaired', 'also', 'roads', 'broken', 'instead', 'chipping', 'existing', 'roads', 'resurfaced', 'increases', 'height', 'roads', 'height', 'roads', 'increased', 'height', 'houses', 'office', 'complexes', 'increased', 'know', 'take', 'help', 'media', 'channels', 'like', 'asianet', 'news', 'manorama', 'news', 'indiavision', 'etc', 'also', 'common', 'trend', 'ie', 'roads', 'repaired', 'broken', 'manually', 'underlying', 'cables', 'underground', 'left', 'without', 'repairing', 'sirmadam', 'per', 'day', 'lacs', 'vehicles', 'entering', 'roads', 'showrooms', 'much', 'tax', 'collected', 'isnt', 'sufficient', 'repair', 'roads', 'roads', 'leveled', 'properly', 'much', 'fuel', 'saved', 'prices', 'fuel', 'controlled', 'certain', 'extent', 'right', 'hope', 'would', 'take', 'factors', 'consideration'], ['dear', 'sir', 'due', 'intervention', 'politicians', 'every', 'footpath', 'flooded', 'vendors', 'selling', 'tender', 'cocanuts', 'shoe', 'repairs', 'petty', 'shops', 'filled', 'pan', 'masalas', 'liquiors', 'tea', 'snacks', 'notnot', 'traders', 'blocking', 'view', 'side', 'roads', 'motorist', 'predict', 'see', 'going', 'emerge', 'side', 'roads', 'petty', 'shops', 'well', 'concreated', 'base', 'running', 'businesses', 'like', 'real', 'estate', 'rentals', 'etcetc', 'idea', 'growing', 'nation', 'city', 'clean', 'roads', 'responsible', 'pwd', 'authority', 'issue', 'notice', 'people', 'evict', 'themgod', 'bad', 'country', 'pity', 'consideration', 'appeal', 'something', 'stop'], ['heavy', 'traffic', 'ss', 'kovil', 'road', 'thampanoor', 'large', 'amount', 'vehicle', 'parking'], ['connection', 'othukkungal', 'town', 'malappuram', 'district', 'kerala', 'beautification', 'existing', 'bus', 'waiting', 'shed', 'towards', 'kottakkal', 'removed', 'last', 'year', 'till', 'date', 'new', 'shed', 'constructed', 'makes', 'difficult', 'common', 'passengers', 'kindly', 'take', 'necessary', 'action', 'construction', 'waiting', 'shed', 'helpful', 'layman'], ['road', 'sreekaryam', 'ambadi', 'nagar', 'chithravila', 'lot', 'gutter', 'please', 'necessary', 'action', 'retar', 'road'], ['road', 'kotooli', 'mangotu', 'vayal', 'road', 'completely', 'digged', 'middle', 'pregnant', 'ladies', 'may', 'give', 'delivery', 'go', 'road', 'slippery', 'chance', 'high', 'please', 'something', 'kind', 'accidents', 'lot', 'kids', 'also', 'passing', 'road', 'chances', 'hitting', 'kids', 'driver', 'try', 'avoid', 'digged', 'part', 'middle', 'road', 'make', 'pedestrians', 'edge', 'may', 'result', 'hit', 'fall', 'resulting', 'injury', 'please', 'consider', 'solve', 'early', 'possible'], ['road', 'going', 'towards', 'trivandrum', 'technopark', 'phase', '3', 'gate', 'towards', 'technopark', 'buildings', 'road', 'starting', 'nippol', 'toyota', 'going', 'towards', 'kallingal', 'road', 'name', 'kallingal', 'attinkuzhy', 'road', 'road', 'damaged', 'last', '15', 'years', 'one', 'caring', 'minimum', '300', 'vehicles', 'daily', 'goes', 'morning', 'evening', 'towards', 'offices', 'like', '1', 'meter', 'gutter', 'middle', 'road', 'making', 'health', 'issues', 'people', 'danger', 'situation', 'urgent', 'action', 'required'], ['current', 'driving', 'licence', 'poor', 'make', 'like', 'pvc', 'card', 'digital', 'card'], ['roads', 'villages', 'proper', 'condition', 'people', 'travelplease', 'take', 'necessary', 'action', 'regarding'], ['road', 'full', 'pits'], ['submitted', 'road', 'condition', 'karukaputhurpallipadamthichur', 'palakkad', 'distis', 'pathetic', 'condition', 'even', 'suitable', 'footwalk', 'due', 'boarder', 'villages', 'two', 'constituency', 'mla', 'bothers', 'area', 'please', 'needful', 'earliest'], ['sir', 'public', 'wants', 'know', 'pwd', 'still', 'continuing', 'toll', 'charge', 'poovathumkadavu', 'bridge', 'poor', 'maintenance', 'road', 'street', 'light', 'misbehaviour', 'toll', 'collectors', 'passengers', 'every', 'day', 'public', 'suffering', 'area', 'whole', 'money', 'bridge', 'collected', 'public', 'gst', 'road', 'tax', 'insurance', 'money', 'going', 'nearby', 'mathilakam', 'bridge', 'toll', 'fee', 'stopped', '1', 'year', 'back'], ['road', 'ramamangalam', 'choondy', 'dug', 'works', 'related', 'kwh', 'condition', 'travel', 'used', 'travel', 'around', '40', 'km', 'daily', 'include', 'road', 'also'], ['register', 'complaint', 'poor', 'road', 'conditions', 'big', 'pot', 'holes', 'arookutty', 'aroor', 'road', 'alappuzha', 'district'], ['damaged', 'long', 'duration', 'potholes', 'converted', 'hidden', 'hazards', 'bikers', 'potential', 'risk', 'road', 'accidents'], ['roads', 'city', 'damaged', 'quality', 'needed'], ['keshavadasapuram', 'traffic', 'block', 'lack', 'planning'], ['low', 'quality', 'roads'], ['drivers', 'dont', 'know', 'correct', 'times', 'roots', 'many', 'bus', 'times'], ['tarring', 'roads', 'damaged', 'suddenly', 'reason', 'behind'], ['road', 'maintained', 'properly'], ['many', 'drivers', 'reckless', 'follow', 'road', 'rules', 'law', 'enforcement', 'lax', 'violations', 'makes', 'things', 'difficult', 'drivers', 'unpredictable', 'driving', 'others', 'road', 'pedestrians', 'walking', 'roadside', 'crossing', 'roads', 'makes', 'roads', 'dangerous', 'ordinary', 'citizens'], ['proper', 'maintenance', 'roads', 'leads', 'pot', 'holes', 'roads'], ['everybody', 'knows', 'highway', 'connecting', 'kochi', 'muvattupzha', 'one', 'busiest', 'crowed', 'road', 'ernakulam', 'city', 'withing', 'last', '3', 'years', 'steady', 'increase', 'unauthorized', 'road', 'side', 'business', 'establishments', 'small', 'tent', 'establishments', 'unauthorized', 'illegal', 'creating', 'lot', 'accidents', 'past', 'months', 'cars', 'bikes', 'tent', 'stop', 'immediately', 'see', 'shacks', 'created', 'sudden', 'unexpected', 'accidents', 'build', 'road', 'reduce', 'road', 'visibility', 'example', 'shack', 'next', 'peruvammuzhy', 'selling', 'fish', 'created', 'countless', 'accidents', 'hope', 'department', 'takes', 'actions', 'business', 'establishments', 'immediatelythe', 'effected', 'areas', 'kolenchery', 'valakom'], ['pathanapurm', 'pattazhy', 'road', 'maintained', 'road', '12m', 'width', 'plan', 'less', '5'], ['manathoor', 'maniykumpparakarimkunnam', 'road', 'worst', 'condition', 'construction', 'pala', 'thodupuzha', 'road', 'many', 'vehicles', 'used', 'road', 'shortcut', 'road', 'fully', 'destroyed', 'please', 'retarr', 'road', 'implement', 'proper', 'sign', 'boards', 'road', 'markings']]\n",
      "[['number', 'deaths', 'cardiovascular', 'disease', 'attributed', 'air', 'pollution', 'much', 'higher', 'expectedair', 'pollution', 'caused', 'twice', 'many', 'deaths', 'cvd', 'respiratory', 'diseases'], ['people', 'make', 'river', 'water', 'polluted', 'dumping', 'household', 'wastage', 'industrial', 'wastage'], ['due', 'sand', 'mining', 'effect', 'ecosystem', 'severe', 'impact', 'plants', 'animals', 'rivers'], ['areas', 'night', 'strangers', 'dumping', 'hotel', 'waste', 'domestic', 'waste', 'foul', 'smelling', 'sides', 'road', 'wastes', 'remain', 'th', 'road', 'uncleaned', 'camera', 'survelliance', 'facility', 'catch', 'people', 'throwing', 'wastes'], ['fishes', 'river', 'pampa', 'dying', 'massively', 'due', 'deposits', 'oil', 'factories', 'nearby'], ['construction', 'going', 'trivandrum', 'paddy', 'fields', 'please', 'take', 'necessary', 'steps', 'save', 'farming'], ['many', 'industries', 'dump', 'wastes', 'rivers', 'lakes', 'ponds', 'streams', 'attempt', 'hide', 'wastes', 'epa', 'nspectors', 'water', 'sources', 'feed', 'major', 'crops', 'food', 'becomes', 'contaminated', 'variety', 'chemicals', 'bacteria', 'causing', 'rampant', 'health', 'problems'], ['burning', 'plastics', 'public', 'places', 'cause', 'major', 'health', 'concern', 'rate', 'lung', 'cancer', 'patients', 'increasing', 'day', 'day', 'high', 'time', 'check', 'activities'], ['dumping', 'wastes', 'public', 'places', 'causes', 'major', 'health', 'issues', 'inviting', 'eradicated', 'disease', 'mechanism', 'collect', 'wastes', 'households', 'disposed', 'properly'], ['activities', 'like', 'waste', 'disposal', 'residential', 'commercial', 'industrial', 'areas', 'oil', 'spills', 'runoff', 'agriculture', 'contaminate', 'bodies', 'water'], ['overfishing', 'causes', 'reduction', 'diversity', 'marine', 'life', 'fishermen', 'considering', 'breeding', 'time'], ['artificial', 'light', 'night', 'one', 'obvious', 'physical', 'changes', 'humans', 'made', 'biosphere', 'artificial', 'light', 'also', 'affects', 'dispersal', 'orientation', 'migration', 'hormone', 'levels', 'resulting', 'disrupted', 'circadian', 'rhythms'], ['electronic', 'waste', 'problem', 'huge', 'electronics', 'end', 'landfills', 'toxics', 'like', 'lead', 'mercury', 'cadmium', 'leach', 'soil', 'water'], ['ocean', 'acidification', 'caused', 'co₂', 'dissolves', 'ocean', 'bonding', 'sea', 'water', 'creating', 'carbonic', 'acid', 'acid', 'reduces', 'ph', 'levels', 'water'], ['noise', 'produced', 'vehicles', 'political', 'parties', 'religious', 'centers', 'causes', 'great', 'harm', 'human', 'ears'], ['days', 'many', 'people', 'cutting', 'trees', 'purposes', 'example', 'many', 'oragansations', 'builders', 'cut', 'trees', 'build', 'projects', 'cause', 'real', 'harm', 'humans', 'also', 'utilizing', 'fertile', 'paddy', 'fields', 'construction', 'affect', 'environment', 'oxygen', 'content', 'air', 'become', 'low', 'oxygen', 'content', 'level', 'decreases', 'air', 'survival', 'living', 'beings', 'move', 'harder', 'way', 'also', 'major', 'factor', 'ozone', 'depletion', 'ozone', 'holes', 'formed', 'harmful', 'radiations', 'enter', 'earth', 'holes', 'causes', 'real', 'harm', 'living', 'beings', 'continues', 'way', 'threatening', 'part', 'lives', 'please', 'take', 'serious', 'issue', 'make', 'necessary', 'useful', 'remedies'], ['polution', 'ksrtc', 'bus', 'heavy', 'today', 'government', 'fix', 'soon', 'possible'], ['quarries', 'bad', 'environment', 'several', 'ways', 'abruptly', 'interrupt', 'continuity', 'open', 'space', 'cause', 'soil', 'erosion', 'air', 'dust', 'pollution', 'deterioration', 'water', 'quality', 'residential', 'area', 'create', 'noise', 'hazards', 'request', 'higher', 'authority', 'investigate', 'punish', 'officials', 'hand', 'quarrying', 'illegal', 'mining', 'time', 'government', 'create', 'awareness', 'potentially', 'negative', 'impact', 'quarrying'], ['save', 'alappad', 'stop', 'mining'], ['would', 'like', 'inform', 'city', 'residents', 'facing', 'difficulties', 'dump', 'domestic', 'wastes', 'humbly', 'request', 'take', 'necessary', 'actions'], ['preserving', 'biodiversity', 'saving', 'forest', 'help', 'overcome', 'issuesalso', 'recycling', 'avoiding', 'usage', 'plastic', 'efficient', 'use', 'fuel'], ['intolerable', 'temperature', 'change', 'summer', 'season', 'cause', 'dried', 'rivers', 'skin', 'disease'], ['intense', 'heat', 'summer'], ['one', 'among', 'factors', 'shaped', 'kerala', 'gods', 'country', 'euphoric', 'climate', 'never', 'touched', 'extremes', 'thanks', 'moderating', 'influence', 'sea', 'mighty', 'western', 'ghats', 'scenario', 'changed', 'lot', 'kerala', 'witnessing', 'unprecedental', 'surge', 'temperature', 'kumbhachoodu', 'termed', 'old', 'generation', 'time', 'high', 'impacting', 'livelihood', 'many'], ['due', 'excessive', 'use', 'chlorine', 'disinfectants', 'water', 'causing', 'long', 'term', 'health', 'problems', 'like', 'hair', 'fall', 'rashes', 'etc', 'amount', 'disinfectants', 'used', 'water', 'supplied', 'need', 'reconsidered'], ['projects', 'planting', 'trees', 'dont', 'cut', 'big', 'grownup', 'trees'], ['need', 'proper', 'regulations', 'control', 'pollution'], ['increased', 'deforestation', 'uncontrolled', 'constructiondevelopment', 'activities', 'vehicle', 'emissions', 'heavily', 'contribute', 'increase', 'temperature', 'government', 'build', 'new', 'policies', 'also', 'consider', 'switching', 'renewable', 'energy', 'sources'], ['pollution', 'levels', 'kochi', 'city', 'rising', 'rapidly', 'government', 'initiate', 'steps', 'curb'], ['waste', 'disposal', 'public', 'premises', 'make', 'life', 'hard', 'people', 'living', 'around']]\n",
      "\n",
      "\n",
      " Env Count \n",
      "\n",
      "\n",
      "<class 'dict'>\n",
      "\n",
      "\n",
      " KSEB Count \n",
      "\n",
      "\n",
      "Counter({'power': 9, 'electricity': 7, 'time': 6, 'connection': 5, 'working': 4, 'got': 4, 'wires': 4, 'issue': 3, 'cuts': 3, 'please': 3, 'voltage': 3, 'due': 3, 'problem': 3, 'even': 3, 'facing': 3, 'high': 3, 'sector': 2, 'load': 2, 'electric': 2, 'every': 2, 'bribe': 2, 'one': 2, 'prior': 2, 'home': 2, 'many': 2, 'house': 2, 'bad': 2, 'transformer': 2, 'night': 2, 'make': 2, 'us': 2, 'lights': 2, 'locality': 2, 'without': 2, 'new': 2, 'shedding': 2, 'phase': 2, 'meter': 2, 'last': 2, 'months': 2, 'supply': 2, 'failure': 2, 'day': 2, 'repair': 2, 'using': 2, 'grabbing': 1, 'ensurde': 1, 'values': 1, 'came': 1, 'times': 1, 'public': 1, 'unable': 1, 'needed': 1, 'experiences': 1, 'fast': 1, 'give': 1, 'industries': 1, 'use': 1, 'major': 1, 'particular': 1, 'state': 1, 'lot': 1, 'normalplease': 1, 'almost': 1, 'procedure': 1, 'force': 1, 'trouble': 1, 'madavoor': 1, 'sure': 1, 'affect': 1, '15': 1, 'must': 1, 'evening': 1, 'morning': 1, 'adjoining': 1, 'easily': 1, 'connectionplease': 1, 'though': 1, 'rectified': 1, 'phases': 1, 'overhead': 1, 'connected': 1, 'fix': 1, 'drunken': 1, 'daysplease': 1, 'look': 1, 'announcements': 1, 'street': 1, 'equipments': 1, 'take': 1, 'electronic': 1, 'avoid': 1, 'low': 1, 'necessary': 1, 'left': 1, 'get': 1, 'three': 1, 'wrongly': 1, 'first': 1, 'frequently': 1, 'going': 1, 'old': 1, 'applied': 1, 'regular': 1, '3': 1, 'recently': 1, 'connections': 1, 'request': 1, 'peak': 1, 'notice': 1, 'employees': 1, 'everyone': 1, 'electrical': 1, 'till': 1, 'sound': 1, 'trivandrum': 1, 'daysremaining': 1, 'examinations': 1, 'heavy': 1, 'set': 1, 'burnt': 1, 'activities': 1, 'farming': 1, 'unnoticed': 1, 'lineman': 1, 'month': 1, 'failures': 1, 'scenario': 1, 'well': 1, 'complaint': 1, 'rs': 1, 'always': 1, 'section': 1, 'shortage': 1, 'result': 1, 'unsocial': 1, 'past': 1, 'frequent': 1, 'action': 1, 'hot': 1, '700': 1, 'big': 1, 'devices': 1, 'knows': 1, 'ease': 1, 'difficult': 1, 'checking': 1, 'showing': 1, 'cut': 1, 'find': 1, 'reason': 1, 'makes': 1, 'platforms': 1, 'say': 1, 'crawl': 1, 'houses': 1, 'misbehaved': 1, 'money': 1, 'fastly': 1, 'peoples': 1, 'different': 1, 'highly': 1, 'summer': 1, 'occurring': 1, 'failed': 1, 'unavailability': 1, 'continues': 1, 'current': 1, 'large': 1, 'since': 1, 'robbery': 1, 'appliances': 1, 'raining': 1, 'near': 1, 'communication': 1, 'condition': 1, 'still': 1, 'overall': 1, 'exploded': 1, 'havent': 1, 'replace': 1, 'comes': 1, 'damaged': 1, 'loss': 1, 'corrupt': 1, 'city': 1, 'years': 1, 'proper': 1})\n",
      "\n",
      "\n",
      " KSRTC Count \n",
      "\n",
      "\n",
      "Counter({'bus': 14, 'buses': 10, 'ksrtc': 8, 'service': 5, 'seat': 4, 'college': 4, 'drivers': 4, 'seats': 4, 'car': 4, 'stop': 4, 'would': 4, 'reach': 4, 'services': 4, 'time': 4, 'city': 4, 'students': 3, 'problem': 3, 'route': 3, 'passengers': 3, 'due': 3, 'recently': 3, 'even': 3, 'public': 3, 'around': 3, 'people': 3, 'late': 3, 'railway': 2, 'take': 2, 'lots': 2, 'back': 2, 'morning': 2, 'passenger': 2, 'request': 2, 'fast': 2, 'provide': 2, 'bothered': 2, 'see': 2, 'atleast': 2, 'hours': 2, 'got': 2, 'helpful': 2, 'area': 2, 'overspeeding': 2, 'lack': 2, 'going': 2, 'driver': 2, 'transport': 2, 'road': 2, 'action': 2, 'commute': 2, 'dont': 2, 'number': 2, 'kumarakam': 2, 'reserved': 2, 'busses': 2, 'kottukunnam': 1, 'argue': 1, 'kills': 1, 'safer': 1, 'earlier': 1, 'week': 1, 'life': 1, 'muvattupuza': 1, 'drove': 1, 'standing': 1, 'arrange': 1, 'caused': 1, 'mukkam': 1, 'done': 1, 'rushy': 1, 'reduced': 1, 'busesplease': 1, 'give': 1, 'buseskindly': 1, 'use': 1, 'solely': 1, 'like': 1, 'forget': 1, 'found': 1, 'almost': 1, 'one': 1, 'much': 1, 'choose': 1, 'pasengers': 1, 'income': 1, 'several': 1, 'rural': 1, 'venjaramood': 1, 'limited': 1, 'raining': 1, 'least': 1, 'usually': 1, 'heavily': 1, 'came': 1, 'rudely': 1, 'onky': 1, 'carelessly': 1, 'town': 1, 'willing': 1, 'night': 1, 'change': 1, 'needful': 1, 'value': 1, 'safety': 1, 'cancelled': 1, 'rashly': 1, 'venjaramoodu': 1, 'sir': 1, 'given': 1, 'ymcacross': 1, 'hill': 1, 'timings': 1, 'cottton': 1, '730': 1, 'females': 1, 'mid': 1, 'traveller': 1, 'schedule': 1, 'day': 1, 'kazhakkuttam': 1, 'balance': 1, 'low': 1, 'intended': 1, 'crossing': 1, 'risk': 1, 'get': 1, 'amount': 1, 'behave': 1, 'dashed': 1, 'station': 1, 'dependable': 1, 'us': 1, 'school': 1, 'necessary': 1, 'kattakkada': 1, 'person': 1, 'please': 1, 'arrive': 1, 'hoping': 1, 'depend': 1, 'inconvenience': 1, 'behaving': 1, '25': 1, 'hit': 1, 'cant': 1, 'increase': 1, 'conductors': 1, 'upon': 1, 'kindly': 1, 'regular': 1, 'causes': 1, 'accidents': 1, 'rescheduling': 1, 'trivandrum': 1, 'femalesas': 1, 'kottayam': 1, 'vehicles': 1, 'right': 1, 'causing': 1, 'word': 1, 'kl157912': 1, 'frequently': 1, 'damage': 1, 'access': 1, 'depending': 1, 'ask': 1, 'brutally': 1, 'local': 1, 'travellers': 1, 'tired': 1, 'conductor': 1, 'busy': 1, 'kulathurstudents': 1, 'well': 1, 'equality': 1, 'travel': 1, 'feelplease': 1, 'femalesthen': 1, 'always': 1, 'class': 1, 'fully': 1, 'past': 1, 'control': 1, 'sreekaryam': 1, 'proper': 1, 'never': 1, 'spend': 1, 'totally': 1, 'percent': 1, 'work': 1, 'difficult': 1, 'reaching': 1, 'ignoring': 1, 'face': 1, 'travelyesterday': 1, 'add': 1, 'via': 1, 'convenient': 1, 'reason': 1, 'student': 1, 'ernakulam': 1, 'kattakada': 1, 'travelling': 1, 'authorites': 1, 'daily': 1, 'inform': 1, 'stopping': 1, 'miss': 1, 'experienced': 1, 'six': 1, 'shortcuts': 1, 'filled': 1, 'sometimes': 1, 'facing': 1, 'travelled': 1, 'workers': 1, 'super': 1, 'lady': 1, 'leave': 1, 'took': 1, 'present': 1, 'busesatleast': 1, 'none': 1, 'keep': 1, 'lines': 1, 'advice': 1, 'government': 1, 'alloted': 1, 'running': 1, 'nadakkavu': 1, 'make': 1, 'effective': 1, 'occupied': 1, 'times': 1, 'man': 1, 'noticed': 1, 'others': 1, 'innocent': 1, 'trivandrumidukkikattappana': 1, '2': 1, 'great': 1, 'using': 1, 'transportation': 1, 'difficulty': 1, 'confirm': 1, 'male': 1, 'speed': 1, '23': 1, 'ran': 1, 'destination': 1, 'stationso': 1, 'locations': 1, 'areas': 1, 'stopped': 1, 'whole': 1})\n",
      "\n",
      "\n",
      " PWD Count \n",
      "\n",
      "\n",
      "<class 'collections.Counter'>\n",
      "\n",
      "\n",
      " KSRTC Count \n",
      "\n",
      "\n",
      "<class 'collections.Counter'>\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "dictionary update sequence element #0 has length 1; 2 is required",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-f62624278131>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    715\u001b[0m \u001b[0;31m#x.most_repeated_keyword_lib(dfenv)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    716\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 717\u001b[0;31m \u001b[0mwater_lis\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0menv_lis\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpwd_lis\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mksrtc_lis\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mkseb_lis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    718\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    719\u001b[0m \u001b[0mkeywords\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-f62624278131>\u001b[0m in \u001b[0;36mjump\u001b[0;34m(self, status)\u001b[0m\n\u001b[1;32m    526\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 528\u001b[0;31m             \u001b[0mwater_lis\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpwd_lis\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mksrtc_lis\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mkseb_lis\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0menv_lis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_repeated_keywords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwater_freq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpwd_freq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mksrtc_freq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mkseb_freq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0menv_freq\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    529\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwater_lis\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0menv_lis\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpwd_lis\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mksrtc_lis\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mkseb_lis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-f62624278131>\u001b[0m in \u001b[0;36mmost_repeated_keywords\u001b[0;34m(self, water_freq, pwd_freq, ksrtc_freq, kseb_freq, env_freq)\u001b[0m\n\u001b[1;32m    279\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlists\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwater_freq\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m             \u001b[0mwater_freq_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 281\u001b[0;31m             \u001b[0mlists\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlists\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    282\u001b[0m             \u001b[0mitems\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlists\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m             \u001b[0mitems\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: dictionary update sequence element #0 has length 1; 2 is required"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords \n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize \n",
    "from collections import Counter\n",
    "from gensim.summarization import keywords\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "import spacy\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "global dataset\n",
    "\n",
    "\n",
    "class Main:\n",
    "         \n",
    "    global departments\n",
    "    \n",
    "    \n",
    "    def __init__(self,dataset):\n",
    "        #/home/gayathri/project/MakeComplaint/data.csv\n",
    "        self.dataset=dataset\n",
    "        #print(self.dataset)\n",
    "        self.dataset= pd.read_csv(self.dataset)\n",
    "        \n",
    "        pass\n",
    "    \n",
    "    def department_class(self):\n",
    "        departments=self.dataset['Departments'].unique() \n",
    "        return departments\n",
    "        \n",
    "    \n",
    "    def punctuate(self):\n",
    "        punctuations = '''!()-[]{};:'\"\\,.<>/?@#$%^&*_~'''\n",
    "        text=self.dataset['Subject']\n",
    "\n",
    "        # To take input from the user\n",
    "        # my_str = input(\"Enter a string: \")\n",
    "\n",
    "        # remove punctuation from the string\n",
    "        no_punct = \"\"\n",
    "        no_punctuate =[]\n",
    "        for char in  self.dataset['Subject']:\n",
    "\n",
    "            if char not in punctuations:\n",
    "                no_punct = no_punct + char\n",
    "                no_punctuate.append(no_punct)\n",
    "\n",
    "        # display the unpunctuated string\n",
    "        #print(no_punctuate)\n",
    "        return(no_punctuate)\n",
    "\n",
    "        \n",
    "    def data_clean(self):\n",
    "        \n",
    "        \n",
    "       \n",
    "        print(self.dataset.head())\n",
    "        # unpunctuate and lower case\n",
    "        self.dataset['Subject'] = self.dataset['Subject'].str.replace('[^\\w\\s]','').str.lower()\n",
    "\n",
    "\n",
    "            # unpunctuate and lower case\n",
    "        self.dataset['Complaint'] =  self.dataset['Complaint'].str.replace(',',' ').str.lower() \n",
    "        self.dataset['Subject'] = self.dataset['Subject'].str.replace('[^\\w\\s]','').str.lower()\n",
    "        self.dataset['Complaint'] = self.dataset['Complaint'].str.replace('[^\\w\\s]','').str.lower()\n",
    "        self.dataset['Subject'] = self.dataset['Subject'].str.replace('[^\\P{P}-]','').str.lower()\n",
    "        #print( self.dataset.head())\n",
    "\n",
    "\n",
    "\n",
    "        #rRemoving new lines in the subject field\n",
    "        self.dataset['Subject'] =  self.dataset['Subject'].str.rstrip('\\n')\n",
    "\n",
    "        #removing Numeric \n",
    "        self.dataset['Complaint'] =  self.dataset['Complaint']\n",
    "        print( self.dataset.head())\n",
    "        \n",
    "    def dataframing(self,dataset):\n",
    "        # creating dataframe for each departments\n",
    "        water = dataset.loc[dataset['Departments'] == 'Water Authority']\n",
    "        pwd = dataset.loc[dataset['Departments'] == 'PWD']\n",
    "        ksrtc = dataset.loc[dataset['Departments'] == 'KSRTC']\n",
    "        kseb = dataset.loc[dataset['Departments'] == 'KSEB']\n",
    "        env = dataset.loc[dataset['Departments'] == 'Environment and climate change']\n",
    "        print(env.shape)    #(30, 4)\n",
    "        #print(water.shape)  #(39, 4)\n",
    "        #print(pwd.shape)    #(42, 4)\n",
    "        #print(ksrtc.shape)  #(17, 4)\n",
    "        #print(kseb.shape)   #(22, 4)\n",
    "        #dataset.head()\n",
    "        #print(pwd)\n",
    "\n",
    "  \n",
    "\n",
    "        #Filtering out Subjects and complaints from the dataframe\n",
    "        df_water = water[['Subject','Complaint']]\n",
    "        df_pwd   = pwd[['Subject','Complaint']]\n",
    "        df_ksrtc = ksrtc[['Subject','Complaint']]\n",
    "        df_kseb  = kseb[['Subject','Complaint']]\n",
    "        df_env   = env[['Subject','Complaint']]\n",
    "\n",
    "        dfwater  = df_water[['Subject','Complaint']]\n",
    "        dfpwd    = df_pwd[['Subject','Complaint']]\n",
    "        dfksrtc  = df_ksrtc[['Subject','Complaint']]\n",
    "        dfkseb   = df_kseb[['Subject','Complaint']]\n",
    "        dfenv    = df_env[['Subject','Complaint']]\n",
    "\n",
    "        \n",
    "        dfwater['Subject_and_Complaint'] = df_water['Subject'] + \" \"+ df_water['Complaint']\n",
    "        dfwater=dfwater[['Subject_and_Complaint']]\n",
    "        \n",
    "        dfpwd['Subject_and_Complaint'] = df_pwd['Subject'] + \" \"+ df_pwd['Complaint']\n",
    "        dfpwd=dfpwd[['Subject_and_Complaint']]\n",
    "        \n",
    "        dfksrtc['Subject_and_Complaint'] = df_ksrtc['Subject'] + \" \"+ df_ksrtc['Complaint']\n",
    "        dfksrtc=dfksrtc[['Subject_and_Complaint']]\n",
    "        \n",
    "        dfkseb['Subject_and_Complaint'] = df_kseb['Subject'] + \" \"+ df_kseb['Complaint']\n",
    "        dfkseb=dfkseb[['Subject_and_Complaint']]\n",
    "        \n",
    "        dfenv ['Subject_and_Complaint'] = df_env['Subject'] + \" \"+ df_env['Complaint']\n",
    "        dfenv =dfenv [['Subject_and_Complaint']]\n",
    "        print(dfenv )\n",
    "        return (dfwater,dfpwd,dfksrtc,dfkseb,dfenv)\n",
    "        \n",
    "        \n",
    "    def tokenisation(self,dfwater,dfpwd,dfksrtc,dfkseb,dfenv):\n",
    "        water_token = []\n",
    "        water_list=[]\n",
    "            #Tokenising water data\n",
    "            \n",
    "        #lemmatizer = WordNetLemmatizer()\n",
    "        \n",
    "        #lemmatizer.lemmatize(\"corpora\")\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        for i, row in dfwater.iterrows():\n",
    "                #print(i,row['Subject'], row['Complaint'])\n",
    "                \n",
    "            water_token = word_tokenize(row['Subject_and_Complaint'])\n",
    "            result = [i for i in water_token if not i in stop_words]\n",
    "            #lemmatized_output = [i for i in lemmatizer.lemmatize(i) for i in result]\n",
    "        \n",
    "            water_list.append(result)\n",
    "        print(\"\\n\\nWater Tokens\\n\\n\")\n",
    "        print(water_list)\n",
    "    #water_token.append(tokenizer.tokenize(row['Subject_and_Complaint']))\n",
    "        pwd_token = []\n",
    "\n",
    "        pwd_list = []\n",
    "        #Tokenising pwd data  \n",
    "        \n",
    "        for i, row in dfpwd.iterrows():\n",
    "        #print(i,row['Subject'], row['Complaint'])\n",
    "            pwd_token = word_tokenize(row['Subject_and_Complaint'])\n",
    "\n",
    "            result1 = [i for i in pwd_token if not i in stop_words]\n",
    "            pwd_list.append(result1)\n",
    "            \n",
    "        print(\"\\n\\nPWD Tokens\\n\\n\")\n",
    "        print(pwd_list)\n",
    "            \n",
    "            #print( pwd_token)\n",
    "\n",
    "\n",
    "        ksrtc_token =[]\n",
    "        ksrtc_list =[]\n",
    "        #Tokenising ksrtc data    \n",
    "        for i, row in dfksrtc.iterrows():\n",
    "            #print(i,row['Subject'], row['Complaint'])\n",
    "            ksrtc_token = word_tokenize(row['Subject_and_Complaint'])\n",
    "            result = [i for i in ksrtc_token if not i in stop_words]\n",
    "            ksrtc_list.append(result)\n",
    "            #print(ksrtc_list)\n",
    "            #print( ksrtc_token)\n",
    "            \n",
    "            \n",
    "            \n",
    "        kseb_token = []\n",
    "        kseb_list= []\n",
    "        #Tokenising kseb data    \n",
    "        for i, row in dfkseb.iterrows():\n",
    "        #print(i,row['Subject'], row['Complaint'])\n",
    "            kseb_token = word_tokenize(row['Subject_and_Complaint'])\n",
    "            result = [i for i in kseb_token if not i in stop_words]\n",
    "            kseb_list.append(result)\n",
    "        #print(kseb_list)\n",
    "        #print(kseb_token)\n",
    "        \n",
    "        \n",
    "        env_token = []\n",
    "        env_list = []\n",
    "        #Tokenising env data  \n",
    "        \n",
    "        for i, row in dfenv.iterrows():\n",
    "            #print(i,row['Subject'], row['Complaint'])\n",
    "            env_token = word_tokenize(row['Subject_and_Complaint'])\n",
    "            result = [i for i in env_token if not i in stop_words]\n",
    "            env_list.append(result)\n",
    "        print(env_list)\n",
    "            #print(env_token)\n",
    "        return(water_list,pwd_list,ksrtc_list,kseb_list,env_list)\n",
    "\n",
    "\n",
    "   \n",
    "    def word_frequency(self,water_list,pwd_list,ksrtc_list,kseb_list,env_list):\n",
    "        \n",
    "        #word frequencies  Environment department\n",
    "\n",
    "        wordfreq = []\n",
    "       \n",
    "    \n",
    "        for word  in env_list:\n",
    "            for i in word:\n",
    "                wordfreq.append(i)\n",
    "        env_count =Counter(wordfreq)\n",
    "        env_count= dict(env_count)\n",
    "        print(\"\\n\\n Env Count \\n\\n\")\n",
    "        print(type(env_count))\n",
    "        \n",
    "    \n",
    "        #word frequencies  KSEB department\n",
    "        wordfreq = []\n",
    "        for word  in kseb_list:\n",
    "            for i in word:\n",
    "                wordfreq.append(i)\n",
    "        kseb_count = Counter(wordfreq)\n",
    "        kseb_count= dict(kseb_count) \n",
    "        print(\"\\n\\n KSEB Count \\n\\n\")\n",
    "        print(kseb_count)\n",
    "    \n",
    "\n",
    "        \n",
    "        #word frequencies  KSRTC department\n",
    "\n",
    "        wordfreq = []\n",
    "        \n",
    "        for word  in ksrtc_list:\n",
    "            \n",
    "            for i in word:\n",
    "                wordfreq.append(i)\n",
    "        ksrtc_count =Counter(wordfreq)\n",
    "        ksrtc_count\n",
    "        print(\"\\n\\n KSRTC Count \\n\\n\")\n",
    "        print(ksrtc_count)\n",
    "    \n",
    "        \n",
    "        #word frequencies  pwd department\n",
    "\n",
    "        for word  in pwd_list:\n",
    "            \n",
    "            for i in word:\n",
    "                wordfreq.append(i)\n",
    "        pwd_count =Counter(wordfreq)\n",
    "        print(\"\\n\\n PWD Count \\n\\n\")\n",
    "        print(type(pwd_count))\n",
    "        \n",
    "        \n",
    "        #word frequencies  water department\n",
    "\n",
    "        wordfreq = []\n",
    "        for word  in water_list:\n",
    "            \n",
    "            for i in word:\n",
    "                wordfreq.append(i)\n",
    "        water_count =Counter(wordfreq)\n",
    "        print(\"\\n\\n KSRTC Count \\n\\n\")\n",
    "        print(type(water_count))\n",
    "        return(water_count,pwd_count,ksrtc_count,kseb_count,env_count)\n",
    "\n",
    "\n",
    "    def most_repeated_keywords(self,water_freq,pwd_freq,ksrtc_freq,kseb_freq,env_freq ):\n",
    "    \n",
    "       \n",
    "        water_lis =[]\n",
    "        water_freq_list = { }\n",
    "        for lists in water_freq:\n",
    "            water_freq_list.update({})\n",
    "            lists=dict(lists)\n",
    "            items = [(v, k) for k, v in lists.items()]\n",
    "            items.sort()\n",
    "            items.reverse()\n",
    "            items = [k for v, k in items]\n",
    "            #print(items)\n",
    "            water_dict=(items[:4])\n",
    "            water_lis.append(water_dict)\n",
    "            \n",
    "            \n",
    "        \n",
    "            \n",
    "        print(\"\\n\\nKEYWORDS  WATER\\n\\n\")\n",
    "        print(water_lis)\n",
    "        \n",
    "        \n",
    "        \n",
    "        pwd_concat = []\n",
    "        pwd_lis =[]\n",
    "        pwd_freq_list = { }\n",
    "        for lists in pwd_freq:\n",
    "            pwd_freq_list.update({})\n",
    "            lists=dict(lists)\n",
    "            items = [(v, k) for k, v in lists.items()]\n",
    "            items.sort()\n",
    "            items.reverse()\n",
    "            items = [k for v, k in items]\n",
    "            #print(items)\n",
    "            pwd_dict=(items[:4])\n",
    "            pwd_lis.append(pwd_dict)\n",
    "            \n",
    "        \n",
    "        print(\"\\n\\nKEYWORDS  PWD\\n\\n\")\n",
    "        print(pwd_lis)\n",
    "        \n",
    "        \n",
    "        # Finding the most repeated words kseb\n",
    "        kseb_lis =[]\n",
    "  \n",
    "        kseb_freq_list = { }\n",
    "        for lists in kseb_freq:\n",
    "            kseb_freq_list.update({})\n",
    "            lists=dict(lists)\n",
    "            items = [(v, k) for k, v in lists.items()]\n",
    "            items.sort()\n",
    "            items.reverse()\n",
    "            items = [k for v, k in items]\n",
    "            #print(items)\n",
    "            kseb_dict=(items[0:4])\n",
    "            kseb_lis.append(kseb_dict)\n",
    "        \n",
    "      \n",
    "        print(\"\\n\\nKEYWORDS  KSEB\\n\\n\")\n",
    "        print(kseb_lis)\n",
    "        \n",
    "        \n",
    "        # Finding the most repeated words ksrtc\n",
    "        ksrtc_lis =[]\n",
    "       \n",
    "        ksrtc_freq_list = { }\n",
    "        for lists in ksrtc_freq:\n",
    "            kseb_freq_list.update({})\n",
    "            lists=dict(lists)\n",
    "            items = [(v, k) for k, v in lists.items()]\n",
    "            items.sort()\n",
    "            items.reverse()\n",
    "            items = [k for v, k in items]\n",
    "            #print(items)\n",
    "            ksrtc_dict=(items[:4])\n",
    "            ksrtc_lis.append(ksrtc_dict)\n",
    "        \n",
    "        print(\"\\n\\nKEYWORDS  KSRTC\\n\\n\")\n",
    "        print(ksrtc_lis)\n",
    "       \n",
    "        \n",
    "        # Finding the most repeated words env\n",
    "        env_lis =[]\n",
    "      \n",
    "        #env_freq_list = { }\n",
    "        for lists in env_freq:\n",
    "            #env_freq_list.update({})\n",
    "            lists=dict(lists)\n",
    "            items = [(v, k) for k, v in lists.items()]\n",
    "            #print(type(items))\n",
    "            items.sort()\n",
    "            items.reverse()\n",
    "            items = [k for v, k in items]\n",
    "            #print(items)\n",
    "            env_dict=(items[0:4])\n",
    "            #print(env_dict)\n",
    "            env_lis.append(env_dict) \n",
    "        \n",
    "                \n",
    "        print(\"\\n\\nKEYWORDS  ENV\\n\\n\")\n",
    "        print(env_lis)\n",
    "       \n",
    "        \n",
    "        \n",
    "        return(water_lis,pwd_lis,ksrtc_lis,kseb_lis,env_lis )\n",
    "                \n",
    "        \n",
    "    def most_repeated_keyword_lib(self,dfenv,dfwater,dfpwd,dfksrtc,dfkseb):\n",
    "        \n",
    "        #env keyword\n",
    "        \n",
    "        env_keyword = []\n",
    "        env_summary = []\n",
    "        \n",
    "        stopwords = list(STOP_WORDS)\n",
    "        \n",
    "        print(\"\\n\\nKEYWORD : LIB : ENV\\n\\n\")\n",
    "        #print(dfenv)\n",
    "        \n",
    "        for i, row in dfenv.iterrows():\n",
    "            env_token = word_tokenize(row['Subject_and_Complaint'])\n",
    "\n",
    "            result = [i for i in env_token if not i in stopwords]\n",
    "            #print(result)\n",
    "            env_keyword.append(result)\n",
    "            \n",
    "            str1 = ' '.join(result)\n",
    "            #map(bytes,env_keyword)\n",
    "            #print(str1)\n",
    "            #break\n",
    "        #print(type(env_token))\n",
    "            env_summary.append(keywords(str1).split('\\n'))\n",
    "               \n",
    "        #print(type(env_token[0][0]))\n",
    "        print(env_summary)\n",
    "        \n",
    "        # water Keyword\n",
    "        \n",
    "        water_keyword = []\n",
    "        water_summary = []\n",
    "        \n",
    "        stopwords = list(STOP_WORDS)\n",
    "        \n",
    "        print(\"\\n\\nKEYWORD : LIB : Water\\n\\n\")\n",
    "        \n",
    "        \n",
    "        for i, row in dfwater.iterrows():\n",
    "            water_token = word_tokenize(row['Subject_and_Complaint'])\n",
    "\n",
    "            result = [i for i in water_token if not i in stopwords]\n",
    "            #print(result)\n",
    "            water_keyword.append(result)\n",
    "            \n",
    "            str1 = ' '.join(result)\n",
    "            #map(bytes,env_keyword)\n",
    "            #print(str1)\n",
    "            #break\n",
    "        #print(type(env_token))\n",
    "            water_summary.append(keywords(str1).split('\\n'))\n",
    "               \n",
    "        #print(type(env_token[0][0]))\n",
    "        print(water_summary)\n",
    "        \n",
    "        \n",
    "        #pwd keyword\n",
    "        \n",
    "        pwd_keyword = []\n",
    "        pwd_summary = []\n",
    "        \n",
    "        stopwords = list(STOP_WORDS)\n",
    "        \n",
    "        print(\"\\n\\nKEYWORD : LIB : PWD\\n\\n\")\n",
    "        #print(dfenv)\n",
    "        \n",
    "        for i, row in dfpwd.iterrows():\n",
    "            pwd_token = word_tokenize(row['Subject_and_Complaint'])\n",
    "\n",
    "            result = [i for i in pwd_token if not i in stopwords]\n",
    "            #print(result)\n",
    "            pwd_keyword.append(result)\n",
    "            \n",
    "            str1 = ' '.join(result)\n",
    "            #map(bytes,env_keyword)\n",
    "            #print(str1)\n",
    "            #break\n",
    "        #print(type(env_token))\n",
    "            pwd_summary.append(keywords(str1).split('\\n'))\n",
    "               \n",
    "        #print(type(env_token[0][0]))\n",
    "        print(pwd_summary)\n",
    "        \n",
    "        #env keyword\n",
    "        \n",
    "        ksrtc_keyword = []\n",
    "        ksrtc_summary = []\n",
    "        \n",
    "        stopwords = list(STOP_WORDS)\n",
    "        \n",
    "        print(\"\\n\\nKEYWORD : LIB : KSRTC\\n\\n\")\n",
    "        #print(dfenv)\n",
    "        \n",
    "        for i, row in dfksrtc.iterrows():\n",
    "            ksrtc_token = word_tokenize(row['Subject_and_Complaint'])\n",
    "\n",
    "            result = [i for i in ksrtc_token if not i in stopwords]\n",
    "            #print(result)\n",
    "            ksrtc_keyword.append(result)\n",
    "            \n",
    "            str1 = ' '.join(result)\n",
    "            #map(bytes,env_keyword)\n",
    "            #print(str1)\n",
    "            #break\n",
    "        #print(type(env_token))\n",
    "            ksrtc_summary.append(keywords(str1).split('\\n'))\n",
    "               \n",
    "        #print(type(env_token[0][0]))\n",
    "        print(ksrtc_summary)\n",
    "        \n",
    "        #env keyword\n",
    "        \n",
    "        kseb_keyword = []\n",
    "        kseb_summary = []\n",
    "        \n",
    "        stopwords = list(STOP_WORDS)\n",
    "        \n",
    "        print(\"\\n\\nKEYWORD : LIB : KSEB\\n\\n\")\n",
    "        #print(dfkseb)\n",
    "        \n",
    "        for i, row in dfkseb.iterrows():\n",
    "            kseb_token = word_tokenize(row['Subject_and_Complaint'])\n",
    "\n",
    "            result = [i for i in kseb_token if not i in stopwords]\n",
    "            #print(result)\n",
    "            kseb_keyword.append(result)\n",
    "            \n",
    "            str1 = ' '.join(result)\n",
    "            #map(bytes,env_keyword)\n",
    "            #print(str1)\n",
    "            #break\n",
    "        #print(type(env_token))\n",
    "            kseb_summary.append(keywords(str1).split('\\n'))\n",
    "               \n",
    "        #print(type(env_token[0][0]))\n",
    "        print(kseb_summary)\n",
    "        return(env_summary,water_summary,pwd_summary,ksrtc_summary,kseb_summary)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def jump(self,status):\n",
    "        if status == 1:\n",
    "            env_summary,water_summary,pwd_summary,ksrtc_summary,kseb_summary=self.most_repeated_keyword_lib(dfenv,dfwater,dfpwd,dfksrtc,dfkseb)\n",
    "        else:\n",
    "            \n",
    "            water_lis,pwd_lis,ksrtc_lis,kseb_lis,env_lis = self.most_repeated_keywords(water_freq,pwd_freq,ksrtc_freq,kseb_freq,env_freq )\n",
    "        return(water_lis,env_lis,pwd_lis,ksrtc_lis,kseb_lis)\n",
    "            \n",
    "    def test(self):\n",
    "        text_data = input(\"Enter complaint\")\n",
    "        text_data= text_data.replace('[^\\w\\s]','').lower()\n",
    "        \n",
    "        \n",
    "        punctuations = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~'''\n",
    "  \n",
    "        for x in text_data.lower(): \n",
    "            if x in punctuations: \n",
    "                text_data = text_data.replace(x, \" \") \n",
    "  \n",
    "   \n",
    "        print(text_data) \n",
    "        \n",
    "            \n",
    "        test_token = word_tokenize(text_data)\n",
    "        print(test_token)\n",
    "        test_list =[]\n",
    "        \n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        for i in test_token:\n",
    "            \n",
    "            \n",
    "            if i not in stop_words:\n",
    "        \n",
    "                test_list.append(i)\n",
    "        print(test_list)\n",
    "        \n",
    "        #frequency : \n",
    "        \n",
    "\n",
    "        \n",
    "        counts = Counter(test_list)\n",
    "        count = dict(counts)\n",
    "        print(count)  # word frequency\n",
    "    \n",
    "        items = [(v, k) for k, v in counts.items()]\n",
    "        items.sort()\n",
    "        items.reverse()\n",
    "        items = [k for v, k in items]\n",
    "        print(items)  # sorted high to low\n",
    "        test_dict=(items[0:5])\n",
    "        print(test_dict)  #key only\n",
    "        \n",
    "        return(test_dict)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def evaluate(self,keywords,water_lis,env_lis,pwd_lis,ksrtc_lis,kseb_lis,dept):\n",
    "        #keyword matching \n",
    "        #water_lis,env_lis,pwd_lis,ksrtc_lis,kseb_lis\n",
    "        print(\"\\n\\nTest Keywords \\n \\n\")\n",
    "        print(keywords)\n",
    "        \n",
    "        \n",
    "        #pwd predict\n",
    "        pwd_predict=[]\n",
    "        \n",
    "        i=0\n",
    "        \n",
    "        for ls in pwd_lis:\n",
    "            \n",
    "            if ls[i] in keywords[i]:\n",
    "                pwd_predict.append(ls[i])\n",
    "                i+=1\n",
    "                \n",
    "                \n",
    "        print(\"\\n\\n PWD predict \\n\\n\")    \n",
    "        print(pwd_predict)\n",
    "        \n",
    "        #water predict\n",
    "        \n",
    "        \n",
    "        water_predict=[]\n",
    "        \n",
    "        i=0\n",
    "        \n",
    "        for ls in water_lis:\n",
    "            \n",
    "            if ls[i] in keywords[i]:\n",
    "                water_predict.append(ls[i])\n",
    "                i+=1\n",
    "                \n",
    "                \n",
    "        print(\"\\n\\n Water predict \\n\\n\")    \n",
    "        print(water_predict)\n",
    "        \n",
    "         #ksrtc predict\n",
    "        \n",
    "        \n",
    "        ksrtc_predict=[]\n",
    "        \n",
    "        i=0\n",
    "        \n",
    "        for ls in ksrtc_lis:\n",
    "            \n",
    "            if ls[i] in keywords[i]:\n",
    "                ksrtc_predict.append(ls[i])\n",
    "                i+=1\n",
    "                \n",
    "                \n",
    "        print(\"\\n\\n ksrtc predict \\n\\n\")    \n",
    "        print(ksrtc_predict)\n",
    "        \n",
    "        #kseb predict\n",
    "        \n",
    "        kseb_predict=[]\n",
    "        \n",
    "        i=0\n",
    "        \n",
    "        for ls in kseb_lis:\n",
    "            \n",
    "            if ls[i] in keywords[i]:\n",
    "                kseb_predict.append(ls[i])\n",
    "                i+=1\n",
    "                \n",
    "                \n",
    "        print(\"\\n\\n kseb predict \\n\\n\")    \n",
    "        print(kseb_predict)\n",
    "        \n",
    "        \n",
    "        #env predict\n",
    "        \n",
    "        env_predict=[]\n",
    "        \n",
    "        i=0\n",
    "        \n",
    "        for ls in env_lis:\n",
    "            \n",
    "            if ls[i] in keywords[i]:\n",
    "                env_predict.append(ls[i])\n",
    "                i+=1\n",
    "                \n",
    "                \n",
    "        print(\"\\n\\nenv predict \\n\\n\")    \n",
    "        print(env_predict)\n",
    "        \n",
    "        department=dept\n",
    "        dept_values =[1,2,3,4,5]\n",
    "        depart_dict = dict(zip(dept_values,department))\n",
    "        print(depart_dict)\n",
    "        \n",
    "        if len(env_predict)==0:\n",
    "            pass\n",
    "        else:\n",
    "            print(\"Predicted Class = : \"+depart_dict[5])\n",
    "            \n",
    "        if len(pwd_predict)==0:\n",
    "            pass\n",
    "        else:\n",
    "            print(\"Predicted Class = : \"+depart_dict[2])\n",
    "            \n",
    "        if len(water_predict)==0:\n",
    "            pass\n",
    "        else:\n",
    "            print(\"Predicted Class = : \"+depart_dict[1])\n",
    "            \n",
    "        if len(kseb_predict)==0:\n",
    "            pass\n",
    "        else:\n",
    "            print(\"Predicted Class = : \"+depart_dict[3])\n",
    "            \n",
    "        if len(ksrtc_predict)==0:\n",
    "            pass\n",
    "        else:\n",
    "            print(\"Predicted Class = : \"+depart_dict[4])\n",
    "            \n",
    "            \n",
    "        \n",
    "                  \n",
    "        \n",
    "        \n",
    "file =   '/home/gayathri/project/MakeComplaint/data.csv'   \n",
    "\n",
    "x= Main(file)\n",
    "x.punctuate()\n",
    "x.data_clean()\n",
    "dfwater,dfpwd,dfksrtc,dfkseb,dfenv=x.dataframing(x.dataset)\n",
    "\n",
    "water_list,pwd_list,ksrtc_list,kseb_list,env_list = x.tokenisation(dfwater,dfpwd,dfksrtc,dfkseb,dfenv)\n",
    "\n",
    "water_freq,pwd_freq,ksrtc_freq,kseb_freq,env_freq = x.word_frequency(water_list,pwd_list,ksrtc_list,kseb_list,env_list)\n",
    "#x.most_repeated_keywords(water_freq,pwd_freq,ksrtc_freq,kseb_freq,env_freq)\n",
    "#x.most_repeated_keyword_lib(dfenv) \n",
    "\n",
    "water_lis,env_lis,pwd_lis,ksrtc_lis,kseb_lis=x.jump(0)\n",
    "\n",
    "keywords=x.test()\n",
    "dept=x.department_class()\n",
    "x.evaluate(keywords,water_lis,env_lis,pwd_lis,ksrtc_lis,kseb_lis,dept)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
