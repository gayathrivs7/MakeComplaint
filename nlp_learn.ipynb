{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using NLTK library, we can do lot of text preprocesing\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "#function to split text into word\n",
    "\n",
    "tokens = word_tokenize(\"The quick brown fox jumps over the lazy dog\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['All', 'work', 'and', 'no', 'play', 'makes', 'jack', 'a', 'dull', 'boy', ',', 'all', 'work', 'and', 'no', 'play']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/user/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "nltk.download('punkt')\n",
    "data = \"All work and no play makes jack a dull boy, all work and no play\"\n",
    "print(word_tokenize(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "data = \"All work and no play makes jack dull boy. All work and no play makes jack a dull boy.\"\n",
    "stopWords = set(stopwords.words('english')) \n",
    "print(stopWords)\n",
    "phrases = sent_tokenize(data)\n",
    "words = word_tokenize(data)\n",
    " \n",
    "print(phrases)\n",
    "print(words)\n",
    "wordsFiltered = []\n",
    "for w in words:\n",
    "    if w not in stopWords:\n",
    "        wordsFiltered.append(w)\n",
    "print(wordsFiltered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "data = \"All work and no play makes jack dull boy. All work and no play makes jack a dull boy.\"\n",
    "stopWords = set(stopwords.words('english')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stopword removal\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "data = \"All work and no play makes jack dull boy. All work and no play makes jack a dull boy.\"\n",
    "stopWords = set(stopwords.words('english')) \n",
    "print(stopWords)\n",
    "words = word_tokenize(data)\n",
    "wordsFiltered = []\n",
    "for w in words:\n",
    "    if w not in stopWords:\n",
    "        wordsFiltered.append(w)\n",
    "print(wordsFiltered)\n",
    "print(len(stopWords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stemming\n",
    "#3 stemmer alogirthms are used here\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "\n",
    "# went is not working\n",
    "words = [\"go\",\"went\",\"gone\",\"games\"]\n",
    "ps = PorterStemmer()\n",
    "lanca_stemmer = LancasterStemmer()\n",
    "sb_stemmer = SnowballStemmer(\"english\",)\n",
    "print(\"=========Porter Stemmer======\")\n",
    "for w in words:\n",
    "    print(w ,\" : \" ,ps.stem(w))\n",
    "    \n",
    "print(\"=========Lanca Stemmer======\")\n",
    "for w in words:\n",
    "\n",
    "    print(w ,\" : \" ,lanca_stemmer.stem(w))\n",
    "    \n",
    "print(\"=========Snowball Stemmer======\")\n",
    "for w in words:\n",
    "    \n",
    "    print(w ,\" : \" ,sb_stemmer.stem(w))\n",
    "    \n",
    "    \n",
    " #Porter stemmer and Snowball stemmer working in similar manner   \n",
    "#lanca is not so good\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#speech tagging\n",
    "\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "\n",
    "document = 'Whether you\\'re new to programming or an experienced developer, it\\'s easy to learn and use Python.'\n",
    "sentences = nltk.sent_tokenize(document) \n",
    "for sent in sentences:\n",
    "    print(nltk.pos_tag(nltk.word_tokenize(sent)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filtering from Speech tagging\n",
    "import nltk\n",
    "from nltk.corpus import state_union\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "\n",
    "document = 'Today the Netherlands celebrates King\\'s Day. To honor this tradition, the Dutch embassy in San Francisco invited me to'\n",
    "sentences = nltk.sent_tokenize(document)  \n",
    "data = []\n",
    "for sent in sentences:\n",
    "    data = data + nltk.pos_tag(nltk.word_tokenize(sent))\n",
    "for w in data:\n",
    "    if 'VB' in w[1]:\n",
    "        print(w)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#natural language prediction\n",
    "import nltk\n",
    "\n",
    " \n",
    "from nltk.corpus import names\n",
    "\n",
    "def gender_features(word): \n",
    "    return {'last_letter': word[-1]}\n",
    "\n",
    "names = ([(name, 'male') for name in names.words('male.txt')] + \n",
    "         [(name, 'female') for name in names.words('female.txt')])\n",
    "\n",
    "featuresets = [(gender_features(n), g) for (n,g) in names]\n",
    "train_set = featuresets\n",
    "classifier = nltk.MaxentClassifier.train(train_set) \n",
    "print(classifier.classify(gender_features('Frank')))\n",
    "name = input(\"Name: \")\n",
    "print(classifier.classify(gender_features(name)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.classify.util\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.corpus import names\n",
    " \n",
    "def word_feats(words):\n",
    "    return dict([(word, True) for word in words])\n",
    " \n",
    "positive_vocab = [ 'awesome', 'outstanding', 'fantastic', 'terrific', 'good', 'nice', 'great', ':)' ]\n",
    "negative_vocab = [ 'bad', 'terrible','useless', 'hate', ':(' ]\n",
    "neutral_vocab = [ 'movie','the','sound','was','is','actors','did','know','words','not' ]\n",
    " \n",
    "positive_features = [(word_feats(pos), 'pos') for pos in positive_vocab]\n",
    "negative_features = [(word_feats(neg), 'neg') for neg in negative_vocab]\n",
    "neutral_features = [(word_feats(neu), 'neu') for neu in neutral_vocab]\n",
    " \n",
    "train_set = negative_features + positive_features + neutral_features\n",
    " \n",
    "classifier = NaiveBayesClassifier.train(train_set) \n",
    " \n",
    "# Predict\n",
    "neg = 0\n",
    "pos = 0\n",
    "sentence = \"Awesome movie, I liked it\"\n",
    "sentence = sentence.lower()\n",
    "words = sentence.split(' ')\n",
    "for word in words:\n",
    "    classResult = classifier.classify( word_feats(word))\n",
    "    if classResult == 'neg':\n",
    "        neg = neg + 1\n",
    "    if classResult == 'pos':\n",
    "        pos = pos + 1\n",
    " \n",
    "print('Positive: ' + str(float(pos)/len(words)))\n",
    "print('Negative: ' + str(float(neg)/len(words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tree ['This', 'is', 'Gayathri', \"'s\", 'book', 'is', \"n't\", 'it', '?']\n",
      "WordPunct ['This', 'is', 'Gayathri', \"'\", 's', 'book', 'isn', \"'\", 't', 'it', '?']\n",
      "porter stemmer\n",
      "Lemmatizer\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import nltk\n",
    "\n",
    "#Tokenize using TreeBank Tokenizer\n",
    "\n",
    "text=\"This is Gayathri's book isn't it?\"\n",
    "\n",
    "tokenizer=nltk.tokenize.TreebankWordTokenizer()\n",
    "tokenizer1=nltk.tokenize.WordPunctTokenizer()\n",
    "\n",
    "tokens=tokenizer.tokenize(text)\n",
    "tokens1=tokenizer1.tokenize(text) #wordPunct\n",
    "\n",
    "print(\"Tree\", tokens)\n",
    "print(\"WordPunct\" ,tokens1)\n",
    "\n",
    "#stemmer\n",
    "stemmer=nltk.stem.PorterStemmer()\n",
    "print(\"porter stemmer\")\n",
    "\" \".join(stemmer.stem(token)for token in tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The quick brown fox jump over the lazy dog'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import nltk\n",
    "\n",
    "#Tokenzie using TreeBank Tokenizer\n",
    "\n",
    "text=\"This is Gayathri's book isn't it?\"\n",
    "\n",
    "lemmatizer=nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "\" \".join(lemmatizer.lemmatize(token)for token in tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'a', 'sample', 'sentence', ',', 'showing', 'off', 'is', 'was', 'kay', 'the', 'stop', 'words', 'filtration', '.']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['This',\n",
       " 'sample',\n",
       " 'sentence',\n",
       " ',',\n",
       " 'showing',\n",
       " 'kay',\n",
       " 'stop',\n",
       " 'words',\n",
       " 'filtration',\n",
       " '.']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "\n",
    "\n",
    "sentence= []\n",
    "def stopword_remove(tokens):\n",
    "      \n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    print(tokens)\n",
    "    for token in tokens:\n",
    "        if token not in stop_words:\n",
    "            sentence.append(token)\n",
    "    return sentence\n",
    "                     \n",
    "    \n",
    "    \n",
    "    \n",
    "example_sent = \"This is a sample sentence, showing off is was kay the stop words filtration.\"\n",
    "\n",
    "tokens = word_tokenize(example_sent)\n",
    "#print(type(tokens))\n",
    "stopword_remove(tokens)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "['a', 't', 'hers', 'do', 'above', 'any', 're', 'up', 'other', 'about', 'doing', 'further', 'then', 'if', 'when', 'myself', \"should've\", 'our', 'we', 'off', 'between', \"wouldn't\", 'because', 'don', 'my', 'being', 'himself', 'down', 'of', 'and', 'for', \"hasn't\", \"you'll\", 'against', 'each', 'them', 'their', 'ma', 'only', 'over', \"you're\", \"shan't\", 'y', 'are', 'o', 'very', 'all', 'your', 'wouldn', 'he', 'below', 'more', 'nor', 'mightn', 'by', 'once', 'while', 'wasn', 'as', \"weren't\", 'yourself', 'who', 'which', 'ourselves', 'ours', 'itself', 'yours', 'herself', 'no', \"won't\", 'haven', 'both', 'not', 'or', 'has', 'what', 'have', 'so', \"needn't\", 'm', 'didn', 'mustn', 'this', 'themselves', 'under', 'you', \"mustn't\", 'be', \"shouldn't\", \"didn't\", \"haven't\", 'same', 'doesn', 'am', 'with', \"wasn't\", 'until', 'she', 'had', 'him', 'it', 'that', 'into', 'shan', 'to', 'just', 'an', 'before', \"she's\", 'such', 'yourselves', 'theirs', 's', 'now', 'but', 'those', \"hadn't\", \"it's\", 'from', 'on', \"couldn't\", 'hadn', \"that'll\", \"don't\", 'is', 'the', 'having', 'out', 'd', 'during', 'at', 'won', 'was', 'here', 'were', 'me', \"isn't\", 'i', 'they', \"aren't\", 'his', 'too', 'there', \"you'd\", 'isn', 'been', \"you've\", 've', 'shouldn', 'again', 'most', 'did', 'should', 'its', 'how', \"doesn't\", 'whom', 'after', 'needn', 'weren', 'in', 'these', 'aren', 'does', 'ain', 'will', 'couldn', 'through', 'why', 'own', 'few', 'hasn', 'can', 'than', 'some', 'll', 'where', 'her', \"mightn't\"]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['This',\n",
       " 'sample',\n",
       " 'sentence',\n",
       " ',',\n",
       " 'showing',\n",
       " 'kay',\n",
       " 'stop',\n",
       " 'words',\n",
       " 'filtration',\n",
       " '.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "\n",
    "\n",
    "sentence= []\n",
    "def stopword_remove(tokens):\n",
    "  \n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    stop_words = list(stop_words)\n",
    "    print(stop_words)\n",
    "    for token in tokens:\n",
    "        if token not in stop_words:\n",
    "            sentence.append(token)\n",
    "    return sentence\n",
    "                     \n",
    "    \n",
    "    \n",
    "    \n",
    "example_sent = \"This is a sample sentence, showing off is was kay the stop words filtration.\"\n",
    "tokens = word_tokenize(example_sent)\n",
    "print(type(tokens))\n",
    "stopword_remove(tokens)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NLTK', 'leading', 'platform', 'building', 'Python', 'programs', 'work', 'human', 'language', 'data', '.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "input_str = \"NLTK is a leading platform for building Python programs to work with human language data.\"\n",
    "stop_words = set(stopwords.words('english'))\n",
    "from nltk.tokenize import word_tokenize\n",
    "tokens = word_tokenize(input_str)\n",
    "result = [i for i in tokens if not i in stop_words]\n",
    "print (result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of stop words: 305\n",
      "First ten stop words: ['everywhere', 'a', 'noone', 'hers', 'do', 'onto', 'another', 'above', 'any', 'somewhere']\n",
      "\n",
      "['NLTK', 'leading', 'platform', 'building', 'Python', 'programs', 'work', 'human', 'language', 'data', '.']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "spacy_nlp = spacy.load('en_core_web_sm')\n",
    "spacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
    "print('Number of stop words: %d' % len(spacy_stopwords))\n",
    "print('First ten stop words: %s' % list(spacy_stopwords)[:10])\n",
    "doc = spacy_nlp('NLTK is a leading platform for building Python programs to work with human language data.')\n",
    "tokens = [token.text for token in doc if not token.is_stop]\n",
    "\n",
    "print()\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'spacy.tokens.doc.Doc'>\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'token' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-ea404cf1237f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;31m#tokenizer=nltk.tokenize.TreebankWordTokenizer()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m     \u001b[0mwater_token\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Subject_and_Complaint'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m     \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mwater_token\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_stop\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m     \u001b[0;31m#result = [i for i in water_token if not i in stop_words]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-30-ea404cf1237f>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;31m#tokenizer=nltk.tokenize.TreebankWordTokenizer()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m     \u001b[0mwater_token\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Subject_and_Complaint'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m     \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mwater_token\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_stop\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m     \u001b[0;31m#result = [i for i in water_token if not i in stop_words]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'token' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords \n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize \n",
    "import spacy\n",
    "spacy_nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "\n",
    "#Data cleaning\n",
    "\n",
    "# 1. unpunctuate \n",
    "# 2. to lower\n",
    "# 3. Remove numerals\n",
    "# 4. Remove Newline for subject\n",
    "\n",
    "\n",
    "# Data loading\n",
    "dataset= pd.read_csv('/home/user/Complaint/MakeComplaint/data.csv')\n",
    "\n",
    "\n",
    "# unpunctuate and lower case\n",
    "dataset['Subject'] = dataset['Subject'].str.replace('[^\\w\\s]','').str.lower()\n",
    "\n",
    "\n",
    "# unpunctuate and lower case\n",
    "dataset['Complaint'] = dataset['Complaint'].str.replace('[^\\w\\s]','').str.lower() \n",
    "\n",
    "\n",
    "#rRemoving new lines in the subject field\n",
    "dataset['Subject'] = dataset['Subject'].str.rstrip('\\n')\n",
    "\n",
    "#removing Numeric \n",
    "dataset['Complaint'] = dataset['Complaint']\n",
    "\n",
    "\n",
    "# creating dataframe for each departments\n",
    "water = dataset.loc[dataset['Departments'] == 'Water Authority']\n",
    "pwd = dataset.loc[dataset['Departments'] == 'PWD']\n",
    "ksrtc = dataset.loc[dataset['Departments'] == 'KSRTC']\n",
    "kseb = dataset.loc[dataset['Departments'] == 'KSEB']\n",
    "env = dataset.loc[dataset['Departments'] == 'Environment and climate change']\n",
    "\n",
    "#print(env.shape)    #(29, 4)\n",
    "#print(water.shape)  #(17, 4)\n",
    "#print(pwd.shape)    #(39, 4)\n",
    "#print(ksrtc.shape)  #(13, 4)\n",
    "#print(kseb.shape)   #(22, 4)\n",
    "#dataset.head()\n",
    "#print(pwd)\n",
    "\n",
    "#Filtering out Subjects and complaints from the dataframe\n",
    "df_water = water[['Subject','Complaint']]\n",
    "df_pwd   = pwd[['Subject','Complaint']]\n",
    "df_ksrtc = ksrtc[['Subject','Complaint']]\n",
    "df_kseb  = kseb[['Subject','Complaint']]\n",
    "df_env   = env[['Subject','Complaint']]\n",
    "\n",
    "dfwater  = df_water[['Subject','Complaint']]\n",
    "dfpwd    = df_pwd[['Subject','Complaint']]\n",
    "dfksrtc  = df_ksrtc[['Subject','Complaint']]\n",
    "dfkseb   = df_kseb[['Subject','Complaint']]\n",
    "dfenv    = df_env[['Subject','Complaint']]\n",
    "\n",
    "\n",
    "#Dataframe with complaint and subject as one column = Water\n",
    "dfwater['Subject_and_Complaint'] = df_water['Subject'] + \" \"+ df_water['Complaint']\n",
    "dfwater=dfwater[['Subject_and_Complaint']]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Dataframe with complaint and subject as one column = PWD\n",
    "dfpwd['Subject_and_Complaint'] = df_pwd['Subject'] + \" \"+ df_pwd['Complaint']\n",
    "dfpwd=dfpwd[['Subject_and_Complaint']]\n",
    "#print(dfpwd)\n",
    "\n",
    "#Dataframe with complaint and subject as one column = ksrtc\n",
    "dfksrtc['Subject_and_Complaint'] = df_ksrtc['Subject'] + \" \"+ df_ksrtc['Complaint']\n",
    "dfksrtc=dfksrtc[['Subject_and_Complaint']]\n",
    "#print(dfksrtc)\n",
    "\n",
    "#Dataframe with complaint and subject as one column = kseb\n",
    "dfkseb['Subject_and_Complaint'] = df_kseb['Subject'] + \" \"+ df_kseb['Complaint']\n",
    "dfkseb=dfkseb[['Subject_and_Complaint']]\n",
    "#print(dfkseb)\n",
    "\n",
    "\n",
    "#Dataframe with complaint and subject as one column = env\n",
    "dfenv ['Subject_and_Complaint'] = df_env['Subject'] + \" \"+ df_env['Complaint']\n",
    "dfenv =dfenv [['Subject_and_Complaint']]\n",
    "#print(dfenv )\n",
    "\n",
    "#==================================Tokenization Begins : =============================================\n",
    "\n",
    "\n",
    "print(type(doc ))\n",
    "#print(tokens)\n",
    "\n",
    "sentence= []\n",
    "\"\"\"def stopword_remove(tokens):\n",
    "  \n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    stop_words = list(stop_words)\n",
    "    \n",
    "    for token in tokens:\n",
    "        if token not in stop_words:\n",
    "            sentence.append(token)\n",
    "    return sentence\"\"\"\n",
    "    \n",
    "                     \n",
    "\n",
    "spacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
    "\n",
    "water_token = []\n",
    "#Tokenising water data\n",
    "stop_words = set(stopwords.words('english'))\n",
    "for i, row in dfwater.iterrows():\n",
    "    #print(i,row['Subject'], row['Complaint'])\n",
    "    #tokenizer=nltk.tokenize.TreebankWordTokenizer()\n",
    "    water_token = word_tokenize(row['Subject_and_Complaint'])\n",
    "    tokens = [token.text for water_token in doc if not token.is_stop]\n",
    "    #result = [i for i in water_token if not i in stop_words]\n",
    "    print(result)\n",
    "    #water_token.append(tokenizer.tokenize(row['Subject_and_Complaint']))\n",
    "\n",
    "  \n",
    "   \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#print(type(tokens))\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "pwd_token = []\n",
    "#Tokenising pwd data    \n",
    "for i, row in dfpwd.iterrows():\n",
    "    #print(i,row['Subject'], row['Complaint'])\n",
    "    tokenizer=nltk.tokenize.TreebankWordTokenizer()\n",
    "    pwd_token.append(tokenizer.tokenize(row['Subject_and_Complaint']))\n",
    "#print( pwd_token)\n",
    "\n",
    "ksrtc_token =[]\n",
    "#Tokenising ksrtc data    \n",
    "for i, row in dfksrtc.iterrows():\n",
    "    #print(i,row['Subject'], row['Complaint'])\n",
    "    tokenizer=nltk.tokenize.TreebankWordTokenizer()\n",
    "    ksrtc_token.append(tokenizer.tokenize(row['Subject_and_Complaint']))\n",
    "#print( ksrtc_token)\n",
    "\n",
    "kseb_token = []\n",
    "#Tokenising kseb data    \n",
    "for i, row in dfkseb.iterrows():\n",
    "    #print(i,row['Subject'], row['Complaint'])\n",
    "    tokenizer=nltk.tokenize.TreebankWordTokenizer()\n",
    "    kseb_token.append(tokenizer.tokenize(row['Subject_and_Complaint']))\n",
    "#print(kseb_token)\n",
    "\n",
    "env_token = []\n",
    "#Tokenising env data    \n",
    "for i, row in dfenv.iterrows():\n",
    "    #print(i,row['Subject'], row['Complaint'])\n",
    "    tokenizer=nltk.tokenize.TreebankWordTokenizer()\n",
    "    env_token.append(tokenizer.tokenize(row['Subject_and_Complaint']))\n",
    "#print(env_token)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================Original==================================================================\n",
      "There is a road going towards Trivandrum Technopark Phase 3 and it has a gate towards the Technopark buildings from this road. It is starting from Nippol Toyota and going towards Kallingal . The road name is Kallingal - Attinkuzhy Road. This Road is damaged for last 1.5 years and no one is caring about this. there are minimum 300 vehicles daily goes through it in the morning and evening towards offices. Now it is like 1 meter gutter in the middle of the road making health issues to the people. It is in very danger situation. URGENT action required.\n",
      "===================================Processed================================================================\n",
      "there is a road going towards trivandrum technopark phase  and it has a gate towards the technopark buildings from this road. it is starting from nippol toyota and going towards kallingal . the road name is kallingal - attinkuzhy road. this road is damaged for last . years and no one is caring about this. there are minimum  vehicles daily goes through it in the morning and evening towards offices. now it is like  meter gutter in the middle of the road making health issues to the people. it is in very danger situation. urgent action required.\n",
      "=========================================Summary==========================================================\n",
      "it is starting from nippol toyota and going towards kallingal .\n",
      "the road name is kallingal - attinkuzhy road.\n",
      "this road is damaged for last .\n",
      "now it is like  meter gutter in the middle of the road making health issues to the people.\n",
      "==================================Keywords=================================================================\n",
      "technopark\n",
      "road\n",
      "meter\n",
      "vehicles daily\n"
     ]
    }
   ],
   "source": [
    "from gensim.summarization import summarize\n",
    "from gensim.summarization import keywords\n",
    "from nltk.corpus import stopwords \n",
    "import nltk\n",
    "import re\n",
    "\n",
    "\n",
    "\n",
    "text = \"There is a road going towards Trivandrum Technopark Phase 3 and it has a gate towards the \"+ \\\n",
    "\"Technopark buildings from this road. It is starting from Nippol Toyota and going towards \"+\\\n",
    "\"Kallingal . The road name is Kallingal - Attinkuzhy Road. This Road is damaged for last 1.5 \"+\\\n",
    "\"years and no one is caring about this. there are minimum 300 vehicles daily goes through it in \"+\\\n",
    "\"the morning and evening towards offices. Now it is like 1 meter gutter in the middle of the road \"+\\\n",
    " \"making health issues to the people. It is in very danger situation. URGENT action required.\"\n",
    "print(\"================================Original==================================================================\")\n",
    "print(text)\n",
    "\n",
    "text = text.strip().lower()\n",
    "\n",
    "#text=text.re.sub('[^a-zA-Z ]',\"\")\n",
    "\n",
    "#text = nltk.sent_tokenize(text)\n",
    "output = re.sub(r'\\d+', '', text)\n",
    "print(\"===================================Processed================================================================\")\n",
    "print(output)\n",
    "\n",
    "print(\"=========================================Summary==========================================================\")\n",
    "print(summarize(output, ratio = 0.5))\n",
    "print(\"==================================Keywords=================================================================\")\n",
    "\n",
    "print(keywords(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['There', 'road', 'going', 'towards', 'Trivandrum', 'Technopark', 'Phase', 'gate', 'towards', 'Technopark', 'buildings', 'road', 'It', 'starting', 'Nippol', 'Toyota', 'going', 'towards', 'Kallingal', 'The', 'road', 'name', 'Kallingal', 'Attinkuzhy', 'Road', 'This', 'Road', 'damaged', 'last', 'years', 'one', 'caring', 'minimum', 'vehicles', 'daily', 'goes', 'morning', 'evening', 'towards', 'offices', 'Now', 'like', 'meter', 'gutter', 'middle', 'road', 'making', 'health', 'issues', 'people', 'It', 'danger', 'situation', 'URGENT', 'action', 'required']\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "#method 2 summarisation\n",
    "from gensim.summarization import summarize\n",
    "from gensim.summarization import keywords\n",
    "from nltk.corpus import stopwords \n",
    "import nltk\n",
    "import re\n",
    "\n",
    "\n",
    "article_text = \"There is a road going towards Trivandrum Technopark Phase 3 and it has a gate towards the \"+ \\\n",
    "\"Technopark buildings from this road. It is starting from Nippol Toyota and going towards \"+\\\n",
    "\"Kallingal . The road name is Kallingal - Attinkuzhy Road. This Road is damaged for last 1.5 \"+\\\n",
    "\"years and no one is caring about this. there are minimum 300 vehicles daily goes through it in \"+\\\n",
    "\"the morning and evening towards offices. Now it is like 1 meter gutter in the middle of the road \"+\\\n",
    " \"making health issues to the people. It is in very danger situation. URGENT action required.\"\n",
    "stop_words = set(stopwords.words('english'))\n",
    "article_text = re.sub(r'\\[[0-9]*\\]', ' ', article_text)  \n",
    "article_text = re.sub(r'\\s+', ' ', article_text)  \n",
    "formatted_article_text = re.sub('[^a-zA-Z]', ' ', article_text )  \n",
    "formatted_article_text = re.sub(r'\\s+', ' ', formatted_article_text)  \n",
    "sentence_list = nltk.sent_tokenize(article_text)  \n",
    "word_frequencies = []\n",
    "tokenize = nltk.word_tokenize(formatted_article_text)\n",
    "\n",
    "tok =[]\n",
    "for word in tokenize:\n",
    "    result = [i for i in tokenize if not i in stop_words]\n",
    "print(result) \n",
    "\n",
    "word_frequencies = {}  \n",
    "for word in  tokenize:\n",
    "     if word not in stop_words:\n",
    "            if word not in word_frequencies.keys():\n",
    "                word_frequencies[word] = 1\n",
    "            else:\n",
    "                word_frequencies[word] += 1\n",
    "                \n",
    "maximum_frequncy = max(word_frequencies.values()) \n",
    "print(maximum_frequncy)\n",
    "for word in word_frequencies.keys(): \n",
    "     word_frequencies[word] = (word_frequencies[word]/maximum_frequncy)\n",
    "        \n",
    "sentence_scores = {} \n",
    "for sent in sentence_list:\n",
    "    for word in nltk.word_tokenize(sent.lower()):if word in word_frequencies.keys():\n",
    "            \n",
    "        if word in word_frequencies.keys():\n",
    "            if len(sent.split(' ')) < 30:\n",
    "                \n",
    "            \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hellomzcsdn,,m\n",
    "df\n",
    "v\n",
    "fd\n",
    "vjjd\n",
    "ds\n",
    "fsnhs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['The', 'Saudis', 'are', 'preparing', 'a', 'report', 'that', 'will', 'acknowledge', 'that'], ['Saudi', 'journalist', 'Jamal', \"Khashoggi's\", 'death', 'was', 'the', 'result', 'of', 'an'], ['interrogation', 'that', 'went', 'wrong,', 'one', 'that', 'was', 'intended', 'to', 'lead'], ['to', 'his', 'abduction', 'from', 'Turkey,', 'according', 'to', 'two', 'sources.']]\n",
      "Dictionary(33 unique tokens: ['Turkey,', 'his', 'that', 'interrogation', 'death']...)\n",
      "{'Turkey,': 26, 'his': 30, 'that': 7, 'interrogation': 20, 'death': 13, 'lead': 21, 'an': 12, 'of': 15, 'to': 23, 'abduction': 27, \"Khashoggi's\": 10, 'according': 28, 'one': 22, 'wrong,': 25, 'went': 24, 'sources.': 31, 'preparing': 5, 'journalist': 14, 'The': 1, 'a': 2, 'acknowledge': 3, 'two': 32, 'the': 17, 'from': 29, 'report': 6, 'Saudi': 11, 'Jamal': 9, 'intended': 19, 'will': 8, 'result': 16, 'was': 18, 'are': 4, 'Saudis': 0}\n",
      "Dictionary(48 unique tokens: ['his', 'that', 'paths', 'interrogation', 'acknowledge']...)\n",
      "{'his': 30, 'that': 7, 'paths': 36, 'interrogation': 20, 'acknowledge': 3, 'death': 13, 'to': 23, 'intended': 19, 'and': 41, 'A': 46, 'Saudi': 11, 'well': 45, 'in': 34, 'abduction': 27, 'IV': 39, 'according': 28, 'two': 32, 'report': 6, 'ordering': 43, 'result': 16, 'intersection': 35, 'are': 4, 'lead': 21, 'Turkey,': 26, 'of': 15, 'an': 12, 'trees': 37, \"Khashoggi's\": 10, 'survey': 47, 'minors': 42, 'one': 22, 'sources.': 31, 'journalist': 14, 'The': 1, 'a': 2, 'graph': 33, 'the': 17, 'from': 29, 'wrong,': 25, 'preparing': 5, 'Widths': 40, 'Graph': 38, 'went': 24, 'will': 8, 'Jamal': 9, 'quasi': 44, 'was': 18, 'Saudis': 0}\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim import corpora\n",
    "from pprint import pprint\n",
    "\n",
    "documents = [\"The Saudis are preparing a report that will acknowledge that\", \n",
    "             \"Saudi journalist Jamal Khashoggi's death was the result of an\", \n",
    "             \"interrogation that went wrong, one that was intended to lead\", \n",
    "             \"to his abduction from Turkey, according to two sources.\"]\n",
    "\n",
    "documents_2 = [\"One source says the report will likely conclude that\", \n",
    "                \"the operation was carried out without clearance and\", \n",
    "                \"transparency and that those involved will be held\", \n",
    "                \"responsible. One of the sources acknowledged that the\", \n",
    "                \"report is still being prepared and cautioned that\", \n",
    "                \"things could change.\"]\n",
    "\n",
    "# Tokenize(split) the sentences into words\n",
    "texts = [[text for text in doc.split()] for doc in documents]\n",
    "\n",
    "print(texts)\n",
    "\n",
    "# Create dictionary\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "\n",
    "print(dictionary)\n",
    "\n",
    "# Show the word to id map\n",
    "print(dictionary.token2id)\n",
    "\n",
    "# adding new doc to dictionary\n",
    "documents_2 = [\"The intersection graph of paths in trees\",\n",
    "               \"Graph minors IV Widths of trees and well quasi ordering\",\n",
    "               \"Graph minors A survey\"]\n",
    "\n",
    "texts_2 = [[text for text in doc.split()] for doc in documents_2]\n",
    "\n",
    "dictionary.add_documents(texts_2)\n",
    "\n",
    "\n",
    "print(dictionary)\n",
    "\n",
    "\n",
    "print(dictionary.token2id)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'apple': 3, 'egg': 2, 'banana': 1})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "list1=['apple','egg','apple','banana','egg','apple']\n",
    "counts = Counter(list1)\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-32-440e5399e1f1>, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-32-440e5399e1f1>\"\u001b[0;36m, line \u001b[0;32m5\u001b[0m\n\u001b[0;31m    for j in len(:\u001b[0m\n\u001b[0m                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "list1 = [['a','b','c'],['a','a','b']]\n",
    "wordfreq = []\n",
    "inner_list=[]\n",
    "for i in len(list1):\n",
    "    for j in len(:\n",
    "        inner_list.append(i.count(j))\n",
    "    wordfreq.append(inner_list)\n",
    "print(wordfreq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 'Group2', 'A': 'Group1', 'D': 'Group2', 'B': 'Group1'}\n"
     ]
    }
   ],
   "source": [
    "groups = [['Group1', 'A', 'B'], ['Group2', 'C', 'D']]\n",
    "\n",
    "result = {}\n",
    "for group in groups:\n",
    "    for item in group[1:]:\n",
    "        result[item] = group[0]\n",
    "print (result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e 30\n",
      "b 13\n",
      "d 4\n",
      "c 2\n",
      "a 1\n",
      "{'f': 4, 'd': 4, 'a': 1, 'c': 2, 'b': 13, 'e': 30}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "a1 = {'a':1, 'b':13, 'd':4, 'c':2, 'e':30}\n",
    "a1_sorted_keys = sorted(a1, key=a1.get, reverse=True)\n",
    "for r in a1_sorted_keys:\n",
    "    print(r, a1[r])\n",
    "a1.update({'f':4})\n",
    "print(a1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('b', 23), ('d', 17), ('c', 5), ('a', 2), ('e', 1)]\n"
     ]
    }
   ],
   "source": [
    "d = {'a':2, 'b':23, 'c':5, 'd':17, 'e':1}\n",
    "items = [(v, k) for k, v in d.items()]\n",
    "items.sort()\n",
    "items.reverse()\n",
    "items = [(k, v) for v, k in items]\n",
    "print(items)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/gayathri/project/MakeComplaint/data.csv\n",
      "   id                            Subject  \\\n",
      "0   1         no water supply connection   \n",
      "1   2                    road re tarring   \n",
      "2   3  power cuts without a prior notice   \n",
      "3   4     scarcity of water in day time    \n",
      "4   5                     water scarcity   \n",
      "\n",
      "                                           Complaint      Departments  \n",
      "0  no water pipeline connection in amma gardens r...  Water Authority  \n",
      "1  i am a resident of sreekaryam  ambadi nagar la...              PWD  \n",
      "2  as everyone knows that electricity is the majo...             KSEB  \n",
      "3  there is scarcity of water in my area vanchiyo...  Water Authority  \n",
      "4  there is a huge scarcity of water in remote ar...  Water Authority  \n",
      "(30, 4)\n",
      "                                 Subject_and_Complaint\n",
      "12   increasing deaths from air pollution \\nair pol...\n",
      "13   waste disposal to water bodies people make riv...\n",
      "19   illegal sand mines due to the sand mining ther...\n",
      "20   waste disposal on roads in our areas during ni...\n",
      "33   fishes dying massively fishes in the river pam...\n",
      "40   construction of paddy fields construction is g...\n",
      "42   water pollution is a major issue many industri...\n",
      "43   burning of plastics issue burning of plastics ...\n",
      "44   waste collecting mechanism dumping of wastes i...\n",
      "45   save water bodies activities like waste dispos...\n",
      "46   overfishing in rivers overfishing which causes...\n",
      "47   light pollution artificial light at night is o...\n",
      "48   we make a lot of ewaste the electronic waste p...\n",
      "49   ocean acidification ocean acidification is cau...\n",
      "50   city dwellers are prone to noise pollution noi...\n",
      "58   cutting down of trees now a days many people a...\n",
      "62   pollution by ksrtc bus polution by ksrtc bus i...\n",
      "65   horrible effects of quarrying quarries are bad...\n",
      "70                     mining save alappad stop mining\n",
      "72   regarding waste dumping in cities i would like...\n",
      "74   environment and climate change\\n is one of the...\n",
      "100  intolerable temperature rise in summer\\n seaso...\n",
      "103             environment intense heat during summer\n",
      "110  take immediate and necessary \\nsteps to regula...\n",
      "112  excessive use of chlorine due to excessive use...\n",
      "113  increase in heat and temperature we should hav...\n",
      "114  pollution everywhere need proper regulations t...\n",
      "116  rise in temperature increased deforestation un...\n",
      "118  the bad air and water quality in kochi the pol...\n",
      "120  waste disposal  waste disposal in public premi...\n",
      "[['increasing', 'deaths', 'air', 'pollution', 'air', 'pollution', 'number', 'deaths', 'cardiovascular', 'disease', 'attributed', 'air', 'pollution', 'much', 'higher', 'expectedair', 'pollution', 'caused', 'twice', 'many', 'deaths', 'cvd', 'respiratory', 'diseases'], ['waste', 'disposal', 'water', 'bodies', 'people', 'make', 'river', 'water', 'polluted', 'dumping', 'household', 'wastage', 'industrial', 'wastage'], ['illegal', 'sand', 'mines', 'due', 'sand', 'mining', 'effect', 'ecosystem', 'severe', 'impact', 'plants', 'animals', 'rivers'], ['waste', 'disposal', 'roads', 'areas', 'night', 'strangers', 'dumping', 'hotel', 'waste', 'domestic', 'waste', 'foul', 'smelling', 'sides', 'road', 'wastes', 'remain', 'th', 'road', 'uncleaned', 'camera', 'survelliance', 'facility', 'catch', 'people', 'throwing', 'wastes'], ['fishes', 'dying', 'massively', 'fishes', 'river', 'pampa', 'dying', 'massively', 'due', 'deposits', 'oil', 'factories', 'nearby'], ['construction', 'paddy', 'fields', 'construction', 'going', 'trivandrum', 'paddy', 'fields', 'please', 'take', 'necessary', 'steps', 'save', 'farming'], ['water', 'pollution', 'major', 'issue', 'many', 'industries', 'dump', 'wastes', 'rivers', 'lakes', 'ponds', 'streams', 'attempt', 'hide', 'wastes', 'epa', 'nspectors', 'water', 'sources', 'feed', 'major', 'crops', 'food', 'becomes', 'contaminated', 'variety', 'chemicals', 'bacteria', 'causing', 'rampant', 'health', 'problems'], ['burning', 'plastics', 'issue', 'burning', 'plastics', 'public', 'places', 'cause', 'major', 'health', 'concern', 'rate', 'lung', 'cancer', 'patients', 'increasing', 'day', 'day', 'high', 'time', 'check', 'activities'], ['waste', 'collecting', 'mechanism', 'dumping', 'wastes', 'public', 'places', 'causes', 'major', 'health', 'issues', 'inviting', 'eradicated', 'disease', 'mechanism', 'collect', 'wastes', 'households', 'disposed', 'properly'], ['save', 'water', 'bodies', 'activities', 'like', 'waste', 'disposal', 'residential', 'commercial', 'industrial', 'areas', 'oil', 'spills', 'runoff', 'agriculture', 'contaminate', 'bodies', 'water'], ['overfishing', 'rivers', 'overfishing', 'causes', 'reduction', 'diversity', 'marine', 'life', 'fishermen', 'considering', 'breeding', 'time'], ['light', 'pollution', 'artificial', 'light', 'night', 'one', 'obvious', 'physical', 'changes', 'humans', 'made', 'biosphere', 'artificial', 'light', 'also', 'affects', 'dispersal', 'orientation', 'migration', 'hormone', 'levels', 'resulting', 'disrupted', 'circadian', 'rhythms'], ['make', 'lot', 'ewaste', 'electronic', 'waste', 'problem', 'huge', 'electronics', 'end', 'landfills', 'toxics', 'like', 'lead', 'mercury', 'cadmium', 'leach', 'soil', 'water'], ['ocean', 'acidification', 'ocean', 'acidification', 'caused', 'co₂', 'dissolves', 'ocean', 'bonding', 'sea', 'water', 'creating', 'carbonic', 'acid', 'acid', 'reduces', 'ph', 'levels', 'water'], ['city', 'dwellers', 'prone', 'noise', 'pollution', 'noise', 'produced', 'vehicles', 'political', 'parties', 'religious', 'centers', 'causes', 'great', 'harm', 'human', 'ears'], ['cutting', 'trees', 'days', 'many', 'people', 'cutting', 'trees', 'purposes', 'example', 'many', 'oragansations', 'builders', 'cut', 'trees', 'build', 'projects', 'cause', 'real', 'harm', 'humans', 'also', 'utilizing', 'fertile', 'paddy', 'fields', 'construction', 'affect', 'environment', 'oxygen', 'content', 'air', 'become', 'low', 'oxygen', 'content', 'level', 'decreases', 'air', 'survival', 'living', 'beings', 'move', 'harder', 'way', 'also', 'major', 'factor', 'ozone', 'depletion', 'ozone', 'holes', 'formed', 'harmful', 'radiations', 'enter', 'earth', 'holes', 'causes', 'real', 'harm', 'living', 'beings', 'continues', 'way', 'threatening', 'part', 'lives', 'please', 'take', 'serious', 'issue', 'make', 'necessary', 'useful', 'remedies'], ['pollution', 'ksrtc', 'bus', 'polution', 'ksrtc', 'bus', 'heavy', 'today', 'government', 'fix', 'soon', 'possible'], ['horrible', 'effects', 'quarrying', 'quarries', 'bad', 'environment', 'several', 'ways', 'abruptly', 'interrupt', 'continuity', 'open', 'space', 'cause', 'soil', 'erosionair', 'dust', 'pollution', 'deterioration', 'water', 'quality', 'residential', 'areathey', 'create', 'noise', 'hazards', 'request', 'higher', 'authority', 'investigate', 'punish', 'officials', 'hand', 'quarrying', 'illegal', 'mining', 'time', 'government', 'create', 'awareness', 'potentially', 'negative', 'impact', 'quarrying'], ['mining', 'save', 'alappad', 'stop', 'mining'], ['regarding', 'waste', 'dumping', 'cities', 'would', 'like', 'inform', 'city', 'residents', 'facing', 'difficulties', 'dump', 'domestic', 'wastes', 'sohumbly', 'request', 'take', 'necessary', 'actions'], ['environment', 'climate', 'change', 'one', 'big', 'challenge', 'facing', 'right', 'preserving', 'biodiversity', 'saving', 'forest', 'help', 'overcome', 'issuesalso', 'recycling', 'avoiding', 'usage', 'plastic', 'efficient', 'use', 'fuel'], ['intolerable', 'temperature', 'rise', 'summer', 'season', 'intolerable', 'temperature', 'change', 'summer', 'season', 'cause', 'dried', 'rivers', 'skin', 'disease'], ['environment', 'intense', 'heat', 'summer'], ['take', 'immediate', 'necessary', 'steps', 'regulate', 'global', 'climate', 'change', 'one', 'among', 'factors', 'shaped', 'kerala', 'gods', 'country', 'euphoric', 'climate', 'never', 'touched', 'extremes', 'thanks', 'moderating', 'influence', 'sea', 'mighty', 'western', 'ghats', 'scenario', 'changed', 'lot', 'kerala', 'witnessing', 'unprecedental', 'surge', 'temperature', 'kumbhachoodu', 'termed', 'old', 'generation', 'time', 'highimpacting', 'livelihood', 'many'], ['excessive', 'use', 'chlorine', 'due', 'excessive', 'use', 'chlorine', 'disinfectants', 'water', 'causing', 'long', 'term', 'health', 'problems', 'like', 'hair', 'fall', 'rashes', 'etc', 'amount', 'disinfectants', 'used', 'water', 'supplied', 'need', 'reconsidered'], ['increase', 'heat', 'temperature', 'projects', 'planting', 'trees', 'dont', 'cut', 'big', 'grownup', 'trees'], ['pollution', 'everywhere', 'need', 'proper', 'regulations', 'control', 'pollution'], ['rise', 'temperature', 'increased', 'deforestation', 'uncontrolled', 'constructiondevelopment', 'activities', 'vehicle', 'emissions', 'heavily', 'contribute', 'increase', 'temperature', 'government', 'build', 'new', 'policies', 'also', 'consider', 'switching', 'renewable', 'energy', 'sources'], ['bad', 'air', 'water', 'quality', 'kochi', 'pollution', 'levels', 'kochi', 'city', 'rising', 'rapidly', 'government', 'initiate', 'steps', 'curb'], ['waste', 'disposal', 'waste', 'disposal', 'public', 'premises', 'make', 'life', 'hard', 'people', 'living', 'around']]\n",
      "[{'industries': 1, 'sector': 2, 'knows': 1, 'sure': 1, 'make': 1, 'platforms': 1, 'power': 3, 'force': 1, 'proper': 1, 'say': 1, 'cuts': 3, 'different': 1, 'announcements': 1, 'prior': 2, 'supply': 1, 'everyone': 1, 'without': 1, 'notice': 1, 'communication': 1, 'major': 1, 'every': 1, 'even': 1, 'complaint': 1, 'comes': 1, 'overall': 1, 'working': 2, 'farming': 1, 'ensurde': 1, 'shortage': 1, 'must': 1, 'affect': 1, 'electricity': 2, 'unnoticed': 1}, {'day': 1, 'many': 1, 'even': 1, 'shedding': 1, 'procedure': 1, 'power': 1, 'load': 1, 'time': 1, 'times': 1, 'particular': 1, 'loss': 1, 'set': 1, 'experiences': 1, 'frequent': 1}, {'left': 1, 'bribed': 1, 'highly': 1, 'appliances': 1, 'failure': 1, 'money': 1, 'employees': 1, 'got': 1, 'rs': 1, 'result': 1, 'connected': 1, 'bad': 1, 'give': 1, '700': 1, 'supply': 1, 'without': 1, 'connection': 3, 'rectifying': 1, 'corrupt': 1, 'rectified': 1, 'high': 1, 'wires': 1, 'checking': 1, 'first': 1, 'voltage': 1, 'burnt': 1, 'due': 1, 'grabbing': 1, 'wrongly': 1, 'bribe': 2, 'electronic': 1, 'get': 1}, {'street': 2, 'light': 1, 'robbery': 1, 'new': 1, 'facing': 1, 'ease': 1, 'activities': 1, 'lights': 1, 'locality': 1, 'scenario': 1, 'unavailability': 1, 'need': 1, 'unsocial': 1, 'problem': 1}, {'replace': 1, 'new': 1, 'working': 2, '3': 1, 'last': 1, 'one': 1, 'meter': 2, 'electricity': 1, 'daysplease': 1}, {'fastly': 1, 'using': 1, 'high': 1, 'even': 1, 'showing': 1, 'meter': 2, 'repair': 1, 'false': 1, 'normalplease': 1, 'readings': 1, 'values': 1, 'though': 1, 'current': 1, 'electricity': 1}, {'house': 1, 'got': 1, 'high': 2, 'easily': 1, 'since': 1, 'transformer': 1, 'lights': 1, 'damaged': 1, 'facing': 1, 'equipments': 1, 'issue': 2, 'electrical': 1, 'near': 1, 'voltage': 2}, {'house': 1, 'connection': 1, 'lineman': 2, 'issue': 1, 'came': 1, 'misbehaved': 1, 'drunken': 1, 'misbehaviour': 1, 'us': 1, 'repair': 1, 'recently': 1}, {'connection': 2, 'month': 1, 'new': 2, 'home': 1, 'fast': 1, 'delay': 1, 'got': 1, 'last': 1, 'applied': 1, 'electricity': 2, 'till': 1, 'connectionplease': 1, 'make': 1, 'havent': 1}, {'low': 2, 'devices': 1, 'time': 1, 'issue': 1, 'electric': 1, 'facing': 1, 'use': 1, 'problem': 1, 'voltage': 2, 'unable': 1, 'evening': 1}, {'avoid': 1, 'night': 1, 'public': 2, 'load': 2, 'time': 1, 'issue': 1, 'going': 1, 'shedding': 2, 'examinations': 1, 'please': 1, 'examination': 1}, {'connection': 2, 'failed': 1, 'got': 1, 'electric': 1, 'failure': 1, 'raining': 2, 'due': 2, 'heavy': 2}, {'arent': 1, 'many': 1, 'home': 1, 'one': 1, 'state': 1, 'connections': 1, 'working': 2, 'three': 2, 'always': 1, 'phase': 2, 'daysremaining': 1, 'phases': 2}, {'exploded': 1, 'explodes': 1, 'trivandrum': 1, 'sound': 1, 'transformer': 2, 'city': 1, 'large': 1}, {'failure': 2, 'frequent': 1, 'action': 1, 'please': 1, 'power': 2, 'madavoor': 1, 'section': 1, 'necessary': 1, 'frequently': 1, 'occurring': 1, 'take': 1, 'electricity': 1}, {'past': 1, 'every': 1, 'hot': 1, 'shedding': 1, 'months': 2, 'summer': 1, 'locality': 1, 'power': 1, 'night': 1, 'load': 1, 'look': 1, 'regular': 1, 'cuts': 1, 'needed': 1, 'makes': 1, 'request': 1, 'problem': 1, 'difficult': 1, 'us': 1}, {'old': 1, 'almost': 1, 'using': 1, 'big': 1, 'crawl': 1, '15': 1, 'wires': 4, 'continues': 1, 'still': 1, 'trouble': 1, 'due': 1, 'peoples': 1, 'years': 1, 'bad': 1, 'overhead': 2, 'problem': 1, 'electricity': 2, 'condition': 1}, {'day': 1, 'failure': 2, 'lot': 1, 'time': 1, 'power': 2}, {'power': 2, 'morning': 1, 'time': 1, 'frequent': 1, 'cut': 1, 'peak': 1, 'outage': 1}, {'complaint': 1, 'loss': 1, 'electricity': 1, 'power': 1}, {'notice': 1, 'failures': 1, 'failure': 1, 'power': 2, 'prior': 1, 'without': 1}, {'reason': 1, 'problem': 1, 'find': 1, 'time': 1, 'fix': 1, 'failure': 1, 'power': 2, 'adjoining': 1, 'well': 1, 'please': 1, 'houses': 1}]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords \n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize \n",
    "from collections import Counter\n",
    "\n",
    "global dataset\n",
    "\n",
    "\n",
    "class Main:\n",
    "         \n",
    "      \n",
    "    \n",
    "    \n",
    "    def __init__(self,dataset):\n",
    "        #/home/gayathri/project/MakeComplaint/data.csv\n",
    "        self.dataset=dataset\n",
    "        print(self.dataset)\n",
    "        self.dataset= pd.read_csv(self.dataset)\n",
    "        pass\n",
    "        \n",
    "    def data_clean(self):\n",
    "        \n",
    "        \n",
    "        \n",
    "        # unpunctuate and lower case\n",
    "        self.dataset['Subject'] = self.dataset['Subject'].str.replace('[^\\w\\s]','').str.lower()\n",
    "\n",
    "\n",
    "            # unpunctuate and lower case\n",
    "        self.dataset['Complaint'] =  self.dataset['Complaint'].str.replace('[^\\w\\s]','').str.lower() \n",
    "\n",
    "\n",
    "        #rRemoving new lines in the subject field\n",
    "        self.dataset['Subject'] =  self.dataset['Subject'].str.rstrip('\\n')\n",
    "\n",
    "        #removing Numeric \n",
    "        self.dataset['Complaint'] =  self.dataset['Complaint']\n",
    "        print( self.dataset.head())\n",
    "        \n",
    "    def dataframing(self,dataset):\n",
    "        # creating dataframe for each departments\n",
    "        water = dataset.loc[dataset['Departments'] == 'Water Authority']\n",
    "        pwd = dataset.loc[dataset['Departments'] == 'PWD']\n",
    "        ksrtc = dataset.loc[dataset['Departments'] == 'KSRTC']\n",
    "        kseb = dataset.loc[dataset['Departments'] == 'KSEB']\n",
    "        env = dataset.loc[dataset['Departments'] == 'Environment and climate change']\n",
    "        print(env.shape)    #(30, 4)\n",
    "        #print(water.shape)  #(39, 4)\n",
    "        #print(pwd.shape)    #(42, 4)\n",
    "        #print(ksrtc.shape)  #(17, 4)\n",
    "        #print(kseb.shape)   #(22, 4)\n",
    "        #dataset.head()\n",
    "        #print(pwd)\n",
    "\n",
    "  \n",
    "\n",
    "        #Filtering out Subjects and complaints from the dataframe\n",
    "        df_water = water[['Subject','Complaint']]\n",
    "        df_pwd   = pwd[['Subject','Complaint']]\n",
    "        df_ksrtc = ksrtc[['Subject','Complaint']]\n",
    "        df_kseb  = kseb[['Subject','Complaint']]\n",
    "        df_env   = env[['Subject','Complaint']]\n",
    "\n",
    "        dfwater  = df_water[['Subject','Complaint']]\n",
    "        dfpwd    = df_pwd[['Subject','Complaint']]\n",
    "        dfksrtc  = df_ksrtc[['Subject','Complaint']]\n",
    "        dfkseb   = df_kseb[['Subject','Complaint']]\n",
    "        dfenv    = df_env[['Subject','Complaint']]\n",
    "\n",
    "        \n",
    "        dfwater['Subject_and_Complaint'] = df_water['Subject'] + \" \"+ df_water['Complaint']\n",
    "        dfwater=dfwater[['Subject_and_Complaint']]\n",
    "        \n",
    "        dfpwd['Subject_and_Complaint'] = df_pwd['Subject'] + \" \"+ df_pwd['Complaint']\n",
    "        dfpwd=dfpwd[['Subject_and_Complaint']]\n",
    "        \n",
    "        dfksrtc['Subject_and_Complaint'] = df_ksrtc['Subject'] + \" \"+ df_ksrtc['Complaint']\n",
    "        dfksrtc=dfksrtc[['Subject_and_Complaint']]\n",
    "        \n",
    "        dfkseb['Subject_and_Complaint'] = df_kseb['Subject'] + \" \"+ df_kseb['Complaint']\n",
    "        dfkseb=dfkseb[['Subject_and_Complaint']]\n",
    "        \n",
    "        dfenv ['Subject_and_Complaint'] = df_env['Subject'] + \" \"+ df_env['Complaint']\n",
    "        dfenv =dfenv [['Subject_and_Complaint']]\n",
    "        print(dfenv )\n",
    "        return (dfwater,dfpwd,dfksrtc,dfkseb,dfenv)\n",
    "        \n",
    "        \n",
    "    def tokenisation(self,dfwater,dfpwd,dfksrtc,dfkseb,dfenv):\n",
    "        water_token = []\n",
    "        water_list=[]\n",
    "            #Tokenising water data\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        for i, row in dfwater.iterrows():\n",
    "                #print(i,row['Subject'], row['Complaint'])\n",
    "                \n",
    "            water_token = word_tokenize(row['Subject_and_Complaint'])\n",
    "            result = [i for i in water_token if not i in stop_words]\n",
    "            water_list.append(result)\n",
    "        #print(water_list)\n",
    "    #water_token.append(tokenizer.tokenize(row['Subject_and_Complaint']))\n",
    "        pwd_token = []\n",
    "\n",
    "        pwd_list = []\n",
    "        #Tokenising pwd data  \n",
    "        \n",
    "        for i, row in dfpwd.iterrows():\n",
    "        #print(i,row['Subject'], row['Complaint'])\n",
    "            pwd_token = word_tokenize(row['Subject_and_Complaint'])\n",
    "\n",
    "            result1 = [i for i in pwd_token if not i in stop_words]\n",
    "            pwd_list.append(result1)\n",
    "            #print(pwd_list)\n",
    "            #print( pwd_token)\n",
    "\n",
    "\n",
    "        ksrtc_token =[]\n",
    "        ksrtc_list =[]\n",
    "        #Tokenising ksrtc data    \n",
    "        for i, row in dfksrtc.iterrows():\n",
    "            #print(i,row['Subject'], row['Complaint'])\n",
    "            ksrtc_token = word_tokenize(row['Subject_and_Complaint'])\n",
    "            result = [i for i in ksrtc_token if not i in stop_words]\n",
    "            ksrtc_list.append(result)\n",
    "            #print(ksrtc_list)\n",
    "            #print( ksrtc_token)\n",
    "            \n",
    "            \n",
    "            \n",
    "        kseb_token = []\n",
    "        kseb_list= []\n",
    "        #Tokenising kseb data    \n",
    "        for i, row in dfkseb.iterrows():\n",
    "        #print(i,row['Subject'], row['Complaint'])\n",
    "            kseb_token = word_tokenize(row['Subject_and_Complaint'])\n",
    "            result = [i for i in kseb_token if not i in stop_words]\n",
    "            kseb_list.append(result)\n",
    "        #print(kseb_list)\n",
    "        #print(kseb_token)\n",
    "        \n",
    "        \n",
    "        env_token = []\n",
    "        env_list = []\n",
    "        #Tokenising env data  \n",
    "        \n",
    "        for i, row in dfenv.iterrows():\n",
    "            #print(i,row['Subject'], row['Complaint'])\n",
    "            env_token = word_tokenize(row['Subject_and_Complaint'])\n",
    "            result = [i for i in env_token if not i in stop_words]\n",
    "            env_list.append(result)\n",
    "        print(env_list)\n",
    "            #print(env_token)\n",
    "        return(water_list,pwd_list,ksrtc_list,kseb_list,env_list)\n",
    "\n",
    "\n",
    "   \n",
    "    def word_frequency(self,water_list,pwd_list,ksrtc_list,kseb_list,env_list):\n",
    "        \n",
    "        #word frequencies  Environment department\n",
    "\n",
    "        wordfreq = [1]\n",
    "\n",
    "        count = 0\n",
    "        for word  in env_list:\n",
    "            count+=1\n",
    "            wordfreq.append([])\n",
    "            for i in word:\n",
    "                wordfreq[count].append(word.count(i))\n",
    "        wordfreq.pop(0)\n",
    "        env_freq=list(map(dict, map(zip, env_list, wordfreq)))\n",
    "        #print(env_freq)\n",
    "        \n",
    "        \n",
    "        \n",
    "        #word frequencies  KSEB department\n",
    "        wordfreq = [1]\n",
    "        count = 0\n",
    "        for word  in kseb_list:\n",
    "            count+=1\n",
    "            wordfreq.append([])\n",
    "            for i in word:\n",
    "                wordfreq[count].append(word.count(i))\n",
    "        wordfreq.pop(0)\n",
    "\n",
    "        kseb_freq=list(map(dict, map(zip, kseb_list, wordfreq)))\n",
    "        print(kseb_freq)\n",
    "\n",
    "        \n",
    "      \n",
    "        \n",
    "        \n",
    "        \n",
    "file =   '/home/gayathri/project/MakeComplaint/data.csv'   \n",
    "\n",
    "x= Main(file)\n",
    "\n",
    "x.data_clean()\n",
    "dfwater,dfpwd,dfksrtc,dfkseb,dfenv=x.dataframing(x.dataset)\n",
    "\n",
    "water_list,pwd_list,ksrtc_list,kseb_list,env_list = x.tokenisation(dfwater,dfpwd,dfksrtc,dfkseb,dfenv)\n",
    "x.word_frequency(water_list,pwd_list,ksrtc_list,kseb_list,env_list)\n",
    "\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
