{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using NLTK library, we can do lot of text preprocesing\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "#function to split text into word\n",
    "\n",
    "tokens = word_tokenize(\"The quick brown fox jumps over the lazy dog\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['All', 'work', 'and', 'no', 'play', 'makes', 'jack', 'a', 'dull', 'boy', ',', 'all', 'work', 'and', 'no', 'play']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/user/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "nltk.download('punkt')\n",
    "data = \"All work and no play makes jack a dull boy, all work and no play\"\n",
    "print(word_tokenize(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "data = \"All work and no play makes jack dull boy. All work and no play makes jack a dull boy.\"\n",
    "stopWords = set(stopwords.words('english')) \n",
    "print(stopWords)\n",
    "phrases = sent_tokenize(data)\n",
    "words = word_tokenize(data)\n",
    " \n",
    "print(phrases)\n",
    "print(words)\n",
    "wordsFiltered = []\n",
    "for w in words:\n",
    "    if w not in stopWords:\n",
    "        wordsFiltered.append(w)\n",
    "print(wordsFiltered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "data = \"All work and no play makes jack dull boy. All work and no play makes jack a dull boy.\"\n",
    "stopWords = set(stopwords.words('english')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stopword removal\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "data = \"All work and no play makes jack dull boy. All work and no play makes jack a dull boy.\"\n",
    "stopWords = set(stopwords.words('english')) \n",
    "print(stopWords)\n",
    "words = word_tokenize(data)\n",
    "wordsFiltered = []\n",
    "for w in words:\n",
    "    if w not in stopWords:\n",
    "        wordsFiltered.append(w)\n",
    "print(wordsFiltered)\n",
    "print(len(stopWords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stemming\n",
    "#3 stemmer alogirthms are used here\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "\n",
    "# went is not working\n",
    "words = [\"go\",\"went\",\"gone\",\"games\"]\n",
    "ps = PorterStemmer()\n",
    "lanca_stemmer = LancasterStemmer()\n",
    "sb_stemmer = SnowballStemmer(\"english\",)\n",
    "print(\"=========Porter Stemmer======\")\n",
    "for w in words:\n",
    "    print(w ,\" : \" ,ps.stem(w))\n",
    "    \n",
    "print(\"=========Lanca Stemmer======\")\n",
    "for w in words:\n",
    "\n",
    "    print(w ,\" : \" ,lanca_stemmer.stem(w))\n",
    "    \n",
    "print(\"=========Snowball Stemmer======\")\n",
    "for w in words:\n",
    "    \n",
    "    print(w ,\" : \" ,sb_stemmer.stem(w))\n",
    "    \n",
    "    \n",
    " #Porter stemmer and Snowball stemmer working in similar manner   \n",
    "#lanca is not so good\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#speech tagging\n",
    "\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "\n",
    "document = 'Whether you\\'re new to programming or an experienced developer, it\\'s easy to learn and use Python.'\n",
    "sentences = nltk.sent_tokenize(document) \n",
    "for sent in sentences:\n",
    "    print(nltk.pos_tag(nltk.word_tokenize(sent)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filtering from Speech tagging\n",
    "import nltk\n",
    "from nltk.corpus import state_union\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "\n",
    "document = 'Today the Netherlands celebrates King\\'s Day. To honor this tradition, the Dutch embassy in San Francisco invited me to'\n",
    "sentences = nltk.sent_tokenize(document)  \n",
    "data = []\n",
    "for sent in sentences:\n",
    "    data = data + nltk.pos_tag(nltk.word_tokenize(sent))\n",
    "for w in data:\n",
    "    if 'VB' in w[1]:\n",
    "        print(w)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#natural language prediction\n",
    "import nltk\n",
    "\n",
    " \n",
    "from nltk.corpus import names\n",
    "\n",
    "def gender_features(word): \n",
    "    return {'last_letter': word[-1]}\n",
    "\n",
    "names = ([(name, 'male') for name in names.words('male.txt')] + \n",
    "         [(name, 'female') for name in names.words('female.txt')])\n",
    "\n",
    "featuresets = [(gender_features(n), g) for (n,g) in names]\n",
    "train_set = featuresets\n",
    "classifier = nltk.MaxentClassifier.train(train_set) \n",
    "print(classifier.classify(gender_features('Frank')))\n",
    "name = input(\"Name: \")\n",
    "print(classifier.classify(gender_features(name)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.classify.util\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.corpus import names\n",
    " \n",
    "def word_feats(words):\n",
    "    return dict([(word, True) for word in words])\n",
    " \n",
    "positive_vocab = [ 'awesome', 'outstanding', 'fantastic', 'terrific', 'good', 'nice', 'great', ':)' ]\n",
    "negative_vocab = [ 'bad', 'terrible','useless', 'hate', ':(' ]\n",
    "neutral_vocab = [ 'movie','the','sound','was','is','actors','did','know','words','not' ]\n",
    " \n",
    "positive_features = [(word_feats(pos), 'pos') for pos in positive_vocab]\n",
    "negative_features = [(word_feats(neg), 'neg') for neg in negative_vocab]\n",
    "neutral_features = [(word_feats(neu), 'neu') for neu in neutral_vocab]\n",
    " \n",
    "train_set = negative_features + positive_features + neutral_features\n",
    " \n",
    "classifier = NaiveBayesClassifier.train(train_set) \n",
    " \n",
    "# Predict\n",
    "neg = 0\n",
    "pos = 0\n",
    "sentence = \"Awesome movie, I liked it\"\n",
    "sentence = sentence.lower()\n",
    "words = sentence.split(' ')\n",
    "for word in words:\n",
    "    classResult = classifier.classify( word_feats(word))\n",
    "    if classResult == 'neg':\n",
    "        neg = neg + 1\n",
    "    if classResult == 'pos':\n",
    "        pos = pos + 1\n",
    " \n",
    "print('Positive: ' + str(float(pos)/len(words)))\n",
    "print('Negative: ' + str(float(neg)/len(words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tree ['This', 'is', 'Gayathri', \"'s\", 'book', 'is', \"n't\", 'it', '?']\n",
      "WordPunct ['This', 'is', 'Gayathri', \"'\", 's', 'book', 'isn', \"'\", 't', 'it', '?']\n",
      "porter stemmer\n",
      "Lemmatizer\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import nltk\n",
    "\n",
    "#Tokenize using TreeBank Tokenizer\n",
    "\n",
    "text=\"This is Gayathri's book isn't it?\"\n",
    "\n",
    "tokenizer=nltk.tokenize.TreebankWordTokenizer()\n",
    "tokenizer1=nltk.tokenize.WordPunctTokenizer()\n",
    "\n",
    "tokens=tokenizer.tokenize(text)\n",
    "tokens1=tokenizer1.tokenize(text) #wordPunct\n",
    "\n",
    "print(\"Tree\", tokens)\n",
    "print(\"WordPunct\" ,tokens1)\n",
    "\n",
    "#stemmer\n",
    "stemmer=nltk.stem.PorterStemmer()\n",
    "print(\"porter stemmer\")\n",
    "\" \".join(stemmer.stem(token)for token in tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The quick brown fox jump over the lazy dog'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import nltk\n",
    "\n",
    "#Tokenzie using TreeBank Tokenizer\n",
    "\n",
    "text=\"This is Gayathri's book isn't it?\"\n",
    "\n",
    "lemmatizer=nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "\" \".join(lemmatizer.lemmatize(token)for token in tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'a', 'sample', 'sentence', ',', 'showing', 'off', 'is', 'was', 'kay', 'the', 'stop', 'words', 'filtration', '.']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['This',\n",
       " 'sample',\n",
       " 'sentence',\n",
       " ',',\n",
       " 'showing',\n",
       " 'kay',\n",
       " 'stop',\n",
       " 'words',\n",
       " 'filtration',\n",
       " '.']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "\n",
    "\n",
    "sentence= []\n",
    "def stopword_remove(tokens):\n",
    "      \n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    print(tokens)\n",
    "    for token in tokens:\n",
    "        if token not in stop_words:\n",
    "            sentence.append(token)\n",
    "    return sentence\n",
    "                     \n",
    "    \n",
    "    \n",
    "    \n",
    "example_sent = \"This is a sample sentence, showing off is was kay the stop words filtration.\"\n",
    "\n",
    "tokens = word_tokenize(example_sent)\n",
    "#print(type(tokens))\n",
    "stopword_remove(tokens)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "['a', 't', 'hers', 'do', 'above', 'any', 're', 'up', 'other', 'about', 'doing', 'further', 'then', 'if', 'when', 'myself', \"should've\", 'our', 'we', 'off', 'between', \"wouldn't\", 'because', 'don', 'my', 'being', 'himself', 'down', 'of', 'and', 'for', \"hasn't\", \"you'll\", 'against', 'each', 'them', 'their', 'ma', 'only', 'over', \"you're\", \"shan't\", 'y', 'are', 'o', 'very', 'all', 'your', 'wouldn', 'he', 'below', 'more', 'nor', 'mightn', 'by', 'once', 'while', 'wasn', 'as', \"weren't\", 'yourself', 'who', 'which', 'ourselves', 'ours', 'itself', 'yours', 'herself', 'no', \"won't\", 'haven', 'both', 'not', 'or', 'has', 'what', 'have', 'so', \"needn't\", 'm', 'didn', 'mustn', 'this', 'themselves', 'under', 'you', \"mustn't\", 'be', \"shouldn't\", \"didn't\", \"haven't\", 'same', 'doesn', 'am', 'with', \"wasn't\", 'until', 'she', 'had', 'him', 'it', 'that', 'into', 'shan', 'to', 'just', 'an', 'before', \"she's\", 'such', 'yourselves', 'theirs', 's', 'now', 'but', 'those', \"hadn't\", \"it's\", 'from', 'on', \"couldn't\", 'hadn', \"that'll\", \"don't\", 'is', 'the', 'having', 'out', 'd', 'during', 'at', 'won', 'was', 'here', 'were', 'me', \"isn't\", 'i', 'they', \"aren't\", 'his', 'too', 'there', \"you'd\", 'isn', 'been', \"you've\", 've', 'shouldn', 'again', 'most', 'did', 'should', 'its', 'how', \"doesn't\", 'whom', 'after', 'needn', 'weren', 'in', 'these', 'aren', 'does', 'ain', 'will', 'couldn', 'through', 'why', 'own', 'few', 'hasn', 'can', 'than', 'some', 'll', 'where', 'her', \"mightn't\"]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['This',\n",
       " 'sample',\n",
       " 'sentence',\n",
       " ',',\n",
       " 'showing',\n",
       " 'kay',\n",
       " 'stop',\n",
       " 'words',\n",
       " 'filtration',\n",
       " '.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "\n",
    "\n",
    "sentence= []\n",
    "def stopword_remove(tokens):\n",
    "  \n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    stop_words = list(stop_words)\n",
    "    print(stop_words)\n",
    "    for token in tokens:\n",
    "        if token not in stop_words:\n",
    "            sentence.append(token)\n",
    "    return sentence\n",
    "                     \n",
    "    \n",
    "    \n",
    "    \n",
    "example_sent = \"This is a sample sentence, showing off is was kay the stop words filtration.\"\n",
    "tokens = word_tokenize(example_sent)\n",
    "print(type(tokens))\n",
    "stopword_remove(tokens)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NLTK', 'leading', 'platform', 'building', 'Python', 'programs', 'work', 'human', 'language', 'data', '.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "input_str = \"NLTK is a leading platform for building Python programs to work with human language data.\"\n",
    "stop_words = set(stopwords.words('english'))\n",
    "from nltk.tokenize import word_tokenize\n",
    "tokens = word_tokenize(input_str)\n",
    "result = [i for i in tokens if not i in stop_words]\n",
    "print (result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of stop words: 305\n",
      "First ten stop words: ['everywhere', 'a', 'noone', 'hers', 'do', 'onto', 'another', 'above', 'any', 'somewhere']\n",
      "\n",
      "['NLTK', 'leading', 'platform', 'building', 'Python', 'programs', 'work', 'human', 'language', 'data', '.']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "spacy_nlp = spacy.load('en_core_web_sm')\n",
    "spacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
    "print('Number of stop words: %d' % len(spacy_stopwords))\n",
    "print('First ten stop words: %s' % list(spacy_stopwords)[:10])\n",
    "doc = spacy_nlp('NLTK is a leading platform for building Python programs to work with human language data.')\n",
    "tokens = [token.text for token in doc if not token.is_stop]\n",
    "\n",
    "print()\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'spacy.tokens.doc.Doc'>\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'token' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-ea404cf1237f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;31m#tokenizer=nltk.tokenize.TreebankWordTokenizer()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m     \u001b[0mwater_token\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Subject_and_Complaint'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m     \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mwater_token\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_stop\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m     \u001b[0;31m#result = [i for i in water_token if not i in stop_words]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-30-ea404cf1237f>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;31m#tokenizer=nltk.tokenize.TreebankWordTokenizer()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m     \u001b[0mwater_token\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Subject_and_Complaint'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m     \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mwater_token\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_stop\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m     \u001b[0;31m#result = [i for i in water_token if not i in stop_words]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'token' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords \n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize \n",
    "import spacy\n",
    "spacy_nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "\n",
    "#Data cleaning\n",
    "\n",
    "# 1. unpunctuate \n",
    "# 2. to lower\n",
    "# 3. Remove numerals\n",
    "# 4. Remove Newline for subject\n",
    "\n",
    "\n",
    "# Data loading\n",
    "dataset= pd.read_csv('/home/user/Complaint/MakeComplaint/data.csv')\n",
    "\n",
    "\n",
    "# unpunctuate and lower case\n",
    "dataset['Subject'] = dataset['Subject'].str.replace('[^\\w\\s]','').str.lower()\n",
    "\n",
    "\n",
    "# unpunctuate and lower case\n",
    "dataset['Complaint'] = dataset['Complaint'].str.replace('[^\\w\\s]','').str.lower() \n",
    "\n",
    "\n",
    "#rRemoving new lines in the subject field\n",
    "dataset['Subject'] = dataset['Subject'].str.rstrip('\\n')\n",
    "\n",
    "#removing Numeric \n",
    "dataset['Complaint'] = dataset['Complaint']\n",
    "\n",
    "\n",
    "# creating dataframe for each departments\n",
    "water = dataset.loc[dataset['Departments'] == 'Water Authority']\n",
    "pwd = dataset.loc[dataset['Departments'] == 'PWD']\n",
    "ksrtc = dataset.loc[dataset['Departments'] == 'KSRTC']\n",
    "kseb = dataset.loc[dataset['Departments'] == 'KSEB']\n",
    "env = dataset.loc[dataset['Departments'] == 'Environment and climate change']\n",
    "\n",
    "#print(env.shape)    #(29, 4)\n",
    "#print(water.shape)  #(17, 4)\n",
    "#print(pwd.shape)    #(39, 4)\n",
    "#print(ksrtc.shape)  #(13, 4)\n",
    "#print(kseb.shape)   #(22, 4)\n",
    "#dataset.head()\n",
    "#print(pwd)\n",
    "\n",
    "#Filtering out Subjects and complaints from the dataframe\n",
    "df_water = water[['Subject','Complaint']]\n",
    "df_pwd   = pwd[['Subject','Complaint']]\n",
    "df_ksrtc = ksrtc[['Subject','Complaint']]\n",
    "df_kseb  = kseb[['Subject','Complaint']]\n",
    "df_env   = env[['Subject','Complaint']]\n",
    "\n",
    "dfwater  = df_water[['Subject','Complaint']]\n",
    "dfpwd    = df_pwd[['Subject','Complaint']]\n",
    "dfksrtc  = df_ksrtc[['Subject','Complaint']]\n",
    "dfkseb   = df_kseb[['Subject','Complaint']]\n",
    "dfenv    = df_env[['Subject','Complaint']]\n",
    "\n",
    "\n",
    "#Dataframe with complaint and subject as one column = Water\n",
    "dfwater['Subject_and_Complaint'] = df_water['Subject'] + \" \"+ df_water['Complaint']\n",
    "dfwater=dfwater[['Subject_and_Complaint']]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Dataframe with complaint and subject as one column = PWD\n",
    "dfpwd['Subject_and_Complaint'] = df_pwd['Subject'] + \" \"+ df_pwd['Complaint']\n",
    "dfpwd=dfpwd[['Subject_and_Complaint']]\n",
    "#print(dfpwd)\n",
    "\n",
    "#Dataframe with complaint and subject as one column = ksrtc\n",
    "dfksrtc['Subject_and_Complaint'] = df_ksrtc['Subject'] + \" \"+ df_ksrtc['Complaint']\n",
    "dfksrtc=dfksrtc[['Subject_and_Complaint']]\n",
    "#print(dfksrtc)\n",
    "\n",
    "#Dataframe with complaint and subject as one column = kseb\n",
    "dfkseb['Subject_and_Complaint'] = df_kseb['Subject'] + \" \"+ df_kseb['Complaint']\n",
    "dfkseb=dfkseb[['Subject_and_Complaint']]\n",
    "#print(dfkseb)\n",
    "\n",
    "\n",
    "#Dataframe with complaint and subject as one column = env\n",
    "dfenv ['Subject_and_Complaint'] = df_env['Subject'] + \" \"+ df_env['Complaint']\n",
    "dfenv =dfenv [['Subject_and_Complaint']]\n",
    "#print(dfenv )\n",
    "\n",
    "#==================================Tokenization Begins : =============================================\n",
    "\n",
    "\n",
    "print(type(doc ))\n",
    "#print(tokens)\n",
    "\n",
    "sentence= []\n",
    "\"\"\"def stopword_remove(tokens):\n",
    "  \n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    stop_words = list(stop_words)\n",
    "    \n",
    "    for token in tokens:\n",
    "        if token not in stop_words:\n",
    "            sentence.append(token)\n",
    "    return sentence\"\"\"\n",
    "    \n",
    "                     \n",
    "\n",
    "spacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
    "\n",
    "water_token = []\n",
    "#Tokenising water data\n",
    "stop_words = set(stopwords.words('english'))\n",
    "for i, row in dfwater.iterrows():\n",
    "    #print(i,row['Subject'], row['Complaint'])\n",
    "    #tokenizer=nltk.tokenize.TreebankWordTokenizer()\n",
    "    water_token = word_tokenize(row['Subject_and_Complaint'])\n",
    "    tokens = [token.text for water_token in doc if not token.is_stop]\n",
    "    #result = [i for i in water_token if not i in stop_words]\n",
    "    print(result)\n",
    "    #water_token.append(tokenizer.tokenize(row['Subject_and_Complaint']))\n",
    "\n",
    "  \n",
    "   \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#print(type(tokens))\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "pwd_token = []\n",
    "#Tokenising pwd data    \n",
    "for i, row in dfpwd.iterrows():\n",
    "    #print(i,row['Subject'], row['Complaint'])\n",
    "    tokenizer=nltk.tokenize.TreebankWordTokenizer()\n",
    "    pwd_token.append(tokenizer.tokenize(row['Subject_and_Complaint']))\n",
    "#print( pwd_token)\n",
    "\n",
    "ksrtc_token =[]\n",
    "#Tokenising ksrtc data    \n",
    "for i, row in dfksrtc.iterrows():\n",
    "    #print(i,row['Subject'], row['Complaint'])\n",
    "    tokenizer=nltk.tokenize.TreebankWordTokenizer()\n",
    "    ksrtc_token.append(tokenizer.tokenize(row['Subject_and_Complaint']))\n",
    "#print( ksrtc_token)\n",
    "\n",
    "kseb_token = []\n",
    "#Tokenising kseb data    \n",
    "for i, row in dfkseb.iterrows():\n",
    "    #print(i,row['Subject'], row['Complaint'])\n",
    "    tokenizer=nltk.tokenize.TreebankWordTokenizer()\n",
    "    kseb_token.append(tokenizer.tokenize(row['Subject_and_Complaint']))\n",
    "#print(kseb_token)\n",
    "\n",
    "env_token = []\n",
    "#Tokenising env data    \n",
    "for i, row in dfenv.iterrows():\n",
    "    #print(i,row['Subject'], row['Complaint'])\n",
    "    tokenizer=nltk.tokenize.TreebankWordTokenizer()\n",
    "    env_token.append(tokenizer.tokenize(row['Subject_and_Complaint']))\n",
    "#print(env_token)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================Original==================================================================\n",
      "There is a road going towards Trivandrum Technopark Phase 3 and it has a gate towards the Technopark buildings from this road. It is starting from Nippol Toyota and going towards Kallingal . The road name is Kallingal - Attinkuzhy Road. This Road is damaged for last 1.5 years and no one is caring about this. there are minimum 300 vehicles daily goes through it in the morning and evening towards offices. Now it is like 1 meter gutter in the middle of the road making health issues to the people. It is in very danger situation. URGENT action required.\n",
      "===================================Processed================================================================\n",
      "there is a road going towards trivandrum technopark phase  and it has a gate towards the technopark buildings from this road. it is starting from nippol toyota and going towards kallingal . the road name is kallingal - attinkuzhy road. this road is damaged for last . years and no one is caring about this. there are minimum  vehicles daily goes through it in the morning and evening towards offices. now it is like  meter gutter in the middle of the road making health issues to the people. it is in very danger situation. urgent action required.\n",
      "=========================================Summary==========================================================\n",
      "it is starting from nippol toyota and going towards kallingal .\n",
      "the road name is kallingal - attinkuzhy road.\n",
      "this road is damaged for last .\n",
      "now it is like  meter gutter in the middle of the road making health issues to the people.\n",
      "==================================Keywords=================================================================\n",
      "technopark\n",
      "road\n",
      "meter\n",
      "vehicles daily\n"
     ]
    }
   ],
   "source": [
    "from gensim.summarization import summarize\n",
    "from gensim.summarization import keywords\n",
    "from nltk.corpus import stopwords \n",
    "import nltk\n",
    "import re\n",
    "\n",
    "\n",
    "\n",
    "text = \"There is a road going towards Trivandrum Technopark Phase 3 and it has a gate towards the \"+ \\\n",
    "\"Technopark buildings from this road. It is starting from Nippol Toyota and going towards \"+\\\n",
    "\"Kallingal . The road name is Kallingal - Attinkuzhy Road. This Road is damaged for last 1.5 \"+\\\n",
    "\"years and no one is caring about this. there are minimum 300 vehicles daily goes through it in \"+\\\n",
    "\"the morning and evening towards offices. Now it is like 1 meter gutter in the middle of the road \"+\\\n",
    " \"making health issues to the people. It is in very danger situation. URGENT action required.\"\n",
    "print(\"================================Original==================================================================\")\n",
    "print(text)\n",
    "\n",
    "text = text.strip().lower()\n",
    "\n",
    "#text=text.re.sub('[^a-zA-Z ]',\"\")\n",
    "\n",
    "#text = nltk.sent_tokenize(text)\n",
    "output = re.sub(r'\\d+', '', text)\n",
    "print(\"===================================Processed================================================================\")\n",
    "print(output)\n",
    "\n",
    "print(\"=========================================Summary==========================================================\")\n",
    "print(summarize(output, ratio = 0.5))\n",
    "print(\"==================================Keywords=================================================================\")\n",
    "\n",
    "print(keywords(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['There', 'road', 'going', 'towards', 'Trivandrum', 'Technopark', 'Phase', 'gate', 'towards', 'Technopark', 'buildings', 'road', 'It', 'starting', 'Nippol', 'Toyota', 'going', 'towards', 'Kallingal', 'The', 'road', 'name', 'Kallingal', 'Attinkuzhy', 'Road', 'This', 'Road', 'damaged', 'last', 'years', 'one', 'caring', 'minimum', 'vehicles', 'daily', 'goes', 'morning', 'evening', 'towards', 'offices', 'Now', 'like', 'meter', 'gutter', 'middle', 'road', 'making', 'health', 'issues', 'people', 'It', 'danger', 'situation', 'URGENT', 'action', 'required']\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "#method 2 summarisation\n",
    "from gensim.summarization import summarize\n",
    "from gensim.summarization import keywords\n",
    "from nltk.corpus import stopwords \n",
    "import nltk\n",
    "import re\n",
    "\n",
    "\n",
    "article_text = \"There is a road going towards Trivandrum Technopark Phase 3 and it has a gate towards the \"+ \\\n",
    "\"Technopark buildings from this road. It is starting from Nippol Toyota and going towards \"+\\\n",
    "\"Kallingal . The road name is Kallingal - Attinkuzhy Road. This Road is damaged for last 1.5 \"+\\\n",
    "\"years and no one is caring about this. there are minimum 300 vehicles daily goes through it in \"+\\\n",
    "\"the morning and evening towards offices. Now it is like 1 meter gutter in the middle of the road \"+\\\n",
    " \"making health issues to the people. It is in very danger situation. URGENT action required.\"\n",
    "stop_words = set(stopwords.words('english'))\n",
    "article_text = re.sub(r'\\[[0-9]*\\]', ' ', article_text)  \n",
    "article_text = re.sub(r'\\s+', ' ', article_text)  \n",
    "formatted_article_text = re.sub('[^a-zA-Z]', ' ', article_text )  \n",
    "formatted_article_text = re.sub(r'\\s+', ' ', formatted_article_text)  \n",
    "sentence_list = nltk.sent_tokenize(article_text)  \n",
    "word_frequencies = []\n",
    "tokenize = nltk.word_tokenize(formatted_article_text)\n",
    "\n",
    "tok =[]\n",
    "for word in tokenize:\n",
    "    result = [i for i in tokenize if not i in stop_words]\n",
    "print(result) \n",
    "\n",
    "word_frequencies = {}  \n",
    "for word in  tokenize:\n",
    "     if word not in stop_words:\n",
    "            if word not in word_frequencies.keys():\n",
    "                word_frequencies[word] = 1\n",
    "            else:\n",
    "                word_frequencies[word] += 1\n",
    "                \n",
    "maximum_frequncy = max(word_frequencies.values()) \n",
    "print(maximum_frequncy)\n",
    "for word in word_frequencies.keys(): \n",
    "     word_frequencies[word] = (word_frequencies[word]/maximum_frequncy)\n",
    "        \n",
    "sentence_scores = {} \n",
    "for sent in sentence_list:\n",
    "    for word in nltk.word_tokenize(sent.lower()):if word in word_frequencies.keys():\n",
    "            \n",
    "        if word in word_frequencies.keys():\n",
    "            if len(sent.split(' ')) < 30:\n",
    "                \n",
    "            \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hellomzcsdn,,m\n",
    "df\n",
    "v\n",
    "fd\n",
    "vjjd\n",
    "ds\n",
    "fsnhs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['The', 'Saudis', 'are', 'preparing', 'a', 'report', 'that', 'will', 'acknowledge', 'that'], ['Saudi', 'journalist', 'Jamal', \"Khashoggi's\", 'death', 'was', 'the', 'result', 'of', 'an'], ['interrogation', 'that', 'went', 'wrong,', 'one', 'that', 'was', 'intended', 'to', 'lead'], ['to', 'his', 'abduction', 'from', 'Turkey,', 'according', 'to', 'two', 'sources.']]\n",
      "Dictionary(33 unique tokens: ['Turkey,', 'his', 'that', 'interrogation', 'death']...)\n",
      "{'Turkey,': 26, 'his': 30, 'that': 7, 'interrogation': 20, 'death': 13, 'lead': 21, 'an': 12, 'of': 15, 'to': 23, 'abduction': 27, \"Khashoggi's\": 10, 'according': 28, 'one': 22, 'wrong,': 25, 'went': 24, 'sources.': 31, 'preparing': 5, 'journalist': 14, 'The': 1, 'a': 2, 'acknowledge': 3, 'two': 32, 'the': 17, 'from': 29, 'report': 6, 'Saudi': 11, 'Jamal': 9, 'intended': 19, 'will': 8, 'result': 16, 'was': 18, 'are': 4, 'Saudis': 0}\n",
      "Dictionary(48 unique tokens: ['his', 'that', 'paths', 'interrogation', 'acknowledge']...)\n",
      "{'his': 30, 'that': 7, 'paths': 36, 'interrogation': 20, 'acknowledge': 3, 'death': 13, 'to': 23, 'intended': 19, 'and': 41, 'A': 46, 'Saudi': 11, 'well': 45, 'in': 34, 'abduction': 27, 'IV': 39, 'according': 28, 'two': 32, 'report': 6, 'ordering': 43, 'result': 16, 'intersection': 35, 'are': 4, 'lead': 21, 'Turkey,': 26, 'of': 15, 'an': 12, 'trees': 37, \"Khashoggi's\": 10, 'survey': 47, 'minors': 42, 'one': 22, 'sources.': 31, 'journalist': 14, 'The': 1, 'a': 2, 'graph': 33, 'the': 17, 'from': 29, 'wrong,': 25, 'preparing': 5, 'Widths': 40, 'Graph': 38, 'went': 24, 'will': 8, 'Jamal': 9, 'quasi': 44, 'was': 18, 'Saudis': 0}\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim import corpora\n",
    "from pprint import pprint\n",
    "\n",
    "documents = [\"The Saudis are preparing a report that will acknowledge that\", \n",
    "             \"Saudi journalist Jamal Khashoggi's death was the result of an\", \n",
    "             \"interrogation that went wrong, one that was intended to lead\", \n",
    "             \"to his abduction from Turkey, according to two sources.\"]\n",
    "\n",
    "documents_2 = [\"One source says the report will likely conclude that\", \n",
    "                \"the operation was carried out without clearance and\", \n",
    "                \"transparency and that those involved will be held\", \n",
    "                \"responsible. One of the sources acknowledged that the\", \n",
    "                \"report is still being prepared and cautioned that\", \n",
    "                \"things could change.\"]\n",
    "\n",
    "# Tokenize(split) the sentences into words\n",
    "texts = [[text for text in doc.split()] for doc in documents]\n",
    "\n",
    "print(texts)\n",
    "\n",
    "# Create dictionary\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "\n",
    "print(dictionary)\n",
    "\n",
    "# Show the word to id map\n",
    "print(dictionary.token2id)\n",
    "\n",
    "# adding new doc to dictionary\n",
    "documents_2 = [\"The intersection graph of paths in trees\",\n",
    "               \"Graph minors IV Widths of trees and well quasi ordering\",\n",
    "               \"Graph minors A survey\"]\n",
    "\n",
    "texts_2 = [[text for text in doc.split()] for doc in documents_2]\n",
    "\n",
    "dictionary.add_documents(texts_2)\n",
    "\n",
    "\n",
    "print(dictionary)\n",
    "\n",
    "\n",
    "print(dictionary.token2id)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'apple': 3, 'egg': 2, 'banana': 1})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "list1=['apple','egg','apple','banana','egg','apple']\n",
    "counts = Counter(list1)\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-32-440e5399e1f1>, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-32-440e5399e1f1>\"\u001b[0;36m, line \u001b[0;32m5\u001b[0m\n\u001b[0;31m    for j in len(:\u001b[0m\n\u001b[0m                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "list1 = [['a','b','c'],['a','a','b']]\n",
    "wordfreq = []\n",
    "inner_list=[]\n",
    "for i in len(list1):\n",
    "    for j in len(:\n",
    "        inner_list.append(i.count(j))\n",
    "    wordfreq.append(inner_list)\n",
    "print(wordfreq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 'Group2', 'A': 'Group1', 'D': 'Group2', 'B': 'Group1'}\n"
     ]
    }
   ],
   "source": [
    "groups = [['Group1', 'A', 'B'], ['Group2', 'C', 'D']]\n",
    "\n",
    "result = {}\n",
    "for group in groups:\n",
    "    for item in group[1:]:\n",
    "        result[item] = group[0]\n",
    "print (result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e 30\n",
      "b 13\n",
      "d 4\n",
      "c 2\n",
      "a 1\n",
      "{'f': 4, 'd': 4, 'a': 1, 'c': 2, 'b': 13, 'e': 30}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "a1 = {'a':1, 'b':13, 'd':4, 'c':2, 'e':30}\n",
    "a1_sorted_keys = sorted(a1, key=a1.get, reverse=True)\n",
    "for r in a1_sorted_keys:\n",
    "    print(r, a1[r])\n",
    "a1.update({'f':4})\n",
    "print(a1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('b', 23), ('d', 17), ('c', 5), ('a', 2), ('e', 1)]\n"
     ]
    }
   ],
   "source": [
    "d = {'a':2, 'b':23, 'c':5, 'd':17, 'e':1}\n",
    "items = [(v, k) for k, v in d.items()]\n",
    "items.sort()\n",
    "items.reverse()\n",
    "items = [(k, v) for v, k in items]\n",
    "print(items)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/gayathri/project/MakeComplaint/data.csv\n",
      "   id Subject                                          Complaint  \\\n",
      "0   1          no water pipeline connection in amma gardens r...   \n",
      "1   2          i am a resident of sreekaryam  ambadi nagar la...   \n",
      "2   3          as everyone knows that electricity is the majo...   \n",
      "3   4          there is scarcity of water in my area vanchiyo...   \n",
      "4   5          there is a huge scarcity of water in remote ar...   \n",
      "\n",
      "       Departments  \n",
      "0  Water Authority  \n",
      "1              PWD  \n",
      "2             KSEB  \n",
      "3  Water Authority  \n",
      "4  Water Authority  \n",
      "(30, 4)\n",
      "                                 Subject_and_Complaint\n",
      "12    the number. of deaths from cardiovascular dis...\n",
      "13    people make river water polluted by dumping h...\n",
      "19    due to the sand mining there is more of effec...\n",
      "20    in our areas during night some strangers are ...\n",
      "33    fishes in the river pampa are dying massively...\n",
      "40    construction is going on the trivandrum in th...\n",
      "42    many industries dump wastes into rivers  lake...\n",
      "43    burning of plastics in public places cause a ...\n",
      "44    dumping of wastes in the public places causes...\n",
      "45    activities like waste disposal from residenti...\n",
      "46    overfishing  which causes a reduction in dive...\n",
      "47    artificial light at night is one of the most ...\n",
      "48    the electronic waste problem is huge. the ele...\n",
      "49    ocean acidification is caused when co₂ dissol...\n",
      "50    noise produced by vehicles  political parties...\n",
      "58    now a days many people are cutting down the t...\n",
      "62    polution by ksrtc bus is very heavy today .so...\n",
      "65    quarries are bad for the environment in sever...\n",
      "70                           save alappad stop mining.\n",
      "72    i would like to inform you that city resident...\n",
      "74    preserving biodiversity  saving forest can he...\n",
      "100   intolerable temperature change in summer seas...\n",
      "103                        intense heat during summer.\n",
      "110   one among the factors which shaped kerala as ...\n",
      "112   due to excessive use of chlorine and other di...\n",
      "113   we should have more projects on planting tree...\n",
      "114   need proper regulations to control the pollut...\n",
      "116   increased deforestation  uncontrolled constru...\n",
      "118   the pollution levels in the kochi city is ris...\n",
      "120   waste disposal in public premises make life h...\n",
      "[['number', '.', 'deaths', 'cardiovascular', 'disease', 'attributed', 'air', 'pollution', 'much', 'higher', 'expected.air', 'pollution', 'caused', 'twice', 'many', 'deaths', 'cvd', 'respiratory', 'diseases', '.'], ['people', 'make', 'river', 'water', 'polluted', 'dumping', 'household', 'wastage', 'industrial', 'wastage', '.'], ['due', 'sand', 'mining', 'effect', 'ecosystem', 'severe', 'impact', 'plants', 'animals', 'rivers', '.'], ['areas', 'night', 'strangers', 'dumping', 'hotel', 'waste', 'domestic', 'waste', 'foul', 'smelling', 'sides', 'road', '.', 'wastes', 'remain', 'th', 'road', 'uncleaned', '.', 'camera', 'survelliance', 'facility', 'catch', 'people', 'throwing', 'wastes', '.'], ['fishes', 'river', 'pampa', 'dying', 'massively', 'due', 'deposits', 'oil', 'factories', 'nearby', '.'], ['construction', 'going', 'trivandrum', 'paddy', 'fields', '.', 'please', 'take', 'necessary', 'steps', 'save', 'farming', '.'], ['many', 'industries', 'dump', 'wastes', 'rivers', 'lakes', 'ponds', 'streams', 'attempt', 'hide', 'wastes', 'epa', 'nspectors', '.', 'water', 'sources', 'feed', 'major', 'crops', 'food', 'becomes', 'contaminated', 'variety', 'chemicals', 'bacteria', 'causing', 'rampant', 'health', 'problems', '.'], ['burning', 'plastics', 'public', 'places', 'cause', 'major', 'health', 'concern', '.', 'rate', 'lung', 'cancer', 'patients', 'increasing', 'day', 'day', 'high', 'time', 'check', 'activities', '.'], ['dumping', 'wastes', 'public', 'places', 'causes', 'major', 'health', 'issues', 'inviting', 'eradicated', 'disease', '.', 'mechanism', 'collect', 'wastes', 'households', 'disposed', 'properly', '.'], ['activities', 'like', 'waste', 'disposal', 'residential', 'commercial', 'industrial', 'areas', 'oil', 'spills', 'runoff', 'agriculture', 'contaminate', 'bodies', 'water', '.'], ['overfishing', 'causes', 'reduction', 'diversity', 'marine', 'life', '.', 'fishermen', 'considering', 'breeding', 'time', '.'], ['artificial', 'light', 'night', 'one', 'obvious', 'physical', 'changes', 'humans', 'made', 'biosphere', 'artificial', 'light', 'also', 'affects', 'dispersal', 'orientation', 'migration', 'hormone', 'levels', 'resulting', 'disrupted', 'circadian', 'rhythms', '.'], ['electronic', 'waste', 'problem', 'huge', '.', 'electronics', 'end', 'landfills', 'toxics', 'like', 'lead', 'mercury', 'cadmium', 'leach', 'soil', 'water', '.'], ['ocean', 'acidification', 'caused', 'co₂', 'dissolves', 'ocean', 'bonding', 'sea', 'water', 'creating', 'carbonic', 'acid', '.', 'acid', 'reduces', 'ph', 'levels', 'water', '.'], ['noise', 'produced', 'vehicles', 'political', 'parties', 'religious', 'centers', 'causes', 'great', 'harm', 'human', 'ears', '.'], ['days', 'many', 'people', 'cutting', 'trees', 'purposes', '.', 'example', 'many', 'oragansations', 'builders', 'cut', 'trees', 'build', 'projects', '.', 'cause', 'real', 'harm', 'humans', '.', 'also', 'utilizing', 'fertile', 'paddy', 'fields', 'construction', '.', 'affect', 'environment', 'oxygen', 'content', 'air', 'become', 'low', '.', 'oxygen', 'content', 'level', 'decreases', 'air', 'survival', 'living', 'beings', 'move', 'harder', 'way', '.', 'also', 'major', 'factor', 'ozone', 'depletion', '.', 'ozone', 'holes', 'formed', 'harmful', 'radiations', 'enter', 'earth', 'holes', 'causes', 'real', 'harm', 'living', 'beings', '.', 'continues', 'way', 'threatening', 'part', 'lives', '.', 'please', 'take', 'serious', 'issue', 'make', 'necessary', 'useful', 'remedies', '.'], ['polution', 'ksrtc', 'bus', 'heavy', 'today', '.so', 'government', 'fix', 'soon', 'possible', '.'], ['quarries', 'bad', 'environment', 'several', 'ways', '.they', 'abruptly', 'interrupt', 'continuity', 'open', 'space', 'cause', 'soil', 'erosion', 'air', 'dust', 'pollution', 'deterioration', 'water', 'quality', '.', 'residential', 'area', 'create', 'noise', 'hazards', '.', 'request', 'higher', 'authority', 'investigate', 'punish', 'officials', 'hand', 'quarrying', 'illegal', 'mining', '.', 'time', 'government', 'create', 'awareness', 'potentially', 'negative', 'impact', 'quarrying', '.'], ['save', 'alappad', 'stop', 'mining', '.'], ['would', 'like', 'inform', 'city', 'residents', 'facing', 'difficulties', 'dump', 'domestic', 'wastes', '.', 'humbly', 'request', 'take', 'necessary', 'actions', '.'], ['preserving', 'biodiversity', 'saving', 'forest', 'help', 'overcome', 'issues..also', 'recycling', 'avoiding', 'usage', 'plastic', 'efficient', 'use', 'fuel', '.'], ['intolerable', 'temperature', 'change', 'summer', 'season', 'cause', 'dried', 'rivers', 'skin', 'disease', '.'], ['intense', 'heat', 'summer', '.'], ['one', 'among', 'factors', 'shaped', 'kerala', \"'gods\", 'country', \"'\", 'euphoric', 'climate', 'never', 'touched', 'extremes', '.', 'thanks', 'moderating', 'influence', 'sea', 'mighty', 'western', 'ghats', '.', 'scenario', 'changed', 'lot', '.', 'kerala', 'witnessing', 'unprecedental', 'surge', 'temperature', '.', \"'kumbhachoodu\", \"'\", 'termed', 'old', 'generation', 'time', 'high', 'impacting', 'livelihood', 'many', '.'], ['due', 'excessive', 'use', 'chlorine', 'disinfectants', 'water', 'causing', 'long', 'term', 'health', 'problems', 'like', 'hair', 'fall', 'rashes', 'etc', '.', 'amount', 'disinfectants', 'used', 'water', 'supplied', 'need', 'reconsidered', '.'], ['projects', 'planting', 'trees', '.', \"n't\", 'cut', 'big', 'grown-up', 'trees', '.'], ['need', 'proper', 'regulations', 'control', 'pollution', '.'], ['increased', 'deforestation', 'uncontrolled', 'construction/development', 'activities', 'vehicle', 'emissions', 'heavily', 'contribute', 'increase', 'temperature', '.', 'government', 'build', 'new', 'policies', 'also', 'consider', 'switching', 'renewable', 'energy', 'sources', '.'], ['pollution', 'levels', 'kochi', 'city', 'rising', 'rapidly', '.', 'government', 'initiate', 'steps', 'curb', '.'], ['waste', 'disposal', 'public', 'premises', 'make', 'life', 'hard', 'people', 'living', 'around', '.']]\n",
      "\n",
      "========= Word Frequency of WATER =========== \n",
      "\n",
      "\n",
      "[{'connection': 1, 'gardens': 1, 'erattakalangu': 1, 'water': 1, 'residential': 1, 'pipeline': 1, 'amma': 1, 'malayinkeezhu': 1, 'area': 1, '.': 1}, {'trouble': 1, 'work': 1, 'water': 1, 'office': 1, 'people': 1, 'scarcity': 1, 'getting': 1, 'ready': 1, 'time': 1, 'day': 1, 'great': 1, '.': 2, 'creates': 1, 'etc': 1, 'schools': 1, 'vanchiyoor': 1, 'area': 1}, {'exiss': 1, 'humbly': 1, 'distances': 1, 'people': 1, 'huge': 1, 'kerala': 1, 'near': 1, 'future': 1, 'si': 1, 'fetch': 1, 'areas': 2, '.': 3, 'queue': 1, 'problem': 2, 'hilly': 1, 'remote': 1, 'idukki': 1, 'tackle': 1, 'actions': 1, 'standing': 1, 'scarcity': 1, 'district': 1, 'take': 1, 'walk': 1, 'request': 1, 'water': 2, 'large': 1, 'necessary': 1}, {'scarcity': 1, 'frequent': 1, 'water': 1, '.': 1, 'morning': 1}, {'available': 1, 'water': 4, 'supply': 3, 'houses': 1, 'needs': 1, 'look': 1, 'matter': 1, 'necessary': 1, '.': 4, 'problem': 2, 'solving': 1, 'area': 1, 'get': 2, 'locality': 1, 'take': 1, 'rid': 1, 'made': 1, 'people': 1, 'therefore': 1, 'request': 1, 'concerning': 1, 'kindly': 1, 'daily': 1, '100': 1, 'there.people': 1, 'steps': 1, 'may': 2, 'seriously': 1, 'authorities': 1}, {'regarding': 1, 'closed': 1, 'also': 1, 'water': 6, 'supply': 2, 'year': 1, 'hardly': 1, 'stand': 1, 'day': 1, 'week': 2, 'connection': 1, 'dues': 1, 'thread': 2, 'authority': 1, '.': 10, 'stayed': 2, 'approx': 1, 'road': 1, 'without': 1, 'big': 1, '(': 2, 'glue': 1, 'per': 1, 'last': 1, 'things': 1, '20000': 2, 'liters': 1, 'kovalam': 3, 'nights': 1, 'called': 1, 'filled': 1, 'immediate': 1, 'fill': 1, 'use': 1, 'cap': 1, 'caped': 1, 'second': 1, '3': 2, 'staff': 1, 'two': 1, '4': 1, 'steal': 1, 'bill': 1, 'pipe': 4, 'grove': 1, 'huge': 1, 'storage': 1, 'near': 1, 'small': 1, 'side': 1, 'bus': 1, 'usually': 1, 'building': 1, 'fix': 1, 'beach': 2, 'ilters': 1, 'please': 2, '1': 1, 'days': 1, 'know': 1, 'action': 2, 'take': 2, 'necessary': 1, 'palm': 1, 'capacity': 2, 'sealed': 3, ')': 2, 'connector': 1, 'stealing': 1, 'lengthy': 1, 'connected': 1, 'told': 1, 'hotel': 3, 'appox': 1, 'came': 1}, {'causes': 1, 'water': 2, 'leakage': 1, 'locality': 1, 'unrepaired': 1, 'shortage': 1, 'areas': 1, 'still': 1, '.': 2}, {'unfits': 1, 'cooking': 1, 'water': 1, 'purposes': 1, '.': 2, 'flowing': 1, 'drinking': 1, 'pipe': 1, 'smell': 1, 'foul': 1, 'therefore': 1}, {'connection': 1, 'bills': 1, 'amount': 1, 'cause': 1, '.': 1, 'consumer': 1, 'huge': 1, 'authorised': 1, 'unauthorised': 1, 'mains': 1, 'water': 1}, {'thus': 1, 'leads': 1, 'city': 1, '.': 1, 'pipe': 1, 'public': 1, 'shortage': 1, 'leaking': 1, 'triivandrum': 1, 'water': 1}, {'days': 1, 'water': 2, 'action': 1, 'drinking': 1, 'flowing': 1, 'take': 1, 'pipe': 1, 'near': 1, '2': 1, 'puthoor': 1, 'necessary': 1, '.': 2, 'broken': 1, 'junction': 1, 'please': 1}, {'shortage': 1, 'water': 1, 'drinking': 1, '.': 1}, {'scarcity': 1, 'water': 1, 'drinking': 1, 'summer': 1}, {'road': 1, 'mg': 1, 'water': 1, 'leakage': 1, '.': 1}, {'water': 1, 'rural': 1, '.': 1, 'resource': 1, 'areas': 1, 'enough': 1, 'setup': 1, 'please': 1}, {')': 1, 'facing': 1, 'also': 2, 'water': 3, 'house': 3, 'panchayat': 1, 'needs': 1, 'rented': 1, 'consumer': 2, 'well': 1, 'water.we': 1, '.': 7, 'meter': 1, 'day-to-day': 1, 'residing': 1, '(': 1, '471/eda': 1, 'neighbouring': 1, 'no.818/eda': 1, 'water.our': 1, 'taluk': 1, 'number': 1, 'action': 1, 'take': 1, 'necessary': 1, 'edathua': 1, 'connection': 1, 'request': 1, 'staying': 1, 'great': 1, 'kindly': 1, 'kuttanad': 1, 'difficulties': 1, 'getting': 2}, {')': 1, 'facing': 1, 'water': 1, 'growing': 1, 'supply': 1, 'shall': 1, 'requesting': 1, 'needful': 1, '5': 1, 'eravipuram': 1, 'look': 1, 'matter': 1, 'many': 1, 'insufficient': 1, '.': 5, 'thankful': 1, 'kollam': 1, 'problem': 1, '(': 1, 'much': 1, 'days': 1, 'last': 1, 'locality': 2, 'totally': 1, 'severe': 1, 'kindly': 1, 'personally': 1, 'complaints': 1, 'residing': 1, 'instances': 1, 'given': 1, 'already': 1, 'attuparambil': 1}, {'facing': 1, 'last': 1, 'water': 2, 'system': 1, 'supply': 2, 'bad': 1, 'shortage': 2, 'condition': 1, 'worse': 1, 'kochi': 1, 'summer': 1, 'month': 1, 'aggravating': 1, '.': 2, 'kwa': 1, 'acute': 1, 'one': 1, 'getting': 1}, {'facing': 1, 'pulinkunnu': 1, 'water': 2, 'account': 1, 'drinking': 1, 'months.in': 1, 'kuttanadu': 1, 'society': 1, 'writing': 1, 'unavailability': 1, 'residents': 1, 'disturbed': 1, 'fact': 1, 'need': 1, 'member': 1, 'supply': 1, 'every': 2, 'last': 1, 'punnakunnam': 1, 'inform': 1, 'shortage': 1, '10': 1, 'human': 1, 'basic': 1, 'alappuzha': 1, '.': 1}, {'water': 2, 'installed': 1, 'leaking': 2, 'force': 1, 'new': 1, 'connection': 1, 'getting': 1, 'great': 1, '.': 1, 'house': 1, 'pipe': 1, 'wasted': 1, 'front': 1}, {'compound': 1, 'water': 1, 'accumulated': 1, 'drinking': 1, 'flowing': 1, 'always': 1, 'keep': 1, 'cabin': 1, 'note': 1, 'fully': 1, '.': 1, 'meter': 1, 'informing': 1, 'road': 1, 'wet': 1, 'please': 1}, {'get': 1, 'late': 1, 'water': 2, 'supply': 1, 'low': 1, 'time': 1, 'like': 1, 'week': 1, 'even': 1, 'midnight': 1, 'provide': 1, \"n't\": 1, 'two': 1, 'one': 1, 'may': 1, 'pressure': 1}, {'water': 1, '.': 1, 'meter': 1, 'issue': 1, 'good': 1, 'please': 1}, {'water': 2, 'portion': 1, 'waterworks': 1, 'sub': 1, 'rectify': 1, 'mentioned': 1, 'months': 1, 'consumer': 2, 'engineer': 1, 'reported': 1, 'corporation': 1, '.': 4, 'house': 2, 'road': 1, 'action': 1, 'number': 1, 'tiled': 1, 'supply': 1, 'four': 1, 'leakage': 2, 'taken': 1, 'thrissur': 2, 'assistant': 1, 'restore': 1, 'per': 1, 'noticed': 1, 'past': 1, 'division': 1, 'getting': 1}, {'resulted': 1, 'mudmix': 1, 'water': 2, 'january': 1, 'authority': 1, 'storage': 1, 'also': 1, 'tank': 1, 'every': 1, 'systems': 1, 'consuming': 1, '.': 3, 'bathroom': 1, 'mud': 1, 'destroy': 1, 'since': 1, 'taps': 1, 'supply': 1}, {'neighborhood': 1, 'tap': 1, 'people': 1, 'public': 1, 'couple': 1, 'remains': 1, 'lives': 1, 'idle': 1, 'past': 1, 'affected': 1, 'awkwardly': 1, 'weeks': 1}, {'recieved': 1, '792': 1, 'consumption': 1, 'details': 1, 'closed': 1, 'months': 1, 'previous': 1, 'reading': 1, 'bill': 2, 'rs': 1, 'state': 1, 'house': 1, 'information': 2, '3': 1, '.': 2}, {'water': 2, 'supply': 1, 'people': 1, 'upon': 1, 'pachayath': 1, 'look': 1, 'frustrating': 1, 'sources': 1, 'vazhathope': 1, 'two': 1, 'much': 1, 'need': 1, 'weeks': 1, '.': 2}, {'regular': 1, 'water': 1, 'supply': 1, 'september': 1, 'till': 1, '2017': 1, 'onwards': 1, 'date': 1, '.': 1}, {'loosing': 1, 'day': 1, 'water': 3, 'use.and': 1, 'time': 2, 'distribution': 1, 'public': 1, 'walk': 1, 'trapped': 1, 'way': 1, 'progress.this': 1, 'every': 1, 'pumping': 1, 'mosqitos': 1, 'office': 1, 'kochi': 1, 'line': 1, 'growing.several': 1, 'athority': 1, 'iform': 1}, {'houses': 1, 'domestic': 1, 'water': 1, 'supply': 1, 'days': 1, 'near': 1, 'kunnathuvathucal': 1, 'last': 1, 'bridge': 1, 'situated': 1, '3': 1, '.': 1}, {'name': 1, 'connection': 1, 'water': 1, 'wrongly': 1, 'records': 1, 'recorded': 1}, {'transaction': 1, 'water': 1, 'authority': 1, 'payment': 1, 'complete': 1, 'able': 1, 'website': 1}, {'bills': 1, 'facing': 1, 'paying': 1, 'mostly': 1, 'online': 1, 'services': 1, 'authority': 1, 'payment': 1, 'utility': 1, 'website': 1, 'water': 2, 'via': 1, 'bill': 2, '.': 2, 'problem': 1, 'pay': 1}, {'water': 1, 'drinking': 1, 'due': 1, 'pipeline': 1, '.': 1, 'wastage': 1, 'damaged': 1}, {'login': 2, 'credentials': 1, 'trying': 1, 'entered': 1, 'everytime': 1, 'payment': 1, 'error': 1, 'tried': 1, 'bill': 1, 'message': 1, '.': 1, 'try': 1, 'account': 1, 'make': 1, 'get': 1}, {'replace': 1, 'early': 1, '.': 1, 'meter': 1, 'possible': 1, 'check': 1, 'request': 1, 'repair': 1}, {'facing': 1, 'connection': 2, 'water': 2, 'set': 1, 'scarcity': 1, 'loction': 1, 'severe': 1, 'summer': 1, 'location': 1, '.': 2, 'since': 1, 'please': 1}, {'10/-': 1, 'e': 1, 'payment': 1, 'rupees': 1, 'site': 1, 'charge': 1, 'transaction': 1, 'bill': 1, 'web': 1, 'additional': 1, 'pay': 1, '.': 1}]\n",
      "\n",
      "\n",
      "KEYWORD : LIB : ENV\n",
      "\n",
      "\n",
      "                                 Subject_and_Complaint\n",
      "12    the number. of deaths from cardiovascular dis...\n",
      "13    people make river water polluted by dumping h...\n",
      "19    due to the sand mining there is more of effec...\n",
      "20    in our areas during night some strangers are ...\n",
      "33    fishes in the river pampa are dying massively...\n",
      "40    construction is going on the trivandrum in th...\n",
      "42    many industries dump wastes into rivers  lake...\n",
      "43    burning of plastics in public places cause a ...\n",
      "44    dumping of wastes in the public places causes...\n",
      "45    activities like waste disposal from residenti...\n",
      "46    overfishing  which causes a reduction in dive...\n",
      "47    artificial light at night is one of the most ...\n",
      "48    the electronic waste problem is huge. the ele...\n",
      "49    ocean acidification is caused when co₂ dissol...\n",
      "50    noise produced by vehicles  political parties...\n",
      "58    now a days many people are cutting down the t...\n",
      "62    polution by ksrtc bus is very heavy today .so...\n",
      "65    quarries are bad for the environment in sever...\n",
      "70                           save alappad stop mining.\n",
      "72    i would like to inform you that city resident...\n",
      "74    preserving biodiversity  saving forest can he...\n",
      "100   intolerable temperature change in summer seas...\n",
      "103                        intense heat during summer.\n",
      "110   one among the factors which shaped kerala as ...\n",
      "112   due to excessive use of chlorine and other di...\n",
      "113   we should have more projects on planting tree...\n",
      "114   need proper regulations to control the pollut...\n",
      "116   increased deforestation  uncontrolled constru...\n",
      "118   the pollution levels in the kochi city is ris...\n",
      "120   waste disposal in public premises make life h...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['deaths', 'disease', 'diseases'], ['river'], ['animals'], ['waste', 'wastes', 'road', 'night'], ['river'], ['save'], ['dump wastes', 'causing rampant health'], ['day', 'plastics', 'check'], ['wastes', 'households disposed'], ['like', 'contaminate bodies'], ['breeding', 'causes'], ['light', 'disrupted circadian'], ['electronic', 'electronics', 'soil'], ['acid', 'water'], ['produced', 'human'], ['harm', 'harmful', 'living', 'lives', 'holes', 'way', 'air', 'cut', 'cutting trees', 'ozone', 'oxygen'], ['fix'], ['quarries', 'quarrying', 'create', 'environment', 'interrupt continuity open space'], [''], ['necessary', 'inform'], ['biodiversity', 'avoiding'], ['skin', 'temperature'], [''], ['shaped kerala', 'generation time high impacting'], ['water', 'fall', 'use'], ['planting'], ['control'], ['increased', 'increase', 'switching renewable energy'], ['levels', 'steps'], ['people']]\n",
      "\n",
      "\n",
      "KEYWORD : LIB : Water\n",
      "\n",
      "\n",
      "[['erattakalangu'], ['water', 'work schools'], ['problem', 'areas', 'water', 'request', 'district'], [''], ['water supply', 'problem', 'people'], ['water', 'sealed pipe', 'hotel kovalam beach', 'thread', 'capacity', 'action', 'week'], ['water'], ['cooking'], ['authorised'], ['water'], ['water', 'necessary'], [''], [''], [''], ['rural'], ['water', 'house', 'eda', 'necessary', 'great'], ['locality', 'needful shall', 'residing attuparambil'], ['shortage water'], ['water', 'inform', 'member', 'need'], ['water', 'leaking'], ['informing', 'meter'], ['water'], [''], ['thrissur', 'leakage', 'water', 'consumer', 'house'], ['water', 'bathroom'], ['neighborhood', 'tap'], ['information'], ['water', 'sources'], ['water'], ['water', 'pumping', 'public'], ['bridge'], [''], ['authority'], ['authority', 'problem'], ['damaged'], ['login', 'entered'], ['early'], ['water'], ['web']]\n",
      "\n",
      "\n",
      "KEYWORD : LIB : PWD\n",
      "\n",
      "\n",
      "[['road', 'sreekaryam'], ['road', 'roads', 'damage', 'leads', 'miserable'], ['area road'], ['landslide', 'process'], ['locality'], ['traffic', 'works'], ['day', 'thannippara'], ['width'], ['potholes', 'pothole', 'know', 'drivers', 'feet diameter', 'attached photos illustrate', 'public works department'], ['roads', 'road', 'vehicle', 'vehicles', 'transportation', 'transport', 'problem', 'problems', 'india', 'create', 'railway', 'government', 'technology', 'businessmen', 'high traffic', 'driven carts', 'attention', 'cars', 'requires immediate', 'safety'], ['unwary'], ['peak traffic', 'handle'], ['roads', 'right'], ['fix', 'dont'], ['road', 'cause health'], ['road', 'water flow', 'directed compound wall', 'requested', 'direction', 'drainage', 'mtr'], ['roads', 'repaired', 'repairing', 'repair', 'increased', 'news', 'fuel', 'know', 'broken', 'increases height', 'sir', 'hope factors'], ['roads', 'petty shops', 'country', 'pity consideration appeal', 'sir', 'estate', 'people', 'tea'], ['road'], ['waiting shed', 'constructed', 'construction', 'othukkungal town'], ['gutter', 'road'], ['road', 'resulting', 'kids', 'result hit', 'chances hitting', 'chance', 'digged'], ['road going', 'technopark', 'urgent action', 'kallingal', 'situation'], ['driving'], ['people'], [''], ['condition', 'needful', 'area'], ['public', 'toll', 'bridge', 'road', 'money', 'stopped'], ['include', 'road'], ['road', 'complaint'], ['road', 'long'], ['quality'], ['traffic'], [''], ['times'], ['roads'], [''], ['road', 'roads', 'drivers', 'makes', 'ordinary'], ['roads'], ['road', 'accidents', 'creating', 'shacks created', 'shack', 'unauthorized', 'establishments', 'areas kolenchery', 'tent'], ['road'], ['maniykumppara road']]\n",
      "\n",
      "\n",
      "KEYWORD : LIB : KSRTC\n",
      "\n",
      "\n",
      "[['college', 'bus service', 'buses', 'student', 'students'], ['drivers', 'rural'], ['running'], ['school', 'hours'], ['behave'], ['bus', 'rescheduling'], ['services', 'service', 'lack'], ['commute', 'buses', 'use public transportation'], [''], ['passengers', 'time', 'bothered', 'passenger got'], ['seat', 'seats buses', 'problem', 'females', 'transport city', 'word'], ['overspeeding', 'present'], ['services', 'helpful students'], ['trivandrum', 'bus', 'mid'], ['stopping', 'stop', 'drivers', 'busses', 'ksrtc', 'kindly'], ['car', 'driver', 'bus', 'muvattupuza'], ['traveller', 'travellers', 'travel', 'kumarakam', 'necessary']]\n",
      "\n",
      "\n",
      "KEYWORD : LIB : KSEB\n",
      "\n",
      "\n",
      "                                 Subject_and_Complaint\n",
      "2     as everyone knows that electricity is the maj...\n",
      "10    i have experiences this ''load shedding'' pro...\n",
      "11    the employees are highly corrupt. i had to gi...\n",
      "15    our locality is facing the problem of unavail...\n",
      "22    electricity meter is not working for last 3 d...\n",
      "23    electricity meter is showing high values even...\n",
      "24    since transformer is very near to my house  i...\n",
      "25    recently lineman came to our house to repair ...\n",
      "26    i applied for new electricity connection for ...\n",
      "27    during evening time we are facing low voltage...\n",
      "28    please avoid night time load shedding as publ...\n",
      "29    electric connection got failed due to heavy r...\n",
      "30    i have three phase connections at home. only ...\n",
      "31    transformer in trivandrum city exploded with ...\n",
      "53    power failure is occurring frequently in mada...\n",
      "54    in my locality  for the past few months there...\n",
      "57    due to electricity overhead wires  we are in ...\n",
      "78                  lot of power failure in day time .\n",
      "94      frequent power cut during morning (peak time).\n",
      "106                                        power loss.\n",
      "108               power failures without prior notice.\n",
      "119   there has been no power in ours as well as ad...\n",
      "[['electricity', 'working', 'power', 'sector', 'farming'], ['times', 'time'], ['connected', 'bribe connection', 'highly', 'burnt'], ['unsocial', 'facing'], ['working'], ['meter'], ['near', 'damaged'], ['lineman'], ['connection'], ['time', 'electric'], ['examinations'], ['heavy'], ['phase', 'phases'], ['large'], ['necessary'], ['months', 'look problem'], ['wires', 'crawl'], ['day'], ['power'], [''], [''], ['fix']]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords \n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize \n",
    "from collections import Counter\n",
    "from gensim.summarization import keywords\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "\n",
    "global dataset\n",
    "\n",
    "\n",
    "class Main:\n",
    "         \n",
    "      \n",
    "    \n",
    "    \n",
    "    def __init__(self,dataset):\n",
    "        #/home/gayathri/project/MakeComplaint/data.csv\n",
    "        self.dataset=dataset\n",
    "        print(self.dataset)\n",
    "        self.dataset= pd.read_csv(self.dataset)\n",
    "        pass\n",
    "    \n",
    "    def punctuate(self):\n",
    "        punctuations = '''!()-[]{};:'\"\\,<>/?@.#$%^&*_~'''\n",
    "        text=self.dataset['Subject']\n",
    "\n",
    "        # To take input from the user\n",
    "        # my_str = input(\"Enter a string: \")\n",
    "\n",
    "        # remove punctuation from the string\n",
    "        no_punct = \"\"\n",
    "        no_punctuate =[]\n",
    "        for char in  self.dataset['Subject']:\n",
    "\n",
    "            if char not in punctuations:\n",
    "                no_punct = no_punct + char\n",
    "                no_punctuate.append(no_punct)\n",
    "\n",
    "        # display the unpunctuated string\n",
    "        print(no_punctuate)\n",
    "        return(no_punctuate)\n",
    "\n",
    "        \n",
    "    def data_clean(self):\n",
    "        \n",
    "        \n",
    "        \n",
    "        # unpunctuate and lower case\n",
    "        self.dataset['Subject'] = self.dataset['Subject'].str.replace('[^\\w\\s]','').str.lower()\n",
    "\n",
    "\n",
    "            # unpunctuate and lower case\n",
    "        self.dataset['Complaint'] =  self.dataset['Complaint'].str.replace(',',' ').str.lower() \n",
    "        self.dataset['Subject'] = self.dataset['Subject'].str.replace('[^\\w\\s]','').str.lower()\n",
    "        self.dataset['Subject'] = self.dataset['Subject'].str.replace('[^\\P{P}-]','').str.lower()\n",
    "        #print( self.dataset.head())\n",
    "\n",
    "\n",
    "\n",
    "        #rRemoving new lines in the subject field\n",
    "        self.dataset['Subject'] =  self.dataset['Subject'].str.rstrip('\\n')\n",
    "\n",
    "        #removing Numeric \n",
    "        self.dataset['Complaint'] =  self.dataset['Complaint']\n",
    "        print( self.dataset.head())\n",
    "        \n",
    "    def dataframing(self,dataset):\n",
    "        # creating dataframe for each departments\n",
    "        water = dataset.loc[dataset['Departments'] == 'Water Authority']\n",
    "        pwd = dataset.loc[dataset['Departments'] == 'PWD']\n",
    "        ksrtc = dataset.loc[dataset['Departments'] == 'KSRTC']\n",
    "        kseb = dataset.loc[dataset['Departments'] == 'KSEB']\n",
    "        env = dataset.loc[dataset['Departments'] == 'Environment and climate change']\n",
    "        print(env.shape)    #(30, 4)\n",
    "        #print(water.shape)  #(39, 4)\n",
    "        #print(pwd.shape)    #(42, 4)\n",
    "        #print(ksrtc.shape)  #(17, 4)\n",
    "        #print(kseb.shape)   #(22, 4)\n",
    "        #dataset.head()\n",
    "        #print(pwd)\n",
    "\n",
    "  \n",
    "\n",
    "        #Filtering out Subjects and complaints from the dataframe\n",
    "        df_water = water[['Subject','Complaint']]\n",
    "        df_pwd   = pwd[['Subject','Complaint']]\n",
    "        df_ksrtc = ksrtc[['Subject','Complaint']]\n",
    "        df_kseb  = kseb[['Subject','Complaint']]\n",
    "        df_env   = env[['Subject','Complaint']]\n",
    "\n",
    "        dfwater  = df_water[['Subject','Complaint']]\n",
    "        dfpwd    = df_pwd[['Subject','Complaint']]\n",
    "        dfksrtc  = df_ksrtc[['Subject','Complaint']]\n",
    "        dfkseb   = df_kseb[['Subject','Complaint']]\n",
    "        dfenv    = df_env[['Subject','Complaint']]\n",
    "\n",
    "        \n",
    "        dfwater['Subject_and_Complaint'] = df_water['Subject'] + \" \"+ df_water['Complaint']\n",
    "        dfwater=dfwater[['Subject_and_Complaint']]\n",
    "        \n",
    "        dfpwd['Subject_and_Complaint'] = df_pwd['Subject'] + \" \"+ df_pwd['Complaint']\n",
    "        dfpwd=dfpwd[['Subject_and_Complaint']]\n",
    "        \n",
    "        dfksrtc['Subject_and_Complaint'] = df_ksrtc['Subject'] + \" \"+ df_ksrtc['Complaint']\n",
    "        dfksrtc=dfksrtc[['Subject_and_Complaint']]\n",
    "        \n",
    "        dfkseb['Subject_and_Complaint'] = df_kseb['Subject'] + \" \"+ df_kseb['Complaint']\n",
    "        dfkseb=dfkseb[['Subject_and_Complaint']]\n",
    "        \n",
    "        dfenv ['Subject_and_Complaint'] = df_env['Subject'] + \" \"+ df_env['Complaint']\n",
    "        dfenv =dfenv [['Subject_and_Complaint']]\n",
    "        print(dfenv )\n",
    "        return (dfwater,dfpwd,dfksrtc,dfkseb,dfenv)\n",
    "        \n",
    "        \n",
    "    def tokenisation(self,dfwater,dfpwd,dfksrtc,dfkseb,dfenv):\n",
    "        water_token = []\n",
    "        water_list=[]\n",
    "            #Tokenising water data\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        for i, row in dfwater.iterrows():\n",
    "                #print(i,row['Subject'], row['Complaint'])\n",
    "                \n",
    "            water_token = word_tokenize(row['Subject_and_Complaint'])\n",
    "            result = [i for i in water_token if not i in stop_words]\n",
    "            water_list.append(result)\n",
    "        #print(water_list)\n",
    "    #water_token.append(tokenizer.tokenize(row['Subject_and_Complaint']))\n",
    "        pwd_token = []\n",
    "\n",
    "        pwd_list = []\n",
    "        #Tokenising pwd data  \n",
    "        \n",
    "        for i, row in dfpwd.iterrows():\n",
    "        #print(i,row['Subject'], row['Complaint'])\n",
    "            pwd_token = word_tokenize(row['Subject_and_Complaint'])\n",
    "\n",
    "            result1 = [i for i in pwd_token if not i in stop_words]\n",
    "            pwd_list.append(result1)\n",
    "            #print(pwd_list)\n",
    "            #print( pwd_token)\n",
    "\n",
    "\n",
    "        ksrtc_token =[]\n",
    "        ksrtc_list =[]\n",
    "        #Tokenising ksrtc data    \n",
    "        for i, row in dfksrtc.iterrows():\n",
    "            #print(i,row['Subject'], row['Complaint'])\n",
    "            ksrtc_token = word_tokenize(row['Subject_and_Complaint'])\n",
    "            result = [i for i in ksrtc_token if not i in stop_words]\n",
    "            ksrtc_list.append(result)\n",
    "            #print(ksrtc_list)\n",
    "            #print( ksrtc_token)\n",
    "            \n",
    "            \n",
    "            \n",
    "        kseb_token = []\n",
    "        kseb_list= []\n",
    "        #Tokenising kseb data    \n",
    "        for i, row in dfkseb.iterrows():\n",
    "        #print(i,row['Subject'], row['Complaint'])\n",
    "            kseb_token = word_tokenize(row['Subject_and_Complaint'])\n",
    "            result = [i for i in kseb_token if not i in stop_words]\n",
    "            kseb_list.append(result)\n",
    "        #print(kseb_list)\n",
    "        #print(kseb_token)\n",
    "        \n",
    "        \n",
    "        env_token = []\n",
    "        env_list = []\n",
    "        #Tokenising env data  \n",
    "        \n",
    "        for i, row in dfenv.iterrows():\n",
    "            #print(i,row['Subject'], row['Complaint'])\n",
    "            env_token = word_tokenize(row['Subject_and_Complaint'])\n",
    "            result = [i for i in env_token if not i in stop_words]\n",
    "            env_list.append(result)\n",
    "        print(env_list)\n",
    "            #print(env_token)\n",
    "        return(water_list,pwd_list,ksrtc_list,kseb_list,env_list)\n",
    "\n",
    "\n",
    "   \n",
    "    def word_frequency(self,water_list,pwd_list,ksrtc_list,kseb_list,env_list):\n",
    "        \n",
    "        #word frequencies  Environment department\n",
    "\n",
    "        wordfreq = [1]\n",
    "\n",
    "        count = 0\n",
    "        for word  in env_list:\n",
    "            count+=1\n",
    "            wordfreq.append([])\n",
    "            for i in word:\n",
    "                wordfreq[count].append(word.count(i))\n",
    "        wordfreq.pop(0)\n",
    "        env_freq=list(map(dict, map(zip, env_list, wordfreq)))\n",
    "        #print(env_freq)\n",
    "        \n",
    "        \n",
    "        \n",
    "        #word frequencies  KSEB department\n",
    "        wordfreq = [1]\n",
    "        count = 0\n",
    "        for word  in kseb_list:\n",
    "            count+=1\n",
    "            wordfreq.append([])\n",
    "            for i in word:\n",
    "                wordfreq[count].append(word.count(i))\n",
    "        wordfreq.pop(0)\n",
    "\n",
    "        kseb_freq=list(map(dict, map(zip, kseb_list, wordfreq)))\n",
    "        #print(kseb_freq)\n",
    "\n",
    "        \n",
    "        #word frequencies  KSRTC department\n",
    "\n",
    "        wordfreq = [1]\n",
    "        count = 0\n",
    "        for word  in ksrtc_list:\n",
    "            count+=1\n",
    "            wordfreq.append([])\n",
    "            for i in word:\n",
    "                wordfreq[count].append(word.count(i))\n",
    "        wordfreq.pop(0)\n",
    "\n",
    "        ksrtc_freq=list(map(dict, map(zip, ksrtc_list, wordfreq)))\n",
    "        #print(\"\\n========= Word Frequency of KSEB =========== \\n\\n\")\n",
    "        #print(ksrtc_freq)\n",
    "        \n",
    "        #word frequencies  pwd department\n",
    "\n",
    "        wordfreq = [1]\n",
    "        count = 0\n",
    "        for word  in pwd_list:\n",
    "            count+=1\n",
    "            wordfreq.append([])\n",
    "            for i in word:\n",
    "                wordfreq[count].append(word.count(i))\n",
    "        wordfreq.pop(0)\n",
    "\n",
    "        pwd_freq=list(map(dict, map(zip, pwd_list, wordfreq)))\n",
    "        #print(\"\\n========= Word Frequency of PWD =========== \\n\\n\")\n",
    "        #print(pwd_freq)\n",
    "        \n",
    "        \n",
    "        #word frequencies  water department\n",
    "\n",
    "        wordfreq = [1]\n",
    "        count = 0\n",
    "        for word  in water_list:\n",
    "            count+=1\n",
    "            wordfreq.append([])\n",
    "            for i in word:\n",
    "                wordfreq[count].append(word.count(i))\n",
    "        wordfreq.pop(0)\n",
    "\n",
    "        water_freq=list(map(dict, map(zip, water_list, wordfreq)))\n",
    "        print(\"\\n========= Word Frequency of WATER =========== \\n\\n\")\n",
    "\n",
    "        print(water_freq)\n",
    "        return(water_freq,pwd_freq,ksrtc_freq,kseb_freq,env_freq)\n",
    "\n",
    "\n",
    "    def most_repeated_keywords(self,water_freq,pwd_freq,ksrtc_freq,kseb_freq,env_freq ):\n",
    "    \n",
    "        \n",
    "        water_lis =[]\n",
    "        water_freq_list = { }\n",
    "        for lists in water_freq:\n",
    "            water_freq_list.update({})\n",
    "            lists=dict(lists)\n",
    "            items = [(v, k) for k, v in lists.items()]\n",
    "            items.sort()\n",
    "            items.reverse()\n",
    "            items = [k for v, k in items]\n",
    "            #print(items)\n",
    "            water_dict=(items[:4])\n",
    "            water_lis.append(water_dict)\n",
    "            \n",
    "        print(\"\\n\\nKEYWORDS  WATER\\n\\n\")\n",
    "        print(water_lis)\n",
    "        \n",
    "        pwd_lis =[]\n",
    "        pwd_freq_list = { }\n",
    "        for lists in pwd_freq:\n",
    "            pwd_freq_list.update({})\n",
    "            lists=dict(lists)\n",
    "            items = [(v, k) for k, v in lists.items()]\n",
    "            items.sort()\n",
    "            items.reverse()\n",
    "            items = [k for v, k in items]\n",
    "            #print(items)\n",
    "            pwd_dict=(items[:4])\n",
    "            pwd_lis.append(pwd_dict)\n",
    "            \n",
    "        print(\"\\n\\nKEYWORDS  PWD\\n\\n\")\n",
    "        print(pwd_lis)\n",
    "        \n",
    "        # Finding the most repeated words kseb\n",
    "        kseb_lis =[]\n",
    "        kseb_freq_list = { }\n",
    "        for lists in kseb_freq:\n",
    "            kseb_freq_list.update({})\n",
    "            lists=dict(lists)\n",
    "            items = [(v, k) for k, v in lists.items()]\n",
    "            items.sort()\n",
    "            items.reverse()\n",
    "            items = [k for v, k in items]\n",
    "            #print(items)\n",
    "            kseb_dict=(items[0:4])\n",
    "            kseb_lis.append(kseb_dict)\n",
    "        print(\"\\n\\nKEYWORDS  KSEB\\n\\n\")\n",
    "        print(kseb_lis)\n",
    "        \n",
    "        # Finding the most repeated words ksrtc\n",
    "        ksrtc_lis =[]\n",
    "        ksrtc_freq_list = { }\n",
    "        for lists in ksrtc_freq:\n",
    "            kseb_freq_list.update({})\n",
    "            lists=dict(lists)\n",
    "            items = [(v, k) for k, v in lists.items()]\n",
    "            items.sort()\n",
    "            items.reverse()\n",
    "            items = [k for v, k in items]\n",
    "            #print(items)\n",
    "            ksrtc_dict=(items[:4])\n",
    "            ksrtc_lis.append(ksrtc_dict)\n",
    "        print(\"\\n\\nKEYWORDS  KSRTC\\n\\n\")\n",
    "        print(ksrtc_lis)\n",
    "        \n",
    "        # Finding the most repeated words env\n",
    "        env_lis =[]\n",
    "        env_freq_list = { }\n",
    "        for lists in env_freq:\n",
    "            env_freq_list.update({})\n",
    "            lists=dict(lists)\n",
    "            items = [(v, k) for k, v in lists.items()]\n",
    "            #print(type(items))\n",
    "            items.sort()\n",
    "            items.reverse()\n",
    "            items = [k for v, k in items]\n",
    "            #print(items)\n",
    "            env_dict=(items[0:4])\n",
    "            #print(env_dict)\n",
    "            env_lis.append(env_dict) \n",
    "        print(\"\\n\\nKEYWORDS  ENV\\n\\n\")\n",
    "        print(env_lis)\n",
    "                \n",
    "        \n",
    "    def most_repeated_keyword_lib(self,dfenv,dfwater,dfpwd,dfksrtc,dfkseb):\n",
    "        \n",
    "        #env keyword\n",
    "        \n",
    "        env_keyword = []\n",
    "        env_summary = []\n",
    "        \n",
    "        stopwords = list(STOP_WORDS)\n",
    "        \n",
    "        print(\"\\n\\nKEYWORD : LIB : ENV\\n\\n\")\n",
    "        print(dfenv)\n",
    "        \n",
    "        for i, row in dfenv.iterrows():\n",
    "            env_token = word_tokenize(row['Subject_and_Complaint'])\n",
    "\n",
    "            result = [i for i in env_token if not i in stopwords]\n",
    "            #print(result)\n",
    "            env_keyword.append(result)\n",
    "            \n",
    "            str1 = ' '.join(result)\n",
    "            #map(bytes,env_keyword)\n",
    "            #print(str1)\n",
    "            #break\n",
    "        #print(type(env_token))\n",
    "            env_summary.append(keywords(str1).split('\\n'))\n",
    "               \n",
    "        #print(type(env_token[0][0]))\n",
    "        print(env_summary)\n",
    "        \n",
    "        # water Keyword\n",
    "        \n",
    "        water_keyword = []\n",
    "        water_summary = []\n",
    "        \n",
    "        stopwords = list(STOP_WORDS)\n",
    "        \n",
    "        print(\"\\n\\nKEYWORD : LIB : Water\\n\\n\")\n",
    "        \n",
    "        \n",
    "        for i, row in dfwater.iterrows():\n",
    "            water_token = word_tokenize(row['Subject_and_Complaint'])\n",
    "\n",
    "            result = [i for i in water_token if not i in stopwords]\n",
    "            #print(result)\n",
    "            water_keyword.append(result)\n",
    "            \n",
    "            str1 = ' '.join(result)\n",
    "            #map(bytes,env_keyword)\n",
    "            #print(str1)\n",
    "            #break\n",
    "        #print(type(env_token))\n",
    "            water_summary.append(keywords(str1).split('\\n'))\n",
    "               \n",
    "        #print(type(env_token[0][0]))\n",
    "        print(water_summary)\n",
    "        \n",
    "        \n",
    "        #pwd keyword\n",
    "        \n",
    "        pwd_keyword = []\n",
    "        pwd_summary = []\n",
    "        \n",
    "        stopwords = list(STOP_WORDS)\n",
    "        \n",
    "        print(\"\\n\\nKEYWORD : LIB : PWD\\n\\n\")\n",
    "        #print(dfenv)\n",
    "        \n",
    "        for i, row in dfpwd.iterrows():\n",
    "            pwd_token = word_tokenize(row['Subject_and_Complaint'])\n",
    "\n",
    "            result = [i for i in pwd_token if not i in stopwords]\n",
    "            #print(result)\n",
    "            pwd_keyword.append(result)\n",
    "            \n",
    "            str1 = ' '.join(result)\n",
    "            #map(bytes,env_keyword)\n",
    "            #print(str1)\n",
    "            #break\n",
    "        #print(type(env_token))\n",
    "            pwd_summary.append(keywords(str1).split('\\n'))\n",
    "               \n",
    "        #print(type(env_token[0][0]))\n",
    "        print(pwd_summary)\n",
    "        \n",
    "        #env keyword\n",
    "        \n",
    "        ksrtc_keyword = []\n",
    "        ksrtc_summary = []\n",
    "        \n",
    "        stopwords = list(STOP_WORDS)\n",
    "        \n",
    "        print(\"\\n\\nKEYWORD : LIB : KSRTC\\n\\n\")\n",
    "        #print(dfenv)\n",
    "        \n",
    "        for i, row in dfksrtc.iterrows():\n",
    "            ksrtc_token = word_tokenize(row['Subject_and_Complaint'])\n",
    "\n",
    "            result = [i for i in ksrtc_token if not i in stopwords]\n",
    "            #print(result)\n",
    "            ksrtc_keyword.append(result)\n",
    "            \n",
    "            str1 = ' '.join(result)\n",
    "            #map(bytes,env_keyword)\n",
    "            #print(str1)\n",
    "            #break\n",
    "        #print(type(env_token))\n",
    "            ksrtc_summary.append(keywords(str1).split('\\n'))\n",
    "               \n",
    "        #print(type(env_token[0][0]))\n",
    "        print(ksrtc_summary)\n",
    "        \n",
    "        #env keyword\n",
    "        \n",
    "        kseb_keyword = []\n",
    "        kseb_summary = []\n",
    "        \n",
    "        stopwords = list(STOP_WORDS)\n",
    "        \n",
    "        print(\"\\n\\nKEYWORD : LIB : KSEB\\n\\n\")\n",
    "        print(dfkseb)\n",
    "        \n",
    "        for i, row in dfkseb.iterrows():\n",
    "            kseb_token = word_tokenize(row['Subject_and_Complaint'])\n",
    "\n",
    "            result = [i for i in kseb_token if not i in stopwords]\n",
    "            #print(result)\n",
    "            kseb_keyword.append(result)\n",
    "            \n",
    "            str1 = ' '.join(result)\n",
    "            #map(bytes,env_keyword)\n",
    "            #print(str1)\n",
    "            #break\n",
    "        #print(type(env_token))\n",
    "            kseb_summary.append(keywords(str1).split('\\n'))\n",
    "               \n",
    "        #print(type(env_token[0][0]))\n",
    "        print(kseb_summary)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def jump(self,status):\n",
    "        if status == 1:\n",
    "            self.most_repeated_keyword_lib(dfenv,dfwater,dfpwd,dfksrtc,dfkseb)\n",
    "        else:\n",
    "            \n",
    "            self.most_repeated_keywords(water_freq,pwd_freq,ksrtc_freq,kseb_freq,env_freq )\n",
    "            \n",
    "    \n",
    "            \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "file =   '/home/gayathri/project/MakeComplaint/data.csv'   \n",
    "\n",
    "x= Main(file)\n",
    "#x.punctuate()\n",
    "x.data_clean()\n",
    "dfwater,dfpwd,dfksrtc,dfkseb,dfenv=x.dataframing(x.dataset)\n",
    "\n",
    "water_list,pwd_list,ksrtc_list,kseb_list,env_list = x.tokenisation(dfwater,dfpwd,dfksrtc,dfkseb,dfenv)\n",
    "\n",
    "water_freq,pwd_freq,ksrtc_freq,kseb_freq,env_freq = x.word_frequency(water_list,pwd_list,ksrtc_list,kseb_list,env_list)\n",
    "#x.most_repeated_keywords(water_freq,pwd_freq,ksrtc_freq,kseb_freq,env_freq)\n",
    "#x.most_repeated_keyword_lib(dfenv)  \n",
    "x.jump(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
