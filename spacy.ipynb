{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, [], 'increasing deaths from air pollution \\nair pollution.  the number of deaths from cardiovascular disease that can be attributed to air pollution is much higher than expectedair pollution caused twice as many deaths from cvd as from respiratory diseases', [], 'waste disposal to water bodies.  people make river water polluted by dumping household wastage industrial wastage into it', [], 'illegal sand mines.  due to the sand mining there is more of effect on the ecosystem which will have a severe impact on plants animals and rivers', [], 'waste disposal on roads.  in our areas during night some strangers are dumping hotel waste domestic waste which is very foul smelling on the sides of the road those wastes remain in th road uncleaned there shouldbe a camerasurvelliance facility to catchthe people throwing wastes', [], 'fishes dying massively.  fishes in the river pampa are dying massively due to the deposits of oil by the factories nearby', [], 'construction of paddy fields.  construction is going on the trivandrum in the paddy fields please take necessary steps to save farming', [], 'water pollution is a major issue.  many industries dump wastes into rivers lakes ponds and streams in an attempt to hide wastes from epa inspectors these water sources feed major crops and food becomes contaminated with a variety of chemicals and bacteria causing rampant health problems', [], 'burning of plastics issue.  burning of plastics in public places cause a major health concern as the rate of lung cancer patients are increasing day by day it is high time to check such activities', [], 'waste collecting mechanism.  dumping of wastes in the public places causes major health issues and has been inviting the eradicated disease once more there should be a mechanism to collect the wastes from all the households and it should be disposed properly', [], 'save water bodies.  activities like waste disposal from residential commercial and industrial areas oil spills and runoff from agriculture all contaminate bodies of water', [], 'overfishing in rivers.  overfishing which causes a reduction in diversity of marine life fishermen are not considering their breeding time', [], 'light pollution.  artificial light at night is one of the most obvious physical changes that humans have made to the biosphere artificial light also affects dispersal orientation migration and hormone levels resulting in disrupted circadian rhythms', [], 'we make a lot of e-waste.  the electronic waste problem is huge the electronics end up in landfills toxics like lead mercury and cadmium leach into the soil and water', [], 'ocean acidification.  ocean acidification is caused when co dissolves into the ocean bonding with sea water creating carbonic acid the acid reduces the ph levels in the water', [], 'city dwellers are prone to noise pollution.  noise produced by vehicles political parties religious centers causes great harm to human ears', [], 'cutting down of trees.  now a days many people are cutting down the trees for their own purposes for example  many oragansations and some builders cut down the trees and build their projects this will cause a real harm to humans they are also utilizing those fertile paddy fields for construction these will affect the environment and the oxygen content in the air become low when the oxygen content level decreases in the air survival of living beings will move in a harder way it is also a major factor in ozone depletion when ozone holes are formed the harmful radiations will enter into the earth through these holes and causes real harm to the living beings if this continues in this way it is a threatening part for our lives so please take this as a serious issue and make necessary useful remedies ', [], 'pollution by ksrtc bus.  polution by ksrtc bus is very heavy today so government has to fix it as soon as possible', [], 'horrible effects of quarrying  quarries are bad for the environment in several ways they abruptly interrupt the continuity of open space cause soil erosionair and dust pollution deterioration in water qualitywhen they are in residential areathey create noise hazards so i request the higher authority to investigate and punish the officials who have a hand in quarrying and illegal mining at the same time government should create awareness about the potentially negative impact of quarrying', [], 'mining.  save alappad stop mining', [], 'regarding waste dumping in cities.  i would like to inform you that city residents are facing difficulties to dump domestic wastes sohumbly request you to take necessary actions on this', [], 'environment and climate change\\n is one of the big challenge we \\nare facing right now.  preserving biodiversity saving forest can help to overcome these issuesalso by recycling avoiding usage of plastic efficient use of fuel', [], 'intolerable temperature rise in summer\\n season.  intolerable temperature change in summer season cause dried up of rivers skin disease', [], 'environment.  intense heat during summer', [], 'take immediate and necessary \\nsteps to regulate global climate \\nchange.  one among the factors which shaped kerala as gods own country  was its euphoric climate which never touched the extremes thanks to the moderating influence of sea and the mighty western ghats but the scenario has changed a lot now kerala is witnessing an unprecedental surge in temperature kumbhachoodu as termed by old generation is in its alltime highimpacting the livelihood of many', [], 'excessive use of chlorine.  due to excessive use of chlorine and other disinfectants the water is causing long term health problems like hair fall rashes etc the amount of disinfectants to be used in the water to be supplied need to reconsidered', [], 'increase in heat and temperature.  we should have more projects on planting trees and dont cut down big grownup trees', [], 'pollution everywhere.  need proper regulations to control the pollution', [], 'rise in temperature.  increased deforestation uncontrolled constructiondevelopment activities and vehicle emissions heavily contribute to increase in temperature government should build new policies and also consider switching to renewable energy sources', [], 'the bad air and water quality in kochi.  the pollution levels in the kochi city is rising rapidly the government should initiate steps to curb the same', [], 'waste disposal   waste disposal in public premises make life hard for people living around there']\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords \n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize \n",
    "from collections import Counter\n",
    "import spacy\n",
    "import pandas as pd\n",
    "\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "#Data cleaning\n",
    "\n",
    "# 1. unpunctuate /home/gayathri/Complaint/MakeComplaint/\n",
    "# 2. to lower\n",
    "# 3. Remove numerals\n",
    "# 4. Remove Newline for subject\n",
    "\n",
    "\n",
    "# Data loading\n",
    "dataset= pd.read_csv('/home/gayathri/project/MakeComplaint/data.csv')\n",
    "\n",
    "\n",
    "# unpunctuate and lower case\n",
    "dataset['Subject'] = dataset['Subject'].str.lower()\n",
    "\n",
    "\n",
    "# unpunctuate and lower case\n",
    "dataset['Complaint'] = dataset['Complaint'].str.lower() \n",
    "\n",
    "\n",
    "#rRemoving new lines in the subject field\n",
    "dataset['Subject'] = dataset['Subject'].str.rstrip('\\n')\n",
    "\n",
    "#rRemoving new lines in the subject field\n",
    "#dataset['Subject'] = dataset['Complaint'].str.rstrip('\\n')\n",
    "\n",
    "#removing Numeric \n",
    "dataset['Complaint'] = dataset['Complaint'].str.replace('[^a-zA-Z ]','').str.lower()\n",
    "\n",
    "\n",
    "\n",
    "# creating dataframe for each departments\n",
    "water = dataset.loc[dataset['Departments'] == 'Water Authority']\n",
    "pwd = dataset.loc[dataset['Departments'] == 'PWD']\n",
    "ksrtc = dataset.loc[dataset['Departments'] == 'KSRTC']\n",
    "kseb = dataset.loc[dataset['Departments'] == 'KSEB']\n",
    "env = dataset.loc[dataset['Departments'] == 'Environment and climate change']\n",
    "\n",
    "#print(env.shape)    #(30, 4)\n",
    "#print(water.shape)  #(39, 4)\n",
    "#print(pwd.shape)    #(42, 4)\n",
    "#print(ksrtc.shape)  #(17, 4)\n",
    "#print(kseb.shape)   #(22, 4)\n",
    "#dataset.head()\n",
    "#print(pwd)\n",
    "\n",
    "#Filtering out Subjects and complaints from the dataframe\n",
    "df_water = water[['Subject','Complaint']]\n",
    "df_pwd   = pwd[['Subject','Complaint']]\n",
    "df_ksrtc = ksrtc[['Subject','Complaint']]\n",
    "df_kseb  = kseb[['Subject','Complaint']]\n",
    "df_env   = env[['Subject','Complaint']]\n",
    "\n",
    "dfwater  = df_water[['Subject','Complaint']]\n",
    "dfpwd    = df_pwd[['Subject','Complaint']]\n",
    "dfksrtc  = df_ksrtc[['Subject','Complaint']]\n",
    "dfkseb   = df_kseb[['Subject','Complaint']]\n",
    "dfenv    = df_env[['Subject','Complaint']]\n",
    "\n",
    "\n",
    "#Dataframe with complaint and subject as one column = Water\n",
    "dfwater['Subject_and_Complaint'] = df_water['Subject'] + \" \"+ df_water['Complaint']\n",
    "dfwater=dfwater[['Subject_and_Complaint']]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Dataframe with complaint and subject as one column = PWD\n",
    "dfpwd['Subject_and_Complaint'] = df_pwd['Subject'] + \"  \"+ df_pwd['Complaint']\n",
    "dfpwd=dfpwd[['Subject_and_Complaint']]\n",
    "#print(dfpwd)\n",
    "\n",
    "#Dataframe with complaint and subject as one column = ksrtc\n",
    "dfksrtc['Subject_and_Complaint'] = df_ksrtc['Subject'] + \"  \"+ df_ksrtc['Complaint']\n",
    "dfksrtc=dfksrtc[['Subject_and_Complaint']]\n",
    "#print(dfksrtc)\n",
    "\n",
    "#Dataframe with complaint and subject as one column = kseb\n",
    "dfkseb['Subject_and_Complaint'] = df_kseb['Subject'] + \"  \"+ df_kseb['Complaint']\n",
    "dfkseb=dfkseb[['Subject_and_Complaint']]\n",
    "#print(dfkseb)\n",
    "\n",
    "\n",
    "#Dataframe with complaint and subject as one column = env\n",
    "dfenv ['Subject_and_Complaint'] = df_env['Subject'] + \"  \"+ df_env['Complaint']\n",
    "dfenv =dfenv [['Subject_and_Complaint']]\n",
    "#print(dfenv )\n",
    "\n",
    "#dfenv['Subject_and_Complaint'] =dfenv['Subject_and_Complaint'] .apply(nlp) \n",
    "dfenv['Subject_and_Complaint'] = dfenv['Subject_and_Complaint'].apply(lambda x: nlp(x))\n",
    "count= 0\n",
    "sent = [1]\n",
    "for i, row in dfenv.iterrows():\n",
    "    sent.append([])\n",
    "    for j in row:\n",
    "        sent.append(j.text)\n",
    "print(sent)\n",
    "        \n",
    "\n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this\n",
      "is\n",
      "a\n",
      "book\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "doc = nlp(\"this is a book\")\n",
    "for i in doc:\n",
    "    print(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These are apples.\n",
      "These are oranges.\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "doc = nlp(\"These are apples.These are oranges.\")\n",
    "for sent in doc.sents:\n",
    "    print(sent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The film Pulp Fiction was  released  in   year 1994\n"
     ]
    }
   ],
   "source": [
    "from gensim.summarization import keywords\n",
    "from gensim.summarization import summarize\n",
    "import re\n",
    "text = \"The film, '@Pulp Fiction' was ? released _ in % $ year 1994.\"  \n",
    "result = re.sub(r\"[,@\\'?\\.$%_]\", \"\", text, flags=re.I)  \n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['going']\n"
     ]
    }
   ],
   "source": [
    "from spacy.lemmatizer import Lemmatizer\n",
    "from spacy.lang.en import LEMMA_INDEX, LEMMA_EXC, LEMMA_RULES\n",
    "\n",
    "lemmatizer = Lemmatizer(LEMMA_INDEX, LEMMA_EXC, LEMMA_RULES)\n",
    "lemmas = lemmatizer(u'going', u'NOUN')\n",
    "print(lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rocks : supply\n",
      "corpora : corpus\n",
      "better : caring\n"
     ]
    }
   ],
   "source": [
    "# import these modules \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "  \n",
    "lemmatizer = WordNetLemmatizer() \n",
    "  \n",
    "print(\"rocks :\", lemmatizer.lemmatize(\"supplies\")) \n",
    "print(\"corpora :\", lemmatizer.lemmatize(\"corpora\")) \n",
    "  \n",
    "# a denotes adjective in \"pos\" \n",
    "print(\"better :\", lemmatizer.lemmatize(\"caring\", pos =\"a\")) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "game\n",
      "game\n",
      "game\n",
      "game\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    " \n",
    "words = [\"game\",\"gaming\",\"gamed\",\"games\"]\n",
    "ps = PorterStemmer()\n",
    " \n",
    "for word in words:\n",
    "    print(ps.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Road', 'maintenance', 'not', 'proper', 'in', 'the', 'way', 'increase', 'traffic', 'issues.Make', 'Sure', 'the', 'road', 'maintenance', 'is', 'in', 'proper', 'and', 'the', 'absence', 'of', 'maintenance', 'works', 'increase', 'the', 'road', 'accidents', '.']\n",
      "['Road', 'maintenance', 'proper', 'way', 'increase', 'traffic', 'issues.Make', 'Sure', 'road', 'maintenance', 'proper', 'absence', 'maintenance', 'works', 'increase', 'road', 'accidents', '.']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords \n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "text = \"Road maintenance not proper in the way increase traffic issues.Make Sure the road maintenance is in proper and the absence of  maintenance works increase the road accidents.\"\n",
    "\n",
    "water_token = word_tokenize(text)\n",
    "print(water_token)\n",
    "water_list =[]\n",
    "#Tokenising water data\n",
    "stop_words = set(stopwords.words('english'))\n",
    "for i in water_token:\n",
    "\n",
    "    #result = [i for i in water_token if not i in stop_words]\n",
    "    if i not in stop_words:\n",
    "        \n",
    "        water_list.append(i)\n",
    "print(water_list)\n",
    "    #water_token.append(tokenizer.tokenize(row['Subject_and_Complaint']))\n",
    "\n",
    "   \n",
    "   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class a:\n",
    "    def one(self):\n",
    "        a=10\n",
    "        return a\n",
    "    def two(self):\n",
    "        s=2\n",
    "        return s\n",
    "    def sum(self,a,b):\n",
    "        \n",
    "        b=a+b\n",
    "        print(b)\n",
    "        \n",
    "x=a()\n",
    "a=x.one()\n",
    "b=x.two()\n",
    "x.sum(a,b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['little', 'blue']\n"
     ]
    }
   ],
   "source": [
    "list1 = ['little','blue','widget']\n",
    "list2 = ['there','is','a','little','blue','cup','on','the','table']\n",
    "\n",
    "list3 = set(list1)&set(list2) # we don't need to list3 to actually be a list\n",
    "\n",
    "list4 = sorted(list3, key = lambda k : list1.index(k))\n",
    "print(list4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n"
     ]
    }
   ],
   "source": [
    "L2 = ['a','b','c','d']\n",
    "L1 = [1,2,3,4]\n",
    "d = dict(zip(L1,L2))\n",
    "print(d[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'apple': 3, 'egg': 2, 'banana': 1})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "list1=['apple','egg','apple','banana','egg','apple']\n",
    "counts = Counter(list1)\n",
    "print(counts)\n",
    "# Counter({'apple': 3, 'egg': 2, 'banana': 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'all': 16, 'limited': 1, 'concept': 1, 'secondly': 1}\n"
     ]
    }
   ],
   "source": [
    "string = [('limited', 1), ('all', 16), ('concept', 1), ('secondly', 1)]\n",
    "my_dict = dict(string)\n",
    "print(my_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['cat', 'dog', 'cat'], ['cat', 'mous', 'cat'], ['cat', 'dog', 'ant']]\n",
      "['cat', 'dog', 'ant']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "\n",
    "items = [['cats','dogs','cats'],['cats','mouse','cats'],['cats','dogs','ant']]\n",
    "lis =[1]\n",
    "count = 0\n",
    "s= []\n",
    "p= []\n",
    "\n",
    "\n",
    "\n",
    "for i in items:\n",
    "    p=[]\n",
    "    for j in i:\n",
    "        t=ps.stem(j)\n",
    "        p.append(t)\n",
    "    s.append(p)\n",
    "print(s)\n",
    "print(p)\n",
    "        \n",
    "        \n",
    "       \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-731dacfcb4ac>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-731dacfcb4ac>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    from nltk.stem.snowball import SnowballStemmer|\u001b[0m\n\u001b[0m                                                  ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[E050] Can't find model 'en_core_web_md'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-bd7ce8546402>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'en_core_web_md'\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# make sure to use larger model!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mu'dog cat banana'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/spacy/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(name, **overrides)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdepr_path\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mdeprecation_warning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW001\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdepr_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/spacy/util.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(name, **overrides)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'exists'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Path or Path-like to model data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mload_model_from_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE050\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [E050] Can't find model 'en_core_web_md'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory."
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_md')  # make sure to use larger model!\n",
    "tokens = nlp(u'dog cat banana')\n",
    "\n",
    "for token1 in tokens:\n",
    "    for token2 in tokens:\n",
    "        print(token1.text, token2.text, token1.similarity(token2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[E050] Can't find model 'en_core_web_md'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-8f2d9dd9e949>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'en_core_web_md'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mu'dog cat banana afskfsd'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/spacy/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(name, **overrides)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdepr_path\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mdeprecation_warning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW001\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdepr_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/spacy/util.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(name, **overrides)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'exists'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Path or Path-like to model data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mload_model_from_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE050\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [E050] Can't find model 'en_core_web_md'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory."
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "tokens = nlp(u'dog cat banana afskfsd')\n",
    "\n",
    "for token in tokens:\n",
    "    print(token.text, token.has_vector, token.vector_norm, token.is_oov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=14, size=100, alpha=0.025)\n",
      "['word2vec', 'sentence', 'and', 'first', 'is', 'second', 'another', 'for', 'more', 'one', 'this', 'the', 'yet', 'final']\n",
      "[-9.9240162e-04  2.2479747e-03 -1.3809589e-03  2.1756981e-03\n",
      " -3.3729998e-03 -2.0933383e-04  4.7422918e-03  1.1735978e-03\n",
      "  3.6348831e-03 -4.8938650e-03  2.7677694e-03 -2.9684864e-03\n",
      " -4.6216482e-03  3.4165636e-04 -2.7046187e-03  2.8676370e-03\n",
      "  1.4023427e-03 -3.8122672e-03  4.3672705e-03  4.7972030e-03\n",
      " -8.8001712e-04  4.7538397e-03 -3.8891500e-03  3.4212251e-03\n",
      "  8.2627044e-04 -4.4315771e-04  3.7317765e-03  9.2859240e-04\n",
      " -9.0510637e-04  4.4839475e-03  3.2141095e-03  7.7964098e-04\n",
      "  2.4458405e-03 -9.5450517e-04  4.3611685e-03 -3.3037930e-03\n",
      "  2.4999718e-03  2.0264750e-03  1.4665597e-03 -1.6466562e-03\n",
      "  9.4778009e-04  2.3049941e-04 -3.7716422e-03  1.6269435e-03\n",
      " -1.0917555e-03  2.5351450e-03 -3.3329526e-04 -2.4319263e-03\n",
      "  1.2336479e-03  8.9227757e-04 -7.8176131e-04 -3.6618565e-04\n",
      " -2.5570947e-03  1.4011526e-03 -7.4984983e-04 -2.8952151e-03\n",
      "  3.9606742e-03 -4.0236772e-03  3.3976603e-03  1.5528775e-03\n",
      "  8.5056375e-04  3.6172520e-03  4.2806999e-03 -1.5229869e-03\n",
      "  2.6417060e-03  2.1545128e-03  7.0849300e-04  2.0914327e-03\n",
      " -1.6160088e-03  2.3748057e-03 -1.4826390e-03 -9.0950669e-04\n",
      "  4.7254325e-03 -3.9444612e-03  3.3076090e-04  1.7174534e-03\n",
      "  5.2663754e-04  7.9126901e-04  3.2348786e-03  4.6221912e-03\n",
      " -1.8663919e-03 -3.3669022e-03 -3.1448873e-03  2.8925862e-03\n",
      "  1.8536663e-05 -3.4359212e-03 -3.2547635e-03 -1.1976651e-03\n",
      " -5.5203025e-05 -4.8337402e-03 -3.9488268e-03  2.0938516e-03\n",
      " -1.5306410e-03  2.6654096e-03  1.8137118e-03  3.2974956e-05\n",
      "  1.1216650e-03  4.7382684e-03  1.2857117e-03 -1.6520855e-03]\n",
      "Word2Vec(vocab=14, size=100, alpha=0.025)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gayathri/.local/lib/python3.5/site-packages/ipykernel_launcher.py:16: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  app.launch_new_instance()\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "# define training data\n",
    "sentences = [['this', 'is', 'the', 'first', 'sentence', 'for', 'word2vec'],\n",
    "\t\t\t['this', 'is', 'the', 'second', 'sentence'],\n",
    "\t\t\t['yet', 'another', 'sentence'],\n",
    "\t\t\t['one', 'more', 'sentence'],\n",
    "\t\t\t['and', 'the', 'final', 'sentence']]\n",
    "# train model\n",
    "model = Word2Vec(sentences, min_count=1)\n",
    "# summarize the loaded model\n",
    "print(model)\n",
    "# summarize vocabulary\n",
    "words = list(model.wv.vocab)\n",
    "print(words)\n",
    "# access vector for one word\n",
    "print(model['sentence'])\n",
    "# save model\n",
    "model.save('model.bin')\n",
    "# load model\n",
    "new_model = Word2Vec.load('model.bin')\n",
    "print(new_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gayathri True 6.8955503 True\n",
      "cat True 6.6808186 False\n",
      "banana True 6.700014 False\n",
      "afskfsd False 0.0 True\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "tokens = nlp(u'gayathri cat banana afskfsd')\n",
    "\n",
    "for token in tokens:\n",
    "    print(token.text, token.has_vector, token.vector_norm, token.is_oov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "power electricity 0.61113936\n",
      "power rain 0.2596926\n",
      "power power 1.0\n",
      "outage electricity 0.28468737\n",
      "outage rain 0.20991716\n",
      "outage power 0.17547126\n",
      "consumption electricity 0.5190694\n",
      "consumption rain 0.16790774\n",
      "consumption power 0.45900765\n",
      "voltage electricity 0.4578116\n",
      "voltage rain 0.13927579\n",
      "voltage power 0.5085738\n",
      "cat electricity 0.13433506\n",
      "cat rain 0.19856143\n",
      "cat power 0.17348595\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_md')  # make sure to use larger model!\n",
    "tokens = nlp('power outage consumption voltage cat')\n",
    "tok = nlp('electricity rain power')\n",
    "\n",
    "\n",
    "for token1 in tokens:\n",
    "    for token2 in tok:\n",
    "        \n",
    "    \n",
    "        print(token1.text, token2.text, token1.similarity(token2))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-2-b6919aab5a14>, line 15)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-b6919aab5a14>\"\u001b[0;36m, line \u001b[0;32m15\u001b[0m\n\u001b[0;31m    gets\tgetting\tgiven\tgives\tgo\u001b[0m\n\u001b[0m       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "stopwords= [\"able\",\"about\",\"above\",\"according\",\"accordingly\",\"across\",\"actually\",\"after\",\"afterwards\",\"again\",\"against\",\"aint\",\"all\",\"allow\",\n",
    "\"allows\",\"almost\",\"alone\",\"along\",\"already\",\"also\",\"although\",\"always\",\"am\",\"among\",\"amongst\",\"an\",\"and\",\"another\",\"any\",\"anybody\",\"anyhow\",\"anyone\",\"anything\",\"anyway\",\n",
    "\"anyways\",\"anywhere\",\"apart\",\"appear\",\"appreciate\",\"appropriate\",\"are\",\"aren't\",\"around\",\"as\",\n",
    "\"aside\",\"ask\",\"asking\",\"associated\",\"at\",\"available\",\"away\",\"awfully\",\"be\",\"became\",\"because\",\"become\",\"becomes\",\"becoming\",\"been\",\n",
    "\"before\",\"beforehand\",\"behind\",\"being\",\"believe\",\"below\",\"beside\",\"besides\",\"best\",\"better\",\n",
    "\"between\",\"beyond\",\"both\",\"brief\",\"but\",\"by\",\"came\",\"can\",\"can't\",\"cannot\",\"cant\",\"cause\",\"causes\",\n",
    "\"certain\",\"certainly\",\"changes\",\"clearly\",\"common\",\"come\",\"comes\",\"concerning\",\"consequently\",\n",
    "\"consider\",\"considering\",\"contain\",\"containing\",\"contains\",\"corresponding\",\"could\",\t\"couldn't\",\"course\",\"currently\",\n",
    "\"definitely\",\"described\",\"despite\",\"did\",\"didn't\",\"different\",\"do\",\"does\",\"doesn't\",\"doing\",\n",
    "\"don't\",\"done\",\"down\",\"downwards\",\"during\",\"each\",\"edu\",\"eg\",\"eight\",\"either\",\"else\",\n",
    "\"elsewhere\",\"enough\",\"entirely\",\"especially\",\"et\",\"etc\",\"even\",\"ever\",\"every\",\n",
    "\"everybody\",\"everyone\",\"everything\",\"everywhere\",\"ex\",\"exactly\",\"example\",\"except\",\"far\",\"few\",\n",
    "\"fifth\",\"first\",\"five\",\"followed\",\"following\",\"follows\",\"for\",\"former\",\"formerly\",\"forth\",\"four\",\"from\",\"further\",\n",
    "\"furthermore\",\"get\"\n",
    "gets\tgetting\tgiven\tgives\tgo\n",
    "goes\tgoing\tgone\tgot\tgotten\n",
    "greetings\thad\thadn't\thappens\thardly\n",
    "has\thasn't\thave\thaven't\thaving\n",
    "he\the's\thello\thelp\thence\n",
    "her\there\there's\thereafter\thereby\n",
    "herein\thereupon\thers\therself\thi\n",
    "him\thimself\this\thither\thopefully\n",
    "how\thowbeit\thowever\ti'd\ti'll\n",
    "i'm\ti've\tie\tif\tignored\n",
    "immediate\tin\tinasmuch\tinc\tindeed\n",
    "indicate\tindicated\tindicates\tinner\tinsofar\n",
    "instead\tinto\tinward\tis\tisn't\n",
    "it\tit'd\tit'll\tit's\tits\n",
    "itself\tjust\tkeep\tkeeps\tkept\n",
    "know\tknown\tknows\tlast\tlately\n",
    "later\tlatter\tlatterly\tleast\tless\n",
    "lest\tlet\tlet's\tlike\tliked\n",
    "likely\tlittle\tlook\tlooking\tlooks\n",
    "ltd\tmainly\tmany\tmay\tmaybe\n",
    "me\tmean\tmeanwhile\tmerely\tmight\n",
    "more\tmoreover\tmost\tmostly\tmuch\n",
    "must\tmy\tmyself\tname\tnamely\n",
    "nd\tnear\tnearly\tnecessary\tneed\n",
    "needs\tneither\tnever\tnevertheless\tnew\n",
    "next\tnine\tno\tnobody\tnon\n",
    "none\tnoone\tnor\tnormally\tnot\n",
    "nothing\tnovel\tnow\tnowhere\tobviously\n",
    "of\toff\toften\toh\tok\n",
    "okay\told\ton\tonce\tone\n",
    "ones\tonly\tonto\tor\tother\n",
    "others\totherwise\tought\tour\tours\n",
    "ourselves\tout\toutside\tover\toverall\n",
    "own\tparticular\tparticularly\tper\tperhaps\n",
    "placed\tplease\tplus\tpossible\tpresumably\n",
    "probably\tprovides\tque\tquite\tqv\n",
    "rather\trd\tre\treally\treasonably\n",
    "regarding\tregardless\tregards\trelatively\trespectively\n",
    "right\tsaid\tsame\tsaw\tsay\n",
    "saying\tsays\tsecond\tsecondly\tsee\n",
    "seeing\tseem\tseemed\tseeming\tseems\n",
    "seen\tself\tselves\tsensible\tsent\n",
    "serious\tseriously\tseven\tseveral\tshall\n",
    "she\tshould\tshouldn't\tsince\tsix\n",
    "so\tsome\tsomebody\tsomehow\tsomeone\n",
    "something\tsometime\tsometimes\tsomewhat\tsomewhere\n",
    "soon\tsorry\tspecified\tspecify\tspecifying\n",
    "still\tsub\tsuch\tsup\tsure\n",
    "t's\ttake\ttaken\ttell\ttends\n",
    "th\tthan\tthank\tthanks\tthanx\n",
    "that\tthat's\tthats\tthe\ttheir\n",
    "theirs\tthem\tthemselves\tthen\tthence\n",
    "there\tthere's\tthereafter\tthereby\ttherefore\n",
    "therein\ttheres\tthereupon\tthese\tthey\n",
    "they'd\tthey'll\tthey're\tthey've\tthink\n",
    "third\tthis\tthorough\tthoroughly\tthose\n",
    "though\tthree\tthrough\tthroughout\tthru\n",
    "thus\tto\ttogether\ttoo\ttook\n",
    "toward\ttowards\ttried\ttries\ttruly\n",
    "try\ttrying\ttwice\ttwo\tun\n",
    "under\tunfortunately\tunless\tunlikely\tuntil\n",
    "unto\tup\tupon\tus\tuse\n",
    "used\tuseful\tuses\tusing\tusually\n",
    "value\tvarious\tvery\tvia\tviz\n",
    "vs\twant\twants\twas\twasn't\n",
    "way\twe\twe'd\twe'll\twe're\n",
    "we've\twelcome\twell\twent\twere\n",
    "weren't\twhat\twhat's\twhatever\twhen\n",
    "whence\twhenever\twhere\twhere's\twhereafter\n",
    "whereas\twhereby\twherein\twhereupon\twherever\n",
    "whether\twhich\twhile\twhither\twho\n",
    "who's\twhoever\twhole\twhom\twhose\n",
    "why\twill\twilling\twish\twith\n",
    "within\twithout\twon't\twonder\twould\n",
    "wouldn't\tyes\tyet\tyou\tyou'd\n",
    "you'll\tyou're\tyou've\tyour\tyours\n",
    "yourself\tyourselves\tzero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('Sukanya', 'NNP'), (',', ','), ('Rajib', 'NNP'), ('Naba', 'NNP'), ('good', 'JJ'), ('friends', 'NNS'), ('.', '.')]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-06020f6f0add>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtagged\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0msent_clean2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtagged\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'PRP$'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'VBZ'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'NN'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent_clean2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-06020f6f0add>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtagged\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0msent_clean2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtagged\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'PRP$'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'VBZ'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'NN'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent_clean2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "import nltk \n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize, sent_tokenize \n",
    "stop_words = set(stopwords.words('english')) \n",
    "\n",
    "txt = \"Sukanya, Rajib and Naba are my good friends. \"\n",
    "    \n",
    "tagged = []\n",
    "# sent_tokenize is one of instances of  \n",
    "# PunktSentenceTokenizer from the nltk.tokenize.punkt module \n",
    "  \n",
    "tokenized = sent_tokenize(txt) \n",
    "for i in tokenized: \n",
    "      \n",
    "    # Word tokenizers is used to find the words  \n",
    "    # and punctuation in a string \n",
    "    wordsList = nltk.word_tokenize(i) \n",
    "  \n",
    "    # removing stop words from wordList \n",
    "    wordsList = [w for w in wordsList if not w in stop_words]  \n",
    "  \n",
    "    #  Using a Tagger. Which is part-of-speech  \n",
    "    # tagger or POS-tagger.  \n",
    "    tagged.append(nltk.pos_tag(wordsList) )\n",
    "  \n",
    "print(tagged) \n",
    "sent_clean2 = [x for (x,y) in tagged if y not in ('PRP$', 'VBZ', 'NN')]\n",
    "print(sent_clean2)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/gayathri/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
